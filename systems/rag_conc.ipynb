{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd77a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f34272",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Artificial Intelligence and Agents\n",
    "The history of AI is a history of fantasies, possibilities, demonstrations,\n",
    "and promise. Ever since Homer wrote of mechanical “tripods” waiting on\n",
    "the gods at dinner, imagined mechanical assistants have been a part of our\n",
    "culture. However, only in the last half century have we, the AI community, been able to build experimental machines that test hypotheses about\n",
    "the mechanisms of thought and intelligent behavior and thereby demonstrate mechanisms that formerly existed only as theoretical possibilities.\n",
    "– Bruce Buchanan [2005]\n",
    "This book is about artificial intelligence (AI), a field built on centuries of thought,\n",
    "which has been a recognized discipline for over 60 years. As well as solving\n",
    "practical tasks, AI provides tools to test hypotheses about the nature of thought\n",
    "itself. Deep scientific and engineering problems have already been solved and\n",
    "many more are waiting to be solved. Many practical applications are currently\n",
    "deployed and the potential exists for an almost unlimited number of future\n",
    "applications. This book presents the principles that underlie intelligent computational agents.\n",
    "1.1 What is Artificial Intelligence?\n",
    "Artificial intelligence, or AI, is the field that studies the synthesis and analysis of\n",
    "computational agents that act intelligently. Consider each part of this definition.\n",
    "An agent is something that acts in an environment; it does something.\n",
    "Agents include worms, dogs, thermostats, airplanes, robots, humans, companies, and countries.\n",
    "An agent is judged solely by how it acts. Agents that have the same effect\n",
    "in the world are equally good.\n",
    "3\n",
    "4 1. Artificial Intelligence and Agents\n",
    "Intelligence is a matter of degree. The aspects that go into an agent acting\n",
    "intelligently include\n",
    "• what it does is appropriate for its circumstances, its goals, and its perceptual and computational limitations\n",
    "• it takes into account the short-term and long-term consequences of its\n",
    "actions, including the effects on society and the environment\n",
    "• it learns from experience\n",
    "• it is flexible to changing environments and changing goals.\n",
    "A computational agent is an agent whose decisions about its actions can be\n",
    "explained in terms of computation. That is, the decision can be broken down\n",
    "into primitive operations that can be implemented in a physical device. This\n",
    "computation can take many forms. In humans, this computation is carried out\n",
    "in “wetware”; in computers it is carried out in “hardware.” Although there are\n",
    "some agents that are arguably not computational, such as the wind and rain\n",
    "eroding a landscape, it is an open question whether all intelligent agents are\n",
    "computational.\n",
    "All agents are limited. No agent is omniscient (all knowing) or omnipotent\n",
    "(can do anything). Agents can only observe everything in very specialized and\n",
    "constrained domains. Agents have finite memory. Agents in the real world do\n",
    "not have unlimited time to act.\n",
    "The central scientific goal of AI is to understand the principles that make\n",
    "intelligent behavior possible in natural or artificial systems. This is done by\n",
    "• the analysis of natural and artificial agents\n",
    "• formulating and testing hypotheses about what it takes to construct intelligent agents\n",
    "• designing, building, and experimenting with computational systems that\n",
    "perform tasks commonly viewed as requiring intelligence.\n",
    "As part of science, researchers build empirical systems to test hypotheses or to\n",
    "explore the space of possible designs. These are distinct from applications that\n",
    "are built to be useful for an application domain.\n",
    "The definition is not for intelligent thought. The role of thought is to affect\n",
    "action and lead to more intelligent behavior.\n",
    "The central engineering goal of AI is the design and synthesis of agents\n",
    "that act intelligently, which leads to useful artifacts.\n",
    "Building general intelligence isn’t the only goal of AI researchers. The aim\n",
    "of intelligence augmentation is to augment human intelligence and creativity.\n",
    "A diagnostic agent helps medical practitioners make better decisions, a search\n",
    "engine augments human memory, and natural language translation systems\n",
    "help people communicate. AI systems are often in human-in-the-loop mode,\n",
    "where humans and agents work together to solve problems. Sometimes the\n",
    "actions of artificial agents are to give advice to a human. Sometimes humans\n",
    "give advice or feedback to artificial agents, particularly for cases where decisions are made quickly or repeatedly.\n",
    "1.1. What is Artificial Intelligence? 5\n",
    "1.1.1 Artificial and Natural Intelligence\n",
    "Artificial intelligence (AI) is the established name for the field, but the term “artificial intelligence” is a source of much confusion because artificial intelligence\n",
    "may be interpreted as the opposite of real intelligence.\n",
    "For any phenomenon, you can distinguish real versus fake, where the fake\n",
    "is non-real. You can also distinguish natural versus artificial. Natural means\n",
    "occurring in nature and artificial means made by people.\n",
    "Example 1.1 A tsunami is a large wave in an ocean. Natural tsunamis occur from time to time and are caused by earthquakes or landslides. You could\n",
    "imagine an artificial tsunami that was made by people, for example, by exploding a bomb in the ocean, yet which is still a real tsunami. One could also\n",
    "imagine fake tsunamis: either artificial, using computer graphics, or natural,\n",
    "such as a mirage that looks like a tsunami but is not one.\n",
    "It is arguable that intelligence is different: you cannot have fake intelligence.\n",
    "If an agent behaves intelligently, it is intelligent. It is only the external behavior\n",
    "that defines intelligence; acting intelligently is being intelligent. Thus, artificial\n",
    "intelligence, if and when it is achieved, will be real intelligence created artificially.\n",
    "This idea of intelligence being defined by external behavior was the motivation for a test for intelligence designed by Turing [1950], which has become\n",
    "known as the Turing test. The Turing test consists of an imitation game where\n",
    "an interrogator can ask a witness, via a text interface, any question. If the interrogator cannot distinguish the witness from a human, the witness must be\n",
    "intelligent. Figure 1.1 shows a possible dialog that Turing suggested. An agent\n",
    "that is not really intelligent could not fake intelligence for arbitrary topics.\n",
    "Interrogator: In the first line of your sonnet which reads “Shall I compare thee\n",
    "to a summer’s day,” would not ”a spring day” do as well or better?\n",
    "Witness: It wouldn’t scan.\n",
    "Interrogator: How about “a winter’s day,” That would scan all right.\n",
    "Witness: Yes, but nobody wants to be compared to a winter’s day.\n",
    "Interrogator: Would you say Mr. Pickwick reminded you of Christmas?\n",
    "Witness: In a way.\n",
    "Interrogator: Yet Christmas is a winter’s day, and I do not think Mr. Pickwick\n",
    "would mind the comparison.\n",
    "Witness: I don’t think you’re serious. By a winter’s day one means a typical\n",
    "winter’s day, rather than a special one like Christmas.\n",
    "Figure 1.1: Part of Turing’s possible dialog for the Turing test\n",
    "6 1. Artificial Intelligence and Agents\n",
    "There has been much debate about the usefulness of the Turing test. Unfortunately, although it may provide a test for how to recognize intelligence, it\n",
    "does not provide a way to realize intelligence.\n",
    "Levesque [2014] suggested a new form of question, a Winograd schema\n",
    "after the following example of Winograd [1972]:\n",
    "• The city councilmen refused the demonstrators a permit because they\n",
    "feared violence. Who feared violence?\n",
    "• The city councilmen refused the demonstrators a permit because they\n",
    "advocated violence. Who advocated violence?\n",
    "These two sentences only differ in one word – feared/advocated – but have the\n",
    "opposite answer.\n",
    "Winograd schemas have the property that (a) humans can easily disambiguate them and (b) there is no simple grammatical or statistical test that could\n",
    "disambiguate them. For example, the sentences above would not qualify if the\n",
    "phrase “demonstrators feared violence” was much less or more likely than the\n",
    "phrase “councilmen feared violence” independently of the context, and similarly with advocating.\n",
    "Example 1.2 The following examples are due to Davis [2015]:\n",
    "• Steve follows Fred’s example in everything. He [admires/influences] him\n",
    "hugely. Who [admires/influences] whom?\n",
    "• The table won’t fit through the doorway because it is too [wide/narrow].\n",
    "What is too [wide/narrow]?\n",
    "• Grace was happy to trade me her sweater for my jacket. She thinks it looks\n",
    "[great/dowdy] on her. What looks [great/dowdy] on Grace?\n",
    "• Bill thinks that calling attention to himself was rude [to/of] Bert. Who\n",
    "called attention to himself?\n",
    "Each of these have their own reasons why one answer is preferred to the other.\n",
    "A computer that can reliably answer these questions needs to know about all of\n",
    "these reasons, and arguably requires the ability to do commonsense reasoning.\n",
    "Common sense should also allow it to reject sentences such as “The doorway\n",
    "won’t fit through the chair because it is too narrow. What is too narrow?”.\n",
    "Figure 1.2 (page 7) shows some answers provided by ChatGPT [OpenAI,\n",
    "2022], based on GPT-3 [Brown et al., 2020], one of the most capable large language models in 2022. ChatGPT gives a different answer each time it is called.\n",
    "You can decide whether it solves this Winograd schema. The technology behind GPT-3 and related models is described in Section 8.5 (page 350).\n",
    "Grosz [2012], arguing that language is inherently social and connected to\n",
    "human cooperation, suggested that a more appropriate test should involve\n",
    "purposeful natural language, not language just for the purpose of tricking a\n",
    "human. She suggested the question:\n",
    "1.1. What is Artificial Intelligence? 7\n",
    "Human: The table won’t fit through the doorway because it is too narrow.\n",
    "What is too narrow?\n",
    "ChatGPT: If a table won’t fit through a doorway because it is too narrow,\n",
    "it means that the width of the table is greater than the width of the doorway.\n",
    "In other words, the doorway is not wide enough to allow the table to pass\n",
    "through it. This can be a problem if you are trying to move the table from one\n",
    "room to another and the only way to do so is through the doorway. In order\n",
    "to solve this problem, you may need to remove the door or trim the table so\n",
    "that it fits through the doorway. Alternatively, you may need to find a different\n",
    "route to move the table, such as through a window or by taking it apart and\n",
    "reassembling it in the other room.\n",
    "Human: The table won’t fit through the doorway because it is too wide.\n",
    "What is too wide?\n",
    "ChatGPT: If a table won’t fit through a doorway because it is too wide, it\n",
    "means that the width of the doorway is smaller than the width of the table.\n",
    "In other words, the table is too big to fit through the doorway. This can be\n",
    "a problem if you are trying to move the table from one room to another and\n",
    "the only way to do so is through the doorway. In order to solve this problem,\n",
    "you may need to remove the door or trim the table so that it fits through the\n",
    "doorway. Alternatively, you may need to find a different route to move the\n",
    "table, such as through a window or by taking it apart and reassembling it in\n",
    "the other room.\n",
    "Human: The doorway won’t fit through the chair because it is too narrow.\n",
    "What is too narrow?\n",
    "ChatGPT: ”Too narrow” means that the doorway is not wide enough to\n",
    "fit through the chair. In this context, the doorway is the object that is being\n",
    "compared to the chair, and it is not wide enough to pass through the chair\n",
    "without encountering some difficulty or obstruction. This could be because\n",
    "the doorway is physically smaller in width than the chair, or because there is\n",
    "some other physical feature of the chair that is preventing the doorway from\n",
    "fitting through it. For example, the chair might have armrests or other protuberances that make it difficult for the doorway to pass through. In general, the\n",
    "phrase ”too narrow” is used to describe something that is not wide enough to\n",
    "fit through or pass a particular opening or gap.\n",
    "Human: The doorway won’t fit through the chair because it is too narrow.\n",
    "What is too narrow?\n",
    "ChatGPT: If a doorway is too narrow to fit through a chair, it means that the\n",
    "width of the doorway is smaller than the width of the chair. This can happen if\n",
    "the chair is too wide to fit through the doorway, or if the doorway is not wide\n",
    "enough to accommodate the chair. In either case, the chair will not be able to\n",
    "pass through the doorway because there is not enough space for it to fit.\n",
    "Figure 1.2: ChatGPT answers to some Winograd schemas\n",
    "8 1. Artificial Intelligence and Agents\n",
    "Is it imaginable that a computer (agent) team member could behave, over\n",
    "the long term and in uncertain, dynamic environments, in such a way that\n",
    "people on the team will not notice it is not human?\n",
    "– Barbara Grosz [2012]\n",
    "An equal member of the team needs to be trusted enough to act in the world\n",
    "appropriately, know when to ask questions, and when to not act. This challenge also allows for incremental improvement; starting with simple group\n",
    "interactions before moving to complex ones.\n",
    "Interacting in natural language is not the only aspect of intelligence. An\n",
    "agent acting in an environment needs common sense, “the ability to make\n",
    "effective use of ordinary, everyday, experiential knowledge in achieving ordinary, practical goals” [Brachman and Levesque, 2022b]. Here, knowledge\n",
    "is used in a general way to mean any non-transient information in an agent.\n",
    "Such knowledge is typically not stated in natural language; people do not state\n",
    "what everyone knows. Some knowledge, such as how to ride a bike or recognize a face, cannot be effectively conveyed by natural language. Formalizing\n",
    "common sense has a long history [McCarthy, 1958; Davis, 1990], including the\n",
    "development of representations and actual commonsense knowledge.\n",
    "1.1.2 Natural Intelligence\n",
    "The obvious naturally intelligent agent is the human being. Some people might\n",
    "say that worms, insects, or bacteria are intelligent, but more people would say\n",
    "that dogs, whales, or monkeys are intelligent (see Exercise 1.1 (page 48)). One\n",
    "class of intelligent agents that may be more intelligent than humans is the class\n",
    "of organizations. Ant colonies are a prototypical example of organizations.\n",
    "Each individual ant may not be very intelligent, but an ant colony can act more\n",
    "intelligently than any individual ant. The colony can discover food and exploit it very effectively, as well as adapt to changing circumstances. Corporations can be more intelligent than individual people. Companies develop,\n",
    "manufacture, and distribute products where the sum of the skills required is\n",
    "much more than any individual could master. Modern computers, from lowlevel hardware to high-level software, are more complicated than any single\n",
    "human can understand, yet they are manufactured daily by organizations of\n",
    "humans. Human society viewed as an agent is arguably the most intelligent\n",
    "agent known.\n",
    "It is instructive to consider where human intelligence comes from. There\n",
    "are three main sources:\n",
    "Biology Humans have evolved into adaptable animals that can survive in various habitats.\n",
    "Culture Culture provides not only language, but also useful tools, useful concepts, and the wisdom that is passed from parents and teachers to children.\n",
    "1.2. A Brief History of Artificial Intelligence 9\n",
    "Lifelong learning Humans learn throughout their life and accumulate knowledge and skills.\n",
    "These sources interact in complex ways. Biological evolution has provided\n",
    "stages of growth that allow for different learning at different stages of life.\n",
    "Biology and culture have evolved together; humans can be helpless at birth,\n",
    "presumably because of our culture of looking after infants. Culture interacts\n",
    "strongly with learning. A major part of lifelong learning is what people are\n",
    "taught by parents and teachers. Language, which is part of culture, provides\n",
    "distinctions in the world that are useful for learning.\n",
    "When building an intelligent system, the designers have to decide which\n",
    "of these sources of intelligence need to be programmed in, and which can be\n",
    "learned. It is very unlikely that anyone will be able to build an agent that starts\n",
    "with a clean slate and learns everything, particularly for non-repetitive tasks.\n",
    "Similarly, most interesting and useful intelligent agents learn to improve their\n",
    "behavior.\n",
    "1.2 A Brief History of Artificial Intelligence\n",
    "Throughout human history, people have used technology to model themselves.\n",
    "There is evidence of this from ancient China, Egypt, and Greece, bearing witness to the universality of this activity. Each new technology has, in its turn,\n",
    "been exploited to build intelligent agents or models of mind. Clockwork, hydraulics, telephone switching systems, holograms, analog computers, and digital computers have all been proposed both as technological metaphors for intelligence and as mechanisms for modeling mind.\n",
    "Hobbes (1588–1679), who has been described by Haugeland [1985, p. 85]\n",
    "as the “Grandfather of AI,” espoused the position that thinking was symbolic\n",
    "reasoning, like talking out loud or working out an answer with pen and paper. The idea of symbolic reasoning was further developed by Descartes (1596–\n",
    "1650), Pascal (1623–1662), Spinoza (1632–1677), Leibniz (1646–1716), and others\n",
    "who were pioneers in the European philosophy of mind.\n",
    "The idea of symbolic operations became more concrete with the development of computers. Babbage (1792–1871) designed the first general-purpose\n",
    "computer, the Analytical Engine. Leonardo Torres y Quevedo build a chess\n",
    "playing machine based on similar ideas in 1911 [Randell, 1982]. In the early\n",
    "part of the twentieth century, there was much work done on understanding\n",
    "computation. Several models of computation were proposed, including the\n",
    "Turing machine by Alan Turing (1912–1954), a theoretical machine that writes\n",
    "symbols on an infinitely long tape, and the lambda calculus of Church (1903–\n",
    "1995), which is a mathematical formalism for rewriting formulas. It can be\n",
    "shown that these very different formalisms are equivalent in that any function\n",
    "computable by one is computable by the others. This leads to the Church–\n",
    "Turing thesis:\n",
    "10 1. Artificial Intelligence and Agents\n",
    "Any effectively computable function can be carried out on a Turing\n",
    "machine (and so also in the lambda calculus or any of the other\n",
    "equivalent formalisms).\n",
    "Effectively computable means following well-defined operations. In Turing’s\n",
    "day, “computers” were people who followed well-defined steps; computers\n",
    "as known today did not exist. This thesis says that all computation can be\n",
    "carried out on a Turing machine or one of the other equivalent computational\n",
    "machines. The Church–Turing thesis cannot be proved but it is a hypothesis\n",
    "that has stood the test of time. No one has built a machine that has carried\n",
    "out computation that cannot be computed by a Turing machine. There is no\n",
    "evidence that people can compute functions that are not Turing computable.\n",
    "This provides an argument that computation is more than just a metaphor for\n",
    "intelligence; reasoning is computation and computation can be carried out by\n",
    "a computer.\n",
    "Some of the first applications of computers were AI programs. Samuel\n",
    "[1959] built a checkers program in 1952 and implemented a program that learns\n",
    "to play checkers in the late 1950s. His program beat the Connecticut state\n",
    "checkers champion in 1961. Wang [1960] implemented a program that proved\n",
    "every logic theorem (nearly 400) in Principia Mathematica [Whitehead and Russell, 1925, 1927]. Newell and Simon [1956] built a program, Logic Theorist,\n",
    "that discovers proofs in propositional logic.\n",
    "In parallel, there was also much work on neural networks learning inspired\n",
    "by how neurons work. McCulloch and Pitts [1943] showed how a simple\n",
    "thresholding “formal neuron” could be the basis for a Turing-complete machine. Learning for artificial neural networks was first described by Minsky\n",
    "[1952]. One of the early significant works was the perceptron of Rosenblatt\n",
    "[1958]. The work on neural networks became less prominent for a number of\n",
    "years after the 1968 book by Minsky and Papert [1988], which argued that the\n",
    "representations learned were inadequate for intelligent action. Many technical\n",
    "foundations for neural networks were laid in the 1980s and 1990s [Rumelhart\n",
    "et al., 1986; Hochreiter and Schmidhuber, 1997; LeCun et al., 1998a]. Widespread\n",
    "adoption followed the success by Krizhevsky et al. [2012] for ImageNet [Deng\n",
    "et al., 2009], a dataset of over 3 million images labelled with over 5000 categories. Subsequent major advances include the introduction of generative adversarial networks (GANs) [Goodfellow et al., 2014] and transformers [Vaswani\n",
    "et al., 2017]. Neural networks in various forms are now the state of the art for\n",
    "predictive models for large perceptual datasets, including images, video, and\n",
    "speech, as well as some tasks for text. They are also used for generative AI,\n",
    "to generate images, text, code, molecules, and other structured output. See\n",
    "Chapter 8.\n",
    "Neural networks are one of many machine learning tools used for making\n",
    "predictions from data in modern applications. Other methods have been developed though the years, including decision trees [Breiman et al., 1984; Quinlan,\n",
    "1993] and logistic regression, introduced by Verhulst in 1832 [Cramer, 2002].\n",
    "1.2. A Brief History of Artificial Intelligence 11\n",
    "These have diverse applications in many areas of science. Combining these algorithms leads to the state-of-the-art gradient-boosted trees [Friedman, 2001;\n",
    "Chen and Guestrin, 2016], which demonstrates the close interconnections between statistics and machine learning.\n",
    "While useful, making predictions is not sufficient to determine what an\n",
    "agent should do; an agent also needs to plan. Planning in AI was initially\n",
    "based on deterministic actions. Fikes and Nilsson [1971] used deterministic\n",
    "actions to control a mobile robot. Planning under uncertainty has a long history. Markov decision processes (MDPs), the foundation for much of planning\n",
    "under uncertainty, and dynamic programming, a general way to solve them,\n",
    "were invented by Bellman [1957]. These were extended into decision-theoretic\n",
    "planning in the 1990’s [Boutilier et al., 1999]. Decision-theoretic planning with\n",
    "learning is called reinforcement learning. The first reinforcement learning\n",
    "programs were due to Andreae [1963] and Michie [1963]. Major advances\n",
    "came with the inventions of temporal-difference learning [Sutton, 1988] and\n",
    "Q-learning [Watkins and Dayan, 1992]. Work in reinforcement learning has\n",
    "exploded, including superhuman performance in chess, Go and other games\n",
    "[Silver et al., 2017].\n",
    "Planning requires representations. The need for representations was recognized early.\n",
    "A computer program capable of acting intelligently in the world must have\n",
    "a general representation of the world in terms of which its inputs are interpreted. Designing such a program requires commitments about what\n",
    "knowledge is and how it is obtained. . . .More specifically, we want a computer program that decides what to do by inferring in a formal language\n",
    "that a certain strategy will achieve its assigned goal. This requires formalizing concepts of causality, ability, and knowledge.\n",
    "– McCarthy and Hayes [1969]\n",
    "Many of the early representations were ad hoc, such as frames [Minsky,\n",
    "1975], like the schemas of Kant [1787], Bartlett [1932], and Piaget [1953]. Later\n",
    "representations were based on logic [Kowalski, 1979], with knowledge being\n",
    "defined in logic and efficient inference. This resulted in languages such as Prolog [Kowalski, 1988; Colmerauer and Roussel, 1996].\n",
    "Probabilities were eschewed in AI, because of the number of parameters\n",
    "required, until the breakthrough of Bayesian networks (belief networks) and\n",
    "graphical models [Pearl, 1988], which exploit conditional independence, and\n",
    "form a basis for modeling causality. Combining first-order logic and probability is the topic of statistical relational AI [De Raedt et al., 2016].\n",
    "There has been a continual tension between how much knowledge is learned\n",
    "and how much is provided by human experts or is innate to an agent. It has\n",
    "long been recognized that learning is needed, and it is known that learning\n",
    "cannot be achieved with data alone (page 315). During the 1970s and 1980s,\n",
    "12 1. Artificial Intelligence and Agents\n",
    "expert systems came to prominence, where the aim was to capture the knowledge of an expert in some domain so that a computer could carry out expert\n",
    "tasks. DENDRAL [Buchanan and Feigenbaum, 1978], developed from 1965 to\n",
    "1983 in the field of organic chemistry, proposed plausible structures for new\n",
    "organic compounds. MYCIN [Buchanan and Shortliffe, 1984], developed from\n",
    "1972 to 1980, diagnosed infectious diseases of the blood, prescribed antimicrobial therapy, and explained its reasoning.\n",
    "An alternative approach, de-emphasizing explicit knowledge representations, emphasized situated embodied agents [Brooks, 1990; Mackworth, 2009].\n",
    "The hypothesis is that intelligence emerges, in evolution and individual development, through ongoing interaction and coupling with a real environment.\n",
    "During the 1960s and 1970s, natural language understanding systems were\n",
    "developed for limited domains. For example, the STUDENT program of Bobrow [1967] could solve high-school algebra tasks expressed in natural language. Winograd’s [1972] SHRDLU system could, using restricted natural\n",
    "language, discuss and carry out tasks in a simulated blocks world. CHAT80 [Warren and Pereira, 1982] could answer geographical questions placed to it\n",
    "in natural language. Figure 1.3 (page 13) shows some questions that CHAT-80\n",
    "answered based on a database of facts about countries, rivers, and so on. These\n",
    "systems could only reason in very limited domains using restricted vocabulary and sentence structure. Interestingly, IBM’s Watson, which beat the world\n",
    "champion in the TV game show Jeopardy! in 2011, used a technique similar\n",
    "to CHAT-80 [Lally et al., 2012] for understanding questions; see Section 15.7\n",
    "(page 674).\n",
    "In applications using language in the wild, such as speech recognition and\n",
    "translation in phones, many technologies are combined, including neural networks; see Chapter 8. Large language models (page 364), trained on huge\n",
    "datasets, can be used to predict the next word in a text, enabling predictive\n",
    "spelling and the creation of new text.\n",
    "1.2.1 Relationship to Other Disciplines\n",
    "AI is a very young discipline. Other disciplines as diverse as philosophy, neurobiology, evolutionary biology, psychology, economics, political science, sociology, anthropology, control engineering, statistics, and many more have been\n",
    "studying aspects of intelligence much longer.\n",
    "The science of AI could be described as “synthetic psychology,” “experimental philosophy,” or “computational epistemology” – epistemology is the\n",
    "study of knowledge. AI can be seen as a way to study the nature of knowledge\n",
    "and intelligence, but with more powerful experimental tools than were previously available. Instead of being able to observe only the external behavior of\n",
    "intelligent systems, as philosophy, psychology, economics, and sociology have\n",
    "traditionally been able to do, AI researchers experiment with executable models of intelligent behavior. Most important, such models are open to inspection,\n",
    "redesign, and experimentation in a complete and rigorous way. Modern com-\n",
    "1.2. A Brief History of Artificial Intelligence 13\n",
    "puters provide a way to construct the models about which philosophers have\n",
    "only been able to theorize. AI researchers can experiment with these models as opposed to just discussing their abstract properties. AI theories can be\n",
    "empirically grounded in implementations. Sometimes simple agents exhibit\n",
    "complex behavior, and sometimes sophisticated, theoretically motivated algorithms don’t work in real-world domains, which would not be known without\n",
    "implementing the agents.\n",
    "It is instructive to consider an analogy between the development of flying machines over the past few centuries and the development of thinking\n",
    "machines over the past few decades. There are several ways to understand\n",
    "flying. One is to dissect known flying animals and hypothesize their common structural features as necessary fundamental characteristics of any flying\n",
    "agent. With this method, an examination of birds, bats, and insects would suggest that flying involves the flapping of wings made of some structure covered\n",
    "with feathers or a membrane. Furthermore, the hypothesis could be tested by\n",
    "strapping feathers to one’s arms, flapping, and jumping into the air, as Icarus\n",
    "did. An alternative methodology is to try to understand the principles of flying\n",
    "without restricting oneself to the natural occurrences of flying. This typically\n",
    "involves the construction of artifacts that embody the hypothesized principles,\n",
    "Does Afghanistan border China?\n",
    "What is the capital of Upper Volta?\n",
    "Which country’s capital is London?\n",
    "Which is the largest African country?\n",
    "How large is the smallest American country?\n",
    "What is the ocean that borders African countries and that borders\n",
    "Asian countries?\n",
    "What are the capitals of the countries bordering the Baltic?\n",
    "How many countries does the Danube flow through?\n",
    "What is the total area of countries south of the Equator and not in\n",
    "Australasia?\n",
    "What is the average area of the countries in each continent?\n",
    "Is there more than one country in each continent?\n",
    "What are the countries from which a river flows into the Black Sea?\n",
    "What are the continents no country in which contains more than two\n",
    "cities whose population exceeds 1 million?\n",
    "Which country bordering the Mediterranean borders a country that\n",
    "is bordered by a country whose population exceeds the population\n",
    "of India?\n",
    "Which countries with a population exceeding 10 million border the\n",
    "Atlantic?\n",
    "Figure 1.3: Some questions CHAT-80 could answer\n",
    "14 1. Artificial Intelligence and Agents\n",
    "even if they do not behave like flying animals in any way except flying. This\n",
    "second method has provided both useful tools – airplanes – and a better understanding of the principles underlying flying, namely aerodynamics. Birds\n",
    "are still much better at flying though forests.\n",
    "AI takes an approach analogous to that of aerodynamics. AI researchers\n",
    "are interested in testing general hypotheses about the nature of intelligence\n",
    "by building machines that are intelligent and that do not necessarily mimic\n",
    "humans or organizations. This also offers an approach to the question, “Can\n",
    "computers really think?” by considering the analogous question, “Can airplanes really fly?”\n",
    "AI is intimately linked with the discipline of computer science because the\n",
    "study of computation is central to AI. It is essential to understand algorithms,\n",
    "data structures, and combinatorial complexity to build intelligent machines. It\n",
    "is also surprising how much of computer science started as a spinoff from AI,\n",
    "from timesharing to computer algebra systems.\n",
    "Finally, AI can be seen as coming under the umbrella of cognitive science.\n",
    "Cognitive science links various disciplines that study cognition and reasoning, from psychology to linguistics to anthropology to neuroscience. AI distinguishes itself within cognitive science by providing tools to build intelligence\n",
    "rather than just studying the external behavior of intelligent agents or dissecting the inner workings of intelligent systems.\n",
    "1.3 Agents Situated in Environments\n",
    "AI is about practical reasoning: reasoning in order to do something. A coupling\n",
    "of perception, reasoning, and acting comprises an agent. An agent acts in an\n",
    "environment. An agent’s environment often includes other agents. An agent\n",
    "together with its environment is called a world.\n",
    "An agent could be, for example, a coupling of a computational engine with\n",
    "physical sensors and actuators, called a robot, where the environment is a\n",
    "physical setting. An autonomous agent is one that acts in the world without human intervention. A semi-autonomous agent acts with a human-inthe-loop who may provide perceptual information and carry out the task. An\n",
    "agent could be a program that acts in a purely computational environment, a\n",
    "software agent, often called a bot.\n",
    "Figure 1.4 (page 15) shows a black-box view of an agent in terms of its\n",
    "inputs and outputs. At any time, what an agent does depends on:\n",
    "• prior knowledge about the agent and the environment\n",
    "• stimuli received from the environment, which can include observations\n",
    "about the environment (e.g., light, sound, keyboard commands, web requests) as well as actions that the environment imposes on the agent (e.g.,\n",
    "bumping the agent)\n",
    "• past experiences, including history of interaction with the environment\n",
    "(its previous actions and stimuli) and other data, from which it can learn\n",
    "1.3. Agents Situated in Environments 15\n",
    "• goals that it must try to achieve or preferences over states of the world\n",
    "• abilities, the primitive actions the agent is capable of carrying out.\n",
    "Inside the black box, an agent has a belief state that can encode beliefs\n",
    "about its environment, what it has learned, what it is trying to do, and what\n",
    "it intends to do. An agent updates this internal state based on stimuli. It uses\n",
    "the belief state and stimuli to decide on its actions. Much of this book is about\n",
    "what is inside this black box.\n",
    "Purposive agents have preferences or goals. They prefer some states of the\n",
    "world to other states, and they act to try to achieve the states they prefer most.\n",
    "The non-purposive agents are grouped together and called nature. Whether\n",
    "or not an agent is purposive is a modeling assumption that may, or may not,\n",
    "be appropriate. For example, for some applications it may be appropriate to\n",
    "model a dog as purposive, such as drug-sniffing dogs, and for others it may\n",
    "suffice to model a dog as non-purposive, such as when they are just part of the\n",
    "environment.\n",
    "If an agent does not have preferences, by definition it does not care what\n",
    "world state it ends up in, and so it does not matter to it what it does. The reason\n",
    "to design an agent is to instill preferences in it – to make it prefer some world\n",
    "states and try to achieve them. An agent does not have to know its preferences\n",
    "explicitly. For example, a thermostat for a heater is an agent that senses the\n",
    "world and turns the heater either on or off. There are preferences embedded\n",
    "in the thermostat, such as to keep the room at a pleasant temperature, even\n",
    "though the thermostat arguably does not know these are its preferences. The\n",
    "preferences of an agent are often the preferences of the designer of the agent,\n",
    "but sometimes an agent can acquire goals and preferences at run time.\n",
    "Prior Knowledge\n",
    "Environment\n",
    "Stimuli\n",
    "Actions\n",
    "Past Experiences\n",
    "Goals/Preferences\n",
    "Agent\n",
    "Abilities\n",
    "Figure 1.4: An agent interacting with an environment\n",
    "16 1. Artificial Intelligence and Agents\n",
    "This is an all-encompassing view of intelligent agents varying in complexity from a simple thermostat, to a diagnostic advising system whose perceptions and actions are mediated by human beings, to a team of mobile robots,\n",
    "to society itself. An agent does not have access to anything else; anything that\n",
    "does not affect one of these inputs cannot affect the agent’s action.\n",
    "1.4 Prototypical Applications\n",
    "AI applications are widespread and diverse and include medical diagnosis,\n",
    "scheduling factory processes, robots for hazardous environments, game playing, autonomous cars, natural language translation systems, choosing advertisements, personal assistants, and tutoring agents. Rather than treating each\n",
    "application separately, we abstract the essential features of such applications to\n",
    "better understand the principles behind intelligent reasoning and action.\n",
    "Five main application domains are developed in examples throughout the\n",
    "book. Although the particular examples presented are simple – otherwise they\n",
    "would not fit into the book – the application domains are representative of the\n",
    "range of domains in which AI techniques can be, and are being, used.\n",
    "1.4.1 An Autonomous Delivery and Helping Robot\n",
    "Imagine a robot with wheels and the ability to pick up, put down and manipulate objects. It has sensing capabilities allowing it to recognize objects and\n",
    "to avoid obstacles. It can be given orders in natural language and obey them,\n",
    "making reasonable choices about what to do when its goals conflict. Such a\n",
    "robot could deliver packages or coffee in an office environment, clean a home\n",
    "and put things in their appropriate place, or help caregivers in a hospital. Embedded in a wheelchair, it could help disabled people. It should be useful as\n",
    "well as safe.\n",
    "In terms of the black-box characterization of an agent in Figure 1.4 (page 15),\n",
    "the autonomous delivery robot has as inputs:\n",
    "• prior knowledge, provided by the agent designer, about the agent’s capabilities, what objects it may encounter and have to differentiate, what\n",
    "requests mean, and perhaps about its environment, such as a map\n",
    "• past experience obtained while acting, for instance, about the effects of its\n",
    "actions (and – hopefully limited – experiences of breaking objects), what\n",
    "objects are common in the world, and what requests to expect at different\n",
    "times of the day\n",
    "• goals in terms of what it should deliver and when, as well as preferences\n",
    "specifying trade-offs, such as when it must forgo one goal to pursue another, or the trade-off between acting quickly and acting safely\n",
    "• stimuli about its environment from observations from input devices such\n",
    "as cameras, sonar, touch, sound, laser range finders, or keyboards as well\n",
    "as stimuli such as the agent being forcibly moved or crashing.\n",
    "1.4. Prototypical Applications 17\n",
    "The robot’s outputs are motor controls specifying how its wheels should turn,\n",
    "where its limbs should move, and what it should do with its grippers. Other\n",
    "outputs may include speech and a video display.\n",
    "Example 1.3 Figure 1.5 depicts a typical laboratory environment for a delivery\n",
    "robot. This environment consists of four laboratories and many offices. In our\n",
    "examples, the robot can only push doors, and the directions of the doors in\n",
    "the diagram reflect the directions in which the robot can travel. Rooms require\n",
    "keys and those keys can be obtained from various sources. The robot must\n",
    "deliver parcels, beverages, and dishes from room to room. The environment\n",
    "also contains a stairway that is potentially hazardous to the robot.\n",
    "1.4.2 A Diagnostic Assistant\n",
    "A diagnostic assistant is intended to advise a human about some particular\n",
    "system such as a medical patient, the electrical system in a home, or an automobile. The diagnostic assistant should advise about potential underlying\n",
    "faults or diseases, what tests to carry out, and what treatment to prescribe. To\n",
    "give such advice, the assistant requires a model of the system, including knowledge of potential causes, available tests, available treatments, and observations\n",
    "of the system (which are often called symptoms).\n",
    "stairs\n",
    "lab A lab B\n",
    "lab D lab C\n",
    "r101 r103 r105 r107 r109 r111\n",
    "r113\n",
    "r115\n",
    "r117\n",
    "r131 r129 r127 r125 r123 r121 r119\n",
    "main\n",
    "office\n",
    "Figure 1.5: A typical laboratory environment for the delivery robot. This shows\n",
    "the locations of the doors and which way they open.\n",
    "18 1. Artificial Intelligence and Agents\n",
    "To be useful, the diagnostic assistant must provide added value, be easy for\n",
    "a human to use, and not be more trouble than it is worth. A diagnostic assistant\n",
    "connected to the Internet can draw on expertise from throughout the world,\n",
    "and its actions can be based on the most up-to-date research. However, it must\n",
    "be able to justify why the suggested diagnoses or actions are appropriate. Humans are, and should be, suspicious of computer systems that are opaque and\n",
    "impenetrable. When humans are responsible for what they do, even if their\n",
    "actions are based on a computer system’s advice, the system needs to convince\n",
    "the human that the suggested actions are defensible.\n",
    "Example 1.4 Figure 1.6 shows an electrical distribution system in a home.\n",
    "In this home, power comes into the home through circuit breakers and then it\n",
    "goes to power outlets or to lights through light switches. For example, light l1\n",
    "is on if there is power coming into the home, if circuit breaker cb1 is on, and if\n",
    "switches s1 and s2 are either both up or both down. This is the sort of model that\n",
    "someone may have of the electrical power in the home, which they could use\n",
    "to determine what is wrong given evidence about the position of the switches\n",
    "and which lights are on and which are off. The diagnostic assistant is there to\n",
    "help a resident or an electrician troubleshoot electrical problems.\n",
    "In terms of the black-box definition of an agent in Figure 1.4 (page 15), the\n",
    "diagnostic assistant has as inputs:\n",
    "• prior knowledge, such as how switches and lights normally work, how\n",
    "diseases or malfunctions manifest themselves, what information tests provide, the effects of repairs or treatments, and how to find out information\n",
    "w0\n",
    "s2\n",
    "w4\n",
    "s1\n",
    "s3\n",
    "w1\n",
    "w2\n",
    "l\n",
    "1\n",
    "l\n",
    "2\n",
    "cb1\n",
    "w3\n",
    "p1\n",
    "cb2\n",
    "w6\n",
    "p2\n",
    "outside_power\n",
    "w5\n",
    "switch\n",
    "circuit\n",
    "breaker\n",
    "light\n",
    "power\n",
    "outlet\n",
    "Figure 1.6: An electrical environment for the diagnostic assistant\n",
    "1.4. Prototypical Applications 19\n",
    "• past experience, in terms of data of previous cases that include the effects\n",
    "of repairs or treatments, the prevalence of faults or diseases, the prevalence of symptoms for these faults or diseases, and the accuracy of tests\n",
    "• goals of fixing the device or preferences between repairing or replacing\n",
    "components, or a patient’s preferences between living longer or reducing\n",
    "pain\n",
    "• stimuli that are observations of symptoms of a device or patient.\n",
    "The output of the diagnostic assistant is in terms of recommendations of treatments and tests, along with a rationale for its recommendations.\n",
    "1.4.3 A Tutoring Agent\n",
    "A tutoring agent tutors students in some domain of study. The environment\n",
    "of the agent includes students who interact through a computer or tablet interface, and perhaps the students’ parents and teachers.\n",
    "Example 1.5 Consider a tutoring agent to teach elementary physics, such as\n",
    "mechanics, that interacts with a student. In order to successfully tutor a student, the agent needs to be able to solve problems in the physics domain, determine the student’s knowledge and misunderstanding based on interacting\n",
    "with them, and converse using natural language, mathematics, and diagrams.\n",
    "In terms of the black-box definition of an agent in Figure 1.4 (page 15), a\n",
    "tutoring agent has the following as inputs:\n",
    "• prior knowledge, provided by the agent designer, about the subject matter\n",
    "being taught, teaching strategies, possible student errors and misconceptions.\n",
    "• past experience, which the tutoring agent has acquired by interacting with\n",
    "students, such as, what errors students make, how many examples and\n",
    "problems it takes various students to learn various topics, and what students forget; this can be information about students in general as well as\n",
    "about a particular student.\n",
    "• preferences about the importance of each topic, the level of achievement\n",
    "of the student that is desired, and the importance given to student motivation and engagement; there are often complex trade-offs among these.\n",
    "• stimuli include observations of a student’s test results and observations of\n",
    "the student’s interaction (or non-interaction) with the agent; students can\n",
    "also ask questions or request help on new examples and problems.\n",
    "The actions of the tutoring agent include presenting the theory and workedout examples, proposing suitable problems, providing help and feedback on\n",
    "a student’s solution, asking the student questions, answering their questions,\n",
    "and producing reports for parents and teachers.\n",
    "20 1. Artificial Intelligence and Agents\n",
    "1.4.4 A Trading Agent\n",
    "A trading agent is like a robot, but instead of interacting with a physical environment, it interacts with an information environment. Its task is to procure goods and services for a user. It must be able to be told the needs of a\n",
    "user, and it must interact with sellers (e.g., on the Web). The simplest trading\n",
    "agent involves proxy bidding for a user on an auction site, where the system\n",
    "will keep bidding until the user’s price limit is reached. A more complicated\n",
    "trading agent will buy multiple complementary items, like booking a flight, a\n",
    "hotel, and a rental car that fit together, in addition to trading off competing\n",
    "preferences of the user. Web services provide tools on the Web designed to\n",
    "be combined by trading agents. Another example of a trading agent is one\n",
    "that monitors how much food and groceries are in a household, monitors the\n",
    "prices, and orders goods before they are needed, while trying to keep costs to\n",
    "a minimum.\n",
    "In terms of the black-box definition of an agent in Figure 1.4 (page 15), the\n",
    "trading agent has as inputs:\n",
    "• prior knowledge about types of goods and services, selling practices, and\n",
    "how auctions work\n",
    "• past experience about where is the best place to look for specials, how\n",
    "prices vary with time in an auction, and when specials tend to turn up\n",
    "• preferences in terms of what the user wants and how to trade off competing goals\n",
    "• stimuli including observations about what items are available, their price,\n",
    "and, perhaps, how long they are available.\n",
    "The output of the trading agent is either a recommendation the user can accept\n",
    "or reject, or an actual purchase.\n",
    "Because of the personalized nature of the trading agent, it should be able\n",
    "to do better than a generic purchaser that, for example, only offers packaged\n",
    "tours.\n",
    "1.4.5 Smart Home\n",
    "A smart home is a home that looks after itself and its inhabitants. It can be seen\n",
    "as a mix of the other applications.\n",
    "A smart home is an inside-out robot. It has physical sensors and actuators.\n",
    "It should be able to sense where people, pets, and objects are. It should be able\n",
    "to adjust lighting, sound, heat, etc., to suit the needs of its occupants, while\n",
    "reducing costs and minimizing environmental impacts. A smart home will\n",
    "not only have fixed sensors and actuators, but will be combined with mobile\n",
    "robots, and other actuators, such as arms on the kitchen walls to help with\n",
    "cooking, cleaning, and finding ingredients.\n",
    "A purchaser of a smart home may expect it to be able to clean floors, dishes,\n",
    "and clothes and to put things where they are kept. It is easy to clean a floor with\n",
    "1.5. Agent Design Space 21\n",
    "the assumption that everything small on the floor is garbage. It is much more\n",
    "difficult to know which of the small items are precious toys and which are\n",
    "junk that should be discarded, and this depends on the individual inhabitants\n",
    "and their age. Each person may have their own categorization of objects and\n",
    "where they are expected to be kept, which forces a smart home to adapt to the\n",
    "inhabitants.\n",
    "A smart home also must act as a diagnostician. When something goes\n",
    "wrong, it should be able to determine what is the problem and fix it. It should\n",
    "also be able to observe the inhabitants and determine if there is something\n",
    "wrong, such as someone has been injured or there is a burglary.\n",
    "Sometimes a smart home needs to act as a tutoring agent. It may have\n",
    "to teach the occupants how the appliances work, and how to interact with the\n",
    "home (e.g., what should an person expect to happen when they put their coffee\n",
    "cup on the vacuum cleaner). In order to do this, it has to take into account the\n",
    "knowledge and level of understanding of the person.\n",
    "A smart home may also need to act as a purchasing agent. The home should\n",
    "notice when items, such as toilet paper, soap, or essential foodstuffs, are running low and order more of them. Given a decision about what food each\n",
    "inhabitant wants, it should make sure the ingredients are in stock. It might\n",
    "even need to decide when inessential items, such as junk food, should be kept\n",
    "in stock. It also might need to decide when to discard perishable items, without\n",
    "creating too much waste or putting people’s health at risk.\n",
    "A smart home would include energy management. For example, with solar\n",
    "energy providing power during daylight hours, it could determine whether to\n",
    "store the energy locally or buy and sell energy on the smart grid. It could\n",
    "manage appliances to minimize the cost of energy, such as washing clothes\n",
    "when water and electricity are cheaper.\n",
    "1.5 Agent Design Space\n",
    "Agents acting in environments range in complexity from thermostats to companies with multiple goals acting in competitive environments. The ten dimensions of complexity in the design of intelligent agents below are designed\n",
    "to help us understand work that has been done, as well as the potential and\n",
    "limits of AI. These dimensions may be considered separately but must be combined to build an intelligent agent. These dimensions define a design space\n",
    "for AI; different points in this space are obtained by varying the values on each\n",
    "dimension.\n",
    "These dimensions give a coarse division of the design space for intelligent\n",
    "agents. There are many other design choices that must also be made to build\n",
    "an intelligent agent.\n",
    "22 1. Artificial Intelligence and Agents\n",
    "1.5.1 Modularity\n",
    "The first dimension is the level of modularity.\n",
    "Modularity is the extent to which a system can be decomposed into interacting modules that can be understood separately.\n",
    "Modularity is important for reducing complexity. It is apparent in the structure of the brain, serves as a foundation of computer science, and is an important aspect of any large organization.\n",
    "Modularity is typically expressed in terms of a hierarchical decomposition.\n",
    "In the modularity dimension, an agent’s structure is one of the following:\n",
    "• flat – there is no organizational structure\n",
    "• modular – the system is decomposed into interacting modules that can\n",
    "be understood on their own\n",
    "• hierarchical – the system is modular, and the modules themselves are decomposed into simpler modules, each of which are hierarchical systems\n",
    "or simple components.\n",
    "In a flat or modular structure the agent typically reasons at a single level of\n",
    "abstraction. In a hierarchical structure the agent reasons at multiple levels of\n",
    "abstraction. The lower levels of the hierarchy involve reasoning at a lower level\n",
    "of abstraction.\n",
    "Example 1.6 The delivery robot at the highest level has to plan its day, making\n",
    "sure it can deliver coffee on time, but still has time for longer trips and cleaning\n",
    "a room. At the lowest level, it needs to choose what motor controls to send to its\n",
    "wheels, and what movement its gripper should do. Even a task like picking up\n",
    "a glass involves many precise movements that need to be coordinated. Picking\n",
    "up a glass may be just one part of the larger task of cleaning part of a room.\n",
    "Cleaning the room might be one task that has to be scheduled into the robot’s\n",
    "day.\n",
    "In a flat representation, the agent chooses one level of abstraction and reasons at that level. A modular representation would divide the task into a number of subtasks that can be solved separately (e.g., pick up coffee, move from\n",
    "the corridor to lab B, put down coffee). In a hierarchical representation, the\n",
    "agent will solve these subtasks in a hierarchical way, until the task is reduced\n",
    "to simple tasks such a sending an http request or making a particular motor\n",
    "control.\n",
    "Example 1.7 A tutoring agent may have high-level teaching strategies, where\n",
    "it needs to decide which topics are taught and in what order. At a much lower\n",
    "level, it must design the details of concrete examples and specific questions for\n",
    "a test. At the lowest level it needs to combine words and lines in diagrams to\n",
    "express the examples and questions. Students can also be treated as learning in\n",
    "a hierarchical way, with detailed examples as well as higher-level concepts.\n",
    "1.5. Agent Design Space 23\n",
    "Example 1.8 For the trading agent, consider the task of making all of the arrangements and purchases for a custom holiday for a traveler. The agent should\n",
    "be able to make bookings for flights that fit together. Only when it knows where\n",
    "the traveller is staying and when, can it make more detailed arrangements such\n",
    "as dinner and event reservations.\n",
    "A hierarchical decomposition is important for reducing the complexity of\n",
    "building an intelligent agent that acts in a complex environment. Large organizations have a hierarchical organization so that the top-level decision makers\n",
    "are not overwhelmed by details and do not have to micromanage all activities\n",
    "of the organization. Procedural abstraction and object-oriented programming\n",
    "in computer science are designed to enable simplification of a system by exploiting modularity and abstraction. There is much evidence that biological\n",
    "systems are also hierarchical.\n",
    "To explore the other dimensions, initially ignore the hierarchical structure\n",
    "and assume a flat representation. Ignoring hierarchical decomposition is often\n",
    "fine for small or moderately sized tasks, as it is for simple animals, small organizations, or small to moderately sized computer programs. When tasks or\n",
    "systems become complex, some hierarchical organization is required.\n",
    "How to build hierarchically organized agents is discussed in Section 2.2\n",
    "(page 58).\n",
    "1.5.2 Planning Horizon\n",
    "The planning horizon dimension is how far ahead in time the agent plans. For\n",
    "example, consider a dog as an agent. When a dog is called to come, it should\n",
    "turn around to start running in order to get a reward in the future. It does not\n",
    "act only to get an immediate reward. Plausibly, a dog does not act for goals\n",
    "arbitrarily far in the future (e.g., in a few months), whereas people do (e.g.,\n",
    "working hard now to get a holiday next year).\n",
    "How far the agent “looks into the future” when deciding what to do is\n",
    "called the planning horizon. For completeness, let’s include the non-planning\n",
    "case where the agent is not reasoning in time. The time points considered by\n",
    "an agent when planning are called stages.\n",
    "In the planning horizon dimension, an agent is one of the following:\n",
    "• A non-planning agent is an agent that does not consider the future when\n",
    "it decides what to do or when time is not involved.\n",
    "• A finite horizon planner is an agent that looks for a fixed finite number\n",
    "of stages. For example, a doctor may have to treat a patient but may have\n",
    "time for a test and so there may be two stages to plan for: a testing stage\n",
    "and a treatment stage. In the simplest case, a greedy or myopic agent\n",
    "only looks one time step ahead.\n",
    "• An indefinite horizon planner is an agent that looks ahead some finite,\n",
    "but not predetermined, number of stages. For example, an agent that\n",
    "must get to some location may not know a priori how many steps it will\n",
    "24 1. Artificial Intelligence and Agents\n",
    "take to get there, but, when planning, it does not consider what it will do\n",
    "after it gets to the location.\n",
    "• An infinite horizon planner is an agent that plans on going on forever.\n",
    "This is often called a process. For example, the stabilization module of\n",
    "a legged robot should go on forever; it cannot stop when it has achieved\n",
    "stability, because the robot has to keep from falling over.\n",
    "The modules in a hierarchical decomposition may have different horizons,\n",
    "as in the following example.\n",
    "Example 1.9 For the delivery and helping agent, at the lowest level the module that keeps the robot stable, safe, and attentive to requests may be on an\n",
    "infinite horizon, assuming it is running forever. The task of delivering coffee to\n",
    "a particular person may be an indefinite horizon problem. Planning for a fixed\n",
    "number of hours may be a finite horizon problem.\n",
    "Example 1.10 In a tutoring agent, for some subtasks, a finite horizon may be\n",
    "appropriate, such as in a fixed teach, test, re-teach sequence. For other cases,\n",
    "there may be an indefinite horizon where the system may not know at design\n",
    "time how many steps it will take until the student has mastered some concept.\n",
    "It may also be possible to model teaching as an ongoing process of learning and\n",
    "testing with appropriate breaks, with no expectation of the system finishing.\n",
    "1.5.3 Representation\n",
    "The representation dimension concerns how the world is described.\n",
    "The different ways the world could be are called states. A state of the world\n",
    "specifies the agent’s internal state (its belief state) and the environment state.\n",
    "At the simplest level, an agent can reason explicitly in terms of individually\n",
    "identified states.\n",
    "Example 1.11 A thermostat for a heater may have two belief states: off and\n",
    "heating. The environment may have three states: cold, comfortable, and hot. There\n",
    "are thus six states corresponding to the different combinations of belief and\n",
    "environment states. These states may not fully describe the world, but they\n",
    "are adequate to describe what a thermostat should do. The thermostat should\n",
    "move to, or stay in, heating if the environment is cold and move to, or stay in,\n",
    "off if the environment is hot. If the environment is comfortable, the thermostat\n",
    "should stay in its current state. The thermostat agent turns or keeps the heater\n",
    "on in the heating state and turns or keeps the heater off in the off state.\n",
    "Instead of enumerating states, it is often easier to reason in terms of features\n",
    "of the state or propositions that are true or false of the state. A state may be\n",
    "described in terms of features, where a feature has a value in each state (see\n",
    "Section 4.1, page 127).\n",
    "1.5. Agent Design Space 25\n",
    "Example 1.12 Consider designing an agent to diagnose electrical problems\n",
    "in the home of Figure 1.6 (page 18). It may have features for the position of\n",
    "each switch, the status of each switch (whether it is working okay, whether it\n",
    "is shorted, or whether it is broken), and whether each light works. The feature\n",
    "position s2 may be a feature that has value up when switch s2 is up and has\n",
    "value down when the switch is down. The state of the home’s lighting may be\n",
    "described in terms of values for each of these features. These features depend\n",
    "on each other, but not in arbitrarily complex ways; for example, whether a light\n",
    "is on may just depend on whether it is okay, whether the switch is turned on,\n",
    "and whether there is electricity.\n",
    "A proposition is a Boolean feature, which means that its value is either true\n",
    "or false. Thirty propositions can encode 230 = 1, 073, 741, 824 states. It may be\n",
    "easier to specify and reason with the thirty propositions than with more than a\n",
    "billion states. Moreover, having a compact representation of the states indicates\n",
    "understanding, because it means that an agent has captured some regularities\n",
    "in the domain.\n",
    "Example 1.13 Consider an agent that has to recognize digits. Suppose the\n",
    "agent observes a binary image, a 28 × 28 grid of pixels, where each of the 282 =\n",
    "784 grid points is either black or white. The action is to determine which of the\n",
    "digits {0, . . . , 9} is shown in the image. There are 2784 different possible states\n",
    "of the image, and so 102784 different functions from the image state into the\n",
    "characters {a,..., z}. You cannot represent such functions in terms of the state\n",
    "space. Instead, handwriting recognition systems define features of the image,\n",
    "such as line segments, and define the function from images to characters in\n",
    "terms of these features. Modern implementations learn the features that are\n",
    "useful; see Example 8.3 (page 336).\n",
    "When describing a complex world, the features can depend on relations\n",
    "and individuals. An individual is also called a thing, an object, or an entity. A\n",
    "relation on a single individual is a property. There is a feature for each possible\n",
    "relationship among the individuals.\n",
    "Example 1.14 The agent that looks after a home in Example 1.12 could have\n",
    "the lights and switches as individuals, and relations position and connected to.\n",
    "Instead of the feature position s2 = up, it could use the relation position(s2, up).\n",
    "This relation enables the agent to reason about all switches or for an agent to\n",
    "have general knowledge about switches that can be used when the agent encounters a switch.\n",
    "Example 1.15 If an agent is enrolling students in courses, there could be a\n",
    "feature that gives the grade of a student in a course, for every student–course\n",
    "pair where the student took the course. There would be a passed feature for\n",
    "every student–course pair, which depends on the grade feature for that pair. It\n",
    "may be easier to reason in terms of individual students, courses, and grades,\n",
    "and the relations grade and passed. By defining how passed depends on grade\n",
    "26 1. Artificial Intelligence and Agents\n",
    "once, the agent can apply the definition for each student and course. Moreover,\n",
    "this can be done before the agent knows which individuals exist, and so before\n",
    "it knows any of the features.\n",
    "The two-argument relation passed, with 1000 students and 100 courses, can\n",
    "represent 1000 ∗ 100 = 100, 000 propositions and so 2100,000 states.\n",
    "By reasoning in terms of relations and individuals, an agent can reason\n",
    "about whole classes of individuals without ever enumerating the features or\n",
    "propositions, let alone the states. An agent may have to reason about infinite\n",
    "sets of individuals, such as the set of all numbers or the set of all sentences. To\n",
    "reason about an unbounded or infinite number of individuals, an agent cannot\n",
    "reason in terms of states or features; it must reason at the relational level.\n",
    "In the representation dimension, the agent reasons in terms of\n",
    "• states\n",
    "• features, or\n",
    "• individuals and relations (often called relational representations).\n",
    "Some of the frameworks will be developed in terms of states, some in terms of\n",
    "features, and some in terms of individuals and relations.\n",
    "Reasoning in terms of states is introduced in Chapter 3. Reasoning in terms\n",
    "of features is introduced in Chapter 4. Relational reasoning is considered starting from Chapter 15.\n",
    "1.5.4 Computational Limits\n",
    "Sometimes an agent can decide on its best action quickly enough for it to act.\n",
    "Often there are computational resource limits that prevent an agent from carrying out the best action. That is, the agent may not be able to find the best\n",
    "action quickly enough within its memory limitations to act while that action is\n",
    "still the best thing to do. For example, it may not be much use to take 10 minutes to derive what was the best thing to do 10 minutes ago, when the agent\n",
    "has to act now. Often, instead, an agent must trade off how long it takes to get\n",
    "a solution with how good the solution is; it may be better to find a reasonable\n",
    "solution quickly than to find a better solution later because the world will have\n",
    "changed during the computation.\n",
    "The computational limits dimension determines whether an agent has\n",
    "• perfect rationality, where an agent reasons about the best action without\n",
    "taking into account its limited computational resources, or\n",
    "• bounded rationality, where an agent decides on the best action that it\n",
    "can find given its computational limitations.\n",
    "Computational resource limits include computation time, memory, and numerical accuracy caused by computers not representing real numbers exactly.\n",
    "An anytime algorithm is an algorithm where the solution quality improves\n",
    "with time. In particular, it is one that can produce its current best solution at\n",
    "1.5. Agent Design Space 27\n",
    "any time, but given more time it could produce even better solutions. To ensure\n",
    "that the quality does not decrease, the agent can store the best solution found\n",
    "so far, and return that when asked for a solution. Although the solution quality\n",
    "may increase with time, waiting to act has a cost; it may be better for an agent\n",
    "to act before it has found what would be the best solution.\n",
    "Example 1.16 The delivery robot cannot think for a long time about how to\n",
    "avoid a person. There might be a best way to avoid the person and to achieve\n",
    "its other goals, however it might take time to determine that optimal path, and\n",
    "it might be better to act quickly and then recover from a non-optimal action. In\n",
    "the simplest case, a robot could just stop if it encounters a person, but even that\n",
    "is error prone as robots have momentum, so it cannot stop immediately and\n",
    "people behind may run into it if it stops suddenly.\n",
    "Example 1.17 Even a tutoring agent that can act at longer scales than a robot\n",
    "sometimes has to act quickly. When a student has completed a task and wants\n",
    "a new task, the agent needs to decide whether it should assign the student\n",
    "the best task it has found so far, or compute for longer, trying to find an even\n",
    "better task. As the student waits, they might become distracted, which might\n",
    "be worse than giving them a non-optimal task. The computer can be planning\n",
    "the next task when the student is working. Modern computers, as fast as they\n",
    "may be, cannot find optimal solutions to difficult problems quickly.\n",
    "Example 1.18 Figure 1.7 shows how the computation time of an anytime algorithm can affect the solution quality. The agent has to carry out an action but\n",
    "can do some computation to decide what to do. The absolute solution quality,\n",
    "0 1 2 3 4 5\n",
    "Value of Action\n",
    "Time of Action\n",
    "Figure 1.7: Solution quality as a function of time for an anytime algorithm. The\n",
    "meaning is described in Example 1.18\n",
    "28 1. Artificial Intelligence and Agents\n",
    "had the action been carried out at time zero, shown as the dashed line at the\n",
    "top, is improving as the agent takes time to reason. However, there is a penalty\n",
    "associated with taking time to act. In this figure, the penalty, shown as the dotted line at the bottom, is negative and proportional to the time taken before\n",
    "the agent acts. These two values can be added to get the discounted quality,\n",
    "the time-dependent value of computation; this is the solid line in the middle of\n",
    "the graph. For the example of Figure 1.7 (page 27), an agent should compute\n",
    "for about 2.5 time units, and then act, at which point the discounted quality\n",
    "achieves its maximum value. If the computation lasts for longer than 4.3 time\n",
    "units, the resulting discounted solution quality is worse than if the algorithm\n",
    "outputs the initial guess it can produce with virtually no computation. It is typical that the solution quality improves in jumps; when the current best solution\n",
    "changes, there is a jump in the quality. The penalty associated with waiting is\n",
    "rarely a straight line; it is typically a function of deadlines, which may not be\n",
    "known by the agent.\n",
    "To take into account bounded rationality, an agent must decide whether it\n",
    "should act or reason for longer. This is challenging because an agent typically\n",
    "does not know how much better off it would be if it only spent a little bit more\n",
    "time reasoning. Moreover, the time spent thinking about whether it should\n",
    "reason may detract from actually reasoning about the domain.\n",
    "1.5.5 Learning\n",
    "In some cases, a designer of an agent may have a good model of the agent and\n",
    "its environment. But often a designer does not have a good model, and so an\n",
    "agent should use data from its past experiences and other sources to help it\n",
    "decide what to do.\n",
    "The learning dimension determines whether\n",
    "• knowledge is given, or\n",
    "• knowledge is learned (from prior knowledge and data or past experience).\n",
    "Learning typically means finding the best model that fits the data. Sometimes this is as simple as tuning a fixed set of parameters, but it can also mean\n",
    "choosing the best representation out of a class of representations. Learning is\n",
    "a huge field in itself but does not stand in isolation from the rest of AI. There\n",
    "are many issues beyond fitting data, including how to incorporate background\n",
    "knowledge, what data to collect, how to represent the data and the resulting\n",
    "representations, what learning biases are appropriate, and how the learned\n",
    "knowledge can be used to affect how the agent acts.\n",
    "Learning is considered in Chapters 7, 8, 10, 13, and 17.\n",
    "Example 1.19 A robot has a great deal to learn, such as how slippery floors\n",
    "are as a function of their shininess, where each person hangs out at different\n",
    "1.5. Agent Design Space 29\n",
    "parts of the day, when they will ask for coffee, and which actions result in the\n",
    "highest rewards.\n",
    "Modern vision systems are trained to learn good features (such as lines and\n",
    "textures) on millions if not billions of images and videos. These features can\n",
    "be used to recognize objects and for other tasks, even if there have been few\n",
    "examples of the higher-level concepts. A robot might not have seen a baby\n",
    "crawling on a highway, or a particular mug, but should be able to deal with\n",
    "such situations.\n",
    "Example 1.20 Learning is fundamental to diagnosis. It is through learning\n",
    "and science that medical professionals understand the progression of diseases\n",
    "and how well treatments work or do not work. Diagnosis is a challenging domain for learning, because all patients are different, and each individual doctor’s experience is only with a few patients with any particular set of symptoms. Doctors also see a biased sample of the population; those who come to\n",
    "see them usually have unusual or painful symptoms. Drugs are not given to\n",
    "people randomly. You cannot learn the effect of treatment by observation alone,\n",
    "but need a causal model of the causes and effects; see Chapter 11 for details\n",
    "on building causal models. To overcome the limitations of learning from observations alone, drug companies spend billions of dollars doing randomized\n",
    "controlled trials in order to learn the efficacy of drugs.\n",
    "1.5.6 Uncertainty\n",
    "An agent could assume there is no uncertainty, or it could take uncertainty in\n",
    "the domain into consideration. Uncertainty is divided into two dimensions:\n",
    "one for uncertainty from sensing and one for uncertainty about the effects of\n",
    "actions.\n",
    "Sensing Uncertainty\n",
    "In some cases, an agent can observe the state of the world directly. For example,\n",
    "in some board games or on a factory floor, an agent may know exactly the state\n",
    "of the world. In many other cases, it may have some noisy perception of the\n",
    "state and the best it can do is to have a probability distribution over the set\n",
    "of possible states based on what it perceives. For example, given a patient’s\n",
    "symptoms, a medical doctor may not actually know which disease a patient\n",
    "has and may have only a probability distribution over the diseases the patient\n",
    "may have.\n",
    "The sensing uncertainty dimension concerns whether the agent can determine the state from the stimuli:\n",
    "• Fully observable means the agent knows the state of the world from the\n",
    "stimuli.\n",
    "30 1. Artificial Intelligence and Agents\n",
    "• Partially observable means the agent does not directly observe the state\n",
    "of the world. This occurs when many possible states can result in the\n",
    "same stimuli or when stimuli are misleading.\n",
    "Assuming the world is fully observable is a common simplifying assumption\n",
    "to keep reasoning tractable.\n",
    "Example 1.21 The delivery robot does not know exactly where it is, or what\n",
    "else there is, based on its limited sensors. Looking down a corridor does not\n",
    "provide enough information to know where it is or who is behind the doors.\n",
    "Knowing where it was a second ago will help determine where it is now, but\n",
    "even robots can get lost. It may not know where the person who requested\n",
    "coffee is. When it is introduced into a new environment, it may have much\n",
    "more uncertainty.\n",
    "Example 1.22 The tutoring agent cannot directly observe the knowledge of\n",
    "the student. All it has is some sensing input, based on questions the student\n",
    "asks or does not ask, facial expressions, distractedness, and test results. Even\n",
    "test results are very noisy, as a mistake may be due to distraction or test anxiety\n",
    "instead of lack of knowledge, and a correct answer might be due to a lucky\n",
    "guess instead of real understanding. Sometimes students make mistakes in\n",
    "testing situations they wouldn’t make at other times.\n",
    "Example 1.23 A trading agent does not know all available options and their\n",
    "availability, but must find out information that can become outdated quickly\n",
    "(e.g., if a hotel becomes booked up). A travel agent does not know whether a\n",
    "flight will be canceled or delayed, or whether the passenger’s luggage will be\n",
    "lost. This uncertainty means that the agent must plan for the unanticipated.\n",
    "Effect Uncertainty\n",
    "A model of the dynamics of the world is a model of how the world changes as\n",
    "a result of actions, including the case of how it changes if the action were to do\n",
    "nothing. In some cases an agent knows the effects of its action. That is, given\n",
    "a state and an action, the agent can accurately predict the state resulting from\n",
    "carrying out that action in that state. For example, a software agent interacting\n",
    "with the file system of a computer may be able to predict the effects of deleting\n",
    "a file given the state of the file system. However, in many cases, it is difficult\n",
    "to predict the effects of an action, and the best an agent can do is to have a\n",
    "probability distribution over the effects. For example, a teacher may not know\n",
    "the effects explaining a concept, even if the state of the students is known. At\n",
    "the other extreme, if the teacher has no inkling of the effect of its actions, there\n",
    "would be no reason to choose one action over another.\n",
    "The dynamics in the effect uncertainty dimension can be\n",
    "• deterministic when the state resulting from an action is determined by\n",
    "an action and the prior state, or\n",
    "1.5. Agent Design Space 31\n",
    "• stochastic when there is a probability distribution over the resulting states.\n",
    "Example 1.24 For the delivery robot, there can be uncertainty about the effects\n",
    "of an action, both at the low level, say due to slippage of the wheels, or at the\n",
    "high level because the agent might not know whether putting the coffee on a\n",
    "person’s desk succeeded in delivering coffee to the person. This may depend\n",
    "on the individual preferences of users.\n",
    "Example 1.25 Even a trading agent does not know the effect of putting in\n",
    "a trade order, such as booking a flight or a hotel room. These can become unavailable at very short notice (consider two trading agents trying to book the\n",
    "same room at the same time), or the price can vary.\n",
    "The effect dimension only makes sense when the world is fully observable.\n",
    "If the world is partially observable, a stochastic system can be modeled as a\n",
    "deterministic system where the effect of an action depends on unobserved features. It is a separate dimension because many of the frameworks developed\n",
    "are for the fully observable, stochastic action case.\n",
    "Planning with deterministic actions is considered in Chapter 6. Planning\n",
    "with stochastic actions is considered in Chapter 12.\n",
    "1.5.7 Preference\n",
    "Agents normally act to have better outcomes. The only reason to choose one\n",
    "action over another is because the preferred action leads to more desirable outcomes.\n",
    "An agent may have a simple goal, which is a proposition the agent wants\n",
    "to be true in a final state. For example, the goal of getting Sam coffee means\n",
    "the agent wants to reach a state where Sam has coffee. Other agents may have\n",
    "more complex preferences. For example, a medical doctor may be expected to\n",
    "take into account suffering, life expectancy, quality of life, monetary costs (for\n",
    "the patient, the doctor, and society), and the ability to justify decisions in case\n",
    "of a lawsuit. The doctor must trade these considerations off when they conflict,\n",
    "as they invariably do.\n",
    "The preference dimension considers whether the agent has goals or richer\n",
    "preferences:\n",
    "• A goal is either an achievement goal, which is a proposition to be true in\n",
    "some final state, or a maintenance goal, a proposition that must be true\n",
    "in all visited states. For example, the goals for a robot may be to deliver\n",
    "a cup of coffee and a banana to Sam, and not to make a mess or hurt\n",
    "anyone.\n",
    "• Complex preferences involve trade-offs among the desirability of various outcomes, perhaps at different times. An ordinal preference is where\n",
    "only the ordering of the preferences is important. A cardinal preference\n",
    "is where the magnitude of the values matters. For example, an ordinal\n",
    "32 1. Artificial Intelligence and Agents\n",
    "preference may be that Sam prefers cappuccino over black coffee and\n",
    "prefers black coffee over tea. A cardinal preference may give a tradeoff between the wait time and the type of beverage, and a mess versus\n",
    "taste trade-off, where Sam is prepared to put up with more mess in the\n",
    "preparation of the coffee if the taste of the coffee is exceptionally good.\n",
    "Example 1.26 The delivery robot could be given goals, such as “deliver coffee\n",
    "to Chris and make sure you always have power.” A more complex goal may be\n",
    "to “clean up the lab, and put everything where it belongs”, which can only be\n",
    "achieved to some degree. There can be complex preferences, such as “deliver\n",
    "mail when it arrives and service coffee requests as soon as possible, but it is\n",
    "more important to deliver messages marked as urgent, and Chris needs her\n",
    "coffee quickly when she asks for it.”\n",
    "Example 1.27 For the diagnostic assistant, the goal may be as simple as “fix\n",
    "what is wrong,” but often there are complex trade-offs involving costs, pain,\n",
    "life expectancy, and preferences related to the uncertainty that the diagnosis is\n",
    "correct and uncertainty as to efficacy and side-effects of the treatment. There is\n",
    "also a problem of whose preferences are to be taken into account; the patient,\n",
    "the doctor, the payer, and society may all have different preferences that must\n",
    "be reconciled.\n",
    "Example 1.28 Although it may be possible for the tutoring agent to have\n",
    "a simple goal such, as to teach some particular concept, it is more likely that\n",
    "complex preferences must be taken into account. One reason is that, with uncertainty, there may be no way to guarantee that the student knows the concept\n",
    "being taught; any method that tries to maximize the probability that the student knows a concept will be very annoying, because it will repeatedly teach\n",
    "and test if there is a slight chance that the student’s errors are due to misunderstanding as opposed to fatigue or boredom. More complex preferences would\n",
    "enable a trade-off among fully teaching a concept, boring the student, the time\n",
    "taken, and the amount of retesting. The student may also have a preference\n",
    "for a teaching style that could be taken into account. The student, the teacher,\n",
    "the parents, and future employers may have different preferences. The student\n",
    "may have incompatible preferences, for example, to not work hard and to get\n",
    "a good mark. If the teacher is optimizing student evaluations, it might both\n",
    "allow the student to not work hard, and also give good marks. But that might\n",
    "undermine the goal of the student actually learning something.\n",
    "Example 1.29 For a trading agent, preferences of users are typically in terms\n",
    "of functionality, not components. For example, typical computer buyers have\n",
    "no idea of what hardware to buy, but they know what functionality they want\n",
    "and they also want the flexibility to be able to use new software features that\n",
    "might not even exist yet. Similarly, in a travel domain, what activities a user\n",
    "wants may depend on the location. Users also may want the ability to participate in a local custom at their destination, even though they may not know\n",
    "what those customs are. Even a simple path-finding algorithm, such as Google\n",
    "1.5. Agent Design Space 33\n",
    "Maps, which, at the time of writing, assumes all users’ preferences are to minimize travel time, could take into account each individual user’s preferences\n",
    "for diverse views or avoiding going too close to where some particular relative\n",
    "lives.\n",
    "Goals are considered in Chapters 3 and 6. Complex preferences are considered in Chapter 12, and the following chapters.\n",
    "1.5.8 Number of Agents\n",
    "An agent reasoning about what it should do in an environment where it is the\n",
    "only agent is difficult enough. However, reasoning about what to do when\n",
    "there are other agents who are also reasoning is much more difficult. An agent\n",
    "in a multiagent setting may need to reason strategically about other agents;\n",
    "the other agents may act to trick or manipulate the agent or may be available\n",
    "to cooperate with the agent. With multiple agents, it is often optimal to act\n",
    "randomly because other agents can exploit deterministic strategies. Even when\n",
    "the agents are cooperating and have a common goal, the task of coordination\n",
    "and communication makes multiagent reasoning more challenging. However,\n",
    "many domains contain multiple agents and ignoring other agents’ strategic\n",
    "reasoning is not always the best way for an agent to reason.\n",
    "Taking the point of view of a single agent, the number of agents dimension\n",
    "considers whether the agent explicitly considers other agents:\n",
    "• Single agent reasoning means the agent assumes that there are no other\n",
    "agents in the environment or that all other agents are part of nature, and\n",
    "so are non-purposive. This is a reasonable assumption if there are no\n",
    "other agents or if the other agents are not going to change what they do\n",
    "based on the agent’s action.\n",
    "• Adversarial reasoning considers another agent, where when one agent\n",
    "wins, the other loses. This is sometimes called a two-player zero-sum\n",
    "game, as the payoffs for the agents (e.g., +1 for a win and −1 for a loss)\n",
    "sum to zero. This is a simpler case than allowing for arbitrary agents as\n",
    "there is no need to cooperate or otherwise coordinate.\n",
    "• Multiple agent reasoning (or multiagent reasoning) means the agent\n",
    "takes the reasoning of other agents into account. This occurs when there\n",
    "are other intelligent agents whose goals or preferences depend, in part,\n",
    "on what the agent does or if the agent must communicate with other\n",
    "agents. Agents may need to cooperate because coordinated actions can\n",
    "result in outcomes that are better for all agents than each agent considering the other agents as part of nature.\n",
    "Reasoning in the presence of other agents is much more difficult if the\n",
    "agents can act simultaneously or if the environment is only partially observable. Multiagent systems are considered in Chapter 14. Note that the adversarial case is separate as there are some methods that only work for that case.\n",
    "34 1. Artificial Intelligence and Agents\n",
    "Example 1.30 There can be multiple delivery robots, which can coordinate to\n",
    "deliver coffee and parcels more efficiently. They can compete for power outlets\n",
    "or for space to move. Only one might be able to go closest to the wall when\n",
    "turning a corner. There may also be children out to trick the robot, or pets that\n",
    "get in the way.\n",
    "When automated vehicles have to go on a highway, it may be much more\n",
    "efficient and safer for them to travel in a coordinated manner, say one centimeter apart in a convoy, than to travel three vehicle lengths apart. It is more\n",
    "efficient because they can reduce wind drag, and many more vehicles can fit on\n",
    "a highway. It is safer because the difference in speeds is small; if one vehicle\n",
    "slams on its brakes or has engine problems, the car that might crash into the\n",
    "back is going approximately the same speed.\n",
    "Example 1.31 A trading agent has to reason about other agents. In commerce,\n",
    "prices are governed by supply and demand; this means that it is important to\n",
    "reason about the other competing agents. This happens particularly in a world\n",
    "where many items are sold by auction. Such reasoning becomes particularly\n",
    "difficult when there are items that must complement each other, such as flights\n",
    "and hotel bookings, and items that can substitute for each other, such as bus\n",
    "transport or taxis. You don’t want to book the flights if there is no accommodation, or book accommodation if there are no flights.\n",
    "1.5.9 Interactivity\n",
    "In deciding what an agent will do, there are three aspects of computation that\n",
    "must be distinguished: (1) the design-time computation that goes into the design of the agent, carried out by the designer of the agent, not the agent itself;\n",
    "(2) the computation that the agent can do before it observes the world and\n",
    "needs to act; and (3) the computation that is done by the agent as it is acting.\n",
    "The interactivity dimension considers whether the agent does\n",
    "• only offline reasoning, where offline reasoning is the computation done\n",
    "by the agent before it has to act, and can include compilation, learning\n",
    "or finding solutions from every state the agent could find itself in; under\n",
    "this assumption, the agent can carry out simple fixed-cost computation\n",
    "while acting, sometimes even just looking up the action in a table\n",
    "• significant online reasoning, where online computation is the computation done by the agent between observing the environment and acting.\n",
    "An agent acting in the world usually does not have the luxury of having\n",
    "the world wait for it to consider the best option. However, offline reasoning,\n",
    "where the agent can reason about the best thing to do before having to act,\n",
    "is often a simplifying assumption. Online reasoning can include long-range\n",
    "strategic reasoning as well as determining how to react in a timely manner to\n",
    "the environment; see Chapter 2.\n",
    "1.5. Agent Design Space 35\n",
    "Example 1.32 A delivery robot may be able to compute a plan for its day\n",
    "offline, but then it needs to be able to adapt to changes, for example, when\n",
    "someone wants coffee early or something urgent needs to be delivered. It cannot plan for who it will meet and need to avoid in the corridors. It either needs\n",
    "to be able to anticipate and plan for all possible eventualities, or it needs to\n",
    "reason online when it finds something unexpected.\n",
    "Example 1.33 A tutoring agent can determine the general outline of what\n",
    "should be taught offline. But then it needs to be able to react to unexpected\n",
    "behavior online when it occurs. It is difficult to be able to anticipate all eventualities, and might be easier to deal with them online when it encounters them.\n",
    "1.5.10 Interaction of the Dimensions\n",
    "Figure 1.8 summarizes the dimensions of complexity.\n",
    "In terms of the dimensions of complexity, the simplest case for the robot is\n",
    "a flat system, represented in terms of states, with no uncertainty, with achievement goals, with no other agents, with given knowledge, and with perfect rationality. In this case, with an indefinite stage planning horizon, the problem\n",
    "of deciding what to do is reduced to the problem of finding a path in a graph\n",
    "of states. This is explored in Chapter 3.\n",
    "In going beyond the simplest cases, these dimensions cannot be considered\n",
    "independently because they interact in complex ways. Consider the following\n",
    "examples of the interactions.\n",
    "The representation dimension interacts with the modularity dimension in\n",
    "that some modules in a hierarchy may be simple enough to reason in terms of\n",
    "a finite set of states, whereas other levels of abstraction may require reasoning\n",
    "about individuals and relations. For example, in a delivery robot, a module\n",
    "Dimension Values\n",
    "Modularity flat, modular, hierarchical\n",
    "Planning horizon non-planning, finite stage,\n",
    "indefinite stage, infinite stage\n",
    "Representation states, features, relations\n",
    "Computational limits perfect rationality, bounded rationality\n",
    "Learning knowledge is given, knowledge is learned\n",
    "Sensing uncertainty fully observable, partially observable\n",
    "Effect uncertainty deterministic, stochastic\n",
    "Preference goals, complex preferences\n",
    "Number of agents single agent, adversaries, multiple agents\n",
    "Interactivity offline, online\n",
    "Figure 1.8: Dimensions of complexity\n",
    "36 1. Artificial Intelligence and Agents\n",
    "that maintains balance may only have a few states. A module that must prioritize the delivery of multiple parcels to multiple people may have to reason\n",
    "about multiple individuals (e.g., people, packages, and rooms) and the relations between them. At a higher level, a module that reasons about the activity\n",
    "over the day may only require a few states to cover the different phases of the\n",
    "day (e.g., there might be three states of the robot: busy, available for requests,\n",
    "and recharging).\n",
    "The planning horizon interacts with the modularity dimension. For example, at a high level, a dog may be getting an immediate reward when it comes\n",
    "and gets a treat. At the level of deciding where to place its paws, there may be\n",
    "a long time until it gets the reward, and so at this level it may have to plan for\n",
    "an indefinite stage.\n",
    "Sensing uncertainty probably has the greatest impact on the complexity of\n",
    "reasoning. It is much easier for an agent to reason when it knows the state of\n",
    "the world than when it does not.\n",
    "The uncertainty dimensions interact with the modularity dimension: at one\n",
    "level in a hierarchy, an action may be deterministic, whereas at another level, it\n",
    "may be stochastic. As an example, consider the result of flying to a particular\n",
    "overseas destination with a companion you are trying to impress. At one level\n",
    "you may know which country you are in. At a lower level, you may be quite\n",
    "lost and not know where you are on a map of the airport. At an even lower\n",
    "level responsible for maintaining balance, you may know where you are: you\n",
    "are standing on the ground. At the highest level, you may be very unsure\n",
    "whether you have impressed your companion.\n",
    "Preference models interact with uncertainty because an agent needs to trade\n",
    "off between satisfying a very desirable goal with low probability or a less desirable goal with a higher probability. This issue is explored in Section 12.1\n",
    "(page 518).\n",
    "Multiple agents can also be used for modularity; one way to design a single agent is to build multiple interacting agents that share a common goal of\n",
    "making the higher-level agent act intelligently. Some researchers, such as Minsky [1986], argue that intelligence is an emergent feature from a “society” of\n",
    "unintelligent agents.\n",
    "Learning is often cast in terms of learning with features – determining which\n",
    "feature values best predict the value of another feature. However, learning can\n",
    "also be carried out with individuals and relations. Learning with hierarchies,\n",
    "sometimes called deep learning, has enabled the learning of more complex\n",
    "concepts. Much work has been done on learning in partially observable domains, and learning with multiple agents. Each of these is challenging in its\n",
    "own right without considering interactions with multiple dimensions.\n",
    "The interactivity dimension interacts with the planning horizon dimension\n",
    "in that when the agent is reasoning and acting online, it also needs to reason\n",
    "about the long-term horizon. The interactivity dimension also interacts with\n",
    "the computational limits; even if an agent is reasoning offline, it cannot take\n",
    "hundreds of years to compute an answer. However, when it has to reason\n",
    "1.6. Designing Agents 37\n",
    "about what to do in, say, 1/10 of a second, it needs to be concerned about the\n",
    "time taken to reason, and the trade-off between thinking and acting.\n",
    "Two of these dimensions, modularity and bounded rationality, promise to\n",
    "make reasoning more efficient. Although they make the formalism more complicated, breaking the system into smaller components, and making the approximations needed to act in a timely fashion and within memory limitations,\n",
    "should help build more complex systems.\n",
    "1.6 Designing Agents\n",
    "Artificial agents are designed for particular tasks. Researchers have not yet\n",
    "got to the stage of designing an intelligent agent for the task of surviving and\n",
    "reproducing in a complex natural environment.\n",
    "1.6.1 Simplifying Environments and Simplifying Agents\n",
    "It is important to distinguish between the knowledge in the mind of an agent\n",
    "and the knowledge in the mind of the designer of the agent. Consider the\n",
    "extreme cases:\n",
    "• At one extreme is a highly specialized agent that works well in the environment for which it was designed, but is helpless outside of this niche.\n",
    "The designer may have done considerable work in building the agent,\n",
    "but the agent can be extremely specialized to operate well. An example\n",
    "is a traditional thermostat. It may be difficult to design a thermostat so\n",
    "that it turns on and off at exactly the right temperatures, but the thermostat itself does not have to do much computation. Another example\n",
    "is a car-painting robot that always paints the same parts in an automobile factory. There may be much design time or offline computation to\n",
    "get it to work perfectly, but the painting robot can paint parts with little\n",
    "online computation; it senses that there is a part in position, but then it\n",
    "carries out its predefined actions. These very specialized agents do not\n",
    "adapt well to different environments or to changing goals. The painting\n",
    "robot would not notice if a different sort of part were present and, even\n",
    "if it did, it would not know what to do with it. It would have to be redesigned or reprogrammed to paint different parts or to change into a\n",
    "sanding machine or a dog-washing machine.\n",
    "• At the other extreme is a very flexible agent that can survive in arbitrary\n",
    "environments and accept new tasks at run time. Simple biological agents\n",
    "such as insects can adapt to complex changing environments, but they\n",
    "cannot carry out arbitrary tasks. Designing an agent that can adapt to\n",
    "complex environments and changing goals is a major challenge. The\n",
    "agent will know much more about the particulars of a situation than the\n",
    "designer. Even biology has not produced many such agents. Humans\n",
    "38 1. Artificial Intelligence and Agents\n",
    "may be the only extant example, but even humans need time to adapt to\n",
    "new environments.\n",
    "Even if the flexible agent is our ultimate dream, researchers have to reach this\n",
    "goal via more mundane goals. Rather than building a universal agent, which\n",
    "can adapt to any environment and solve any task, researchers have been restricted to particular agents for particular environmental niches. The designer\n",
    "can exploit the structure of the particular niche and the agent does not have to\n",
    "reason about other possibilities.\n",
    "Two broad strategies have been pursued in building agents:\n",
    "• The first is to simplify environments and build complex reasoning systems for these simple environments. For example, factory robots can do\n",
    "sophisticated tasks in the engineered environment of a factory, but they\n",
    "may be hopeless in a natural environment. Much of the complexity of\n",
    "the task can be reduced by simplifying the environment. This is also important for building practical systems because many environments can\n",
    "be engineered to make them simpler for agents.\n",
    "• The second strategy is to build simple agents in natural environments.\n",
    "This is inspired by seeing how insects can survive in complex environments even though they have very limited reasoning abilities. Modern\n",
    "language systems can predict the probability of the next word in an arbitrary text, but this does not mean they can be used for decision making.\n",
    "Researchers then make the agents have more reasoning abilities as their\n",
    "tasks become more complicated.\n",
    "One of the advantages of simplifying environments is that it may enable us to\n",
    "prove properties of agents or to optimize agents for particular situations. Proving properties or optimization typically requires a model of the agent and its\n",
    "environment. The agent may do a little or a lot of reasoning, but an observer or\n",
    "designer of the agent may be able to reason about the agent and the environment. For example, the designer may be able to prove whether the agent can\n",
    "achieve a goal, whether it can avoid getting into situations that may be bad for\n",
    "the agent (safety), whether it can avoid getting stuck somewhere (liveness),\n",
    "or whether it will eventually get around to each of the things it should do\n",
    "(fairness). Of course, the proof is only as good as the model.\n",
    "The advantage of building agents for complex environments is that these\n",
    "are the types of environments in which humans live and where agents could\n",
    "be useful.\n",
    "Even natural environments can be abstracted into simpler environments.\n",
    "For example, for an autonomous car driving on public roads the environment\n",
    "can be conceptually simplified so that everything is either a road, another car,\n",
    "or something to be avoided. Although autonomous cars have sophisticated\n",
    "sensors, they only have limited actions available, namely steering, accelerating,\n",
    "and braking.\n",
    "Fortunately, research along both lines, and between these extremes, is being\n",
    "carried out. In the first case, researchers start with simple environments and\n",
    "1.6. Designing Agents 39\n",
    "make the environments more complex. In the second case, researchers increase\n",
    "the complexity of the behaviors that the agents can carry out.\n",
    "1.6.2 Tasks\n",
    "One way that AI representations differ from computer programs in traditional\n",
    "languages is that an AI representation typically specifies what needs to be computed, not how it is to be computed. You might specify that the agent should\n",
    "find the most likely disease a patient has, or specify that a robot should get\n",
    "coffee, but not give detailed instructions on how to do these things. Much AI\n",
    "reasoning involves searching through the space of possibilities to determine\n",
    "how to complete a task.\n",
    "Typically, a task is only given informally, such as “deliver parcels promptly\n",
    "when they arrive” or “fix whatever is wrong with the electrical system of the\n",
    "home.”\n",
    "The general framework for solving tasks by computer is given in Figure 1.9.\n",
    "To solve a task, the designer of a system must:\n",
    "• determine what constitutes a solution\n",
    "• represent the task in a way a computer can reason about\n",
    "• use the computer to compute an output; either answers presented to a\n",
    "user or actions to be carried out in the environment\n",
    "• interpret the output as a solution to the task.\n",
    "In AI, knowledge is long-term representation of a domain whereas belief is\n",
    "about the immediate environment, for example where the agent is and where\n",
    "other object are. In philosophy, knowledge is usually defined as justified true\n",
    "belief, but in AI the term is used more generally to be any relatively stable information, as opposed to belief, which is more transitory information. The reason\n",
    "for this terminology is that it is difficult for an agent to determine truth, and\n",
    "“justified” is subjective. Knowledge in AI can be represented in terms of logic,\n",
    "task solution\n",
    "representation output\n",
    "solve\n",
    "compute\n",
    "represent interpret informal\n",
    "formal\n",
    "Figure 1.9: The role of representations in solving tasks\n",
    "40 1. Artificial Intelligence and Agents\n",
    "neural networks, or probabilistic models, but belief is typically represented as\n",
    "a distribution over the states.\n",
    "A representation of some piece of knowledge is the particular data structures used to encode the knowledge so it can be reasoned with.\n",
    "The form of representation – what is represented – is a compromise among\n",
    "many competing objectives. A representation should be:\n",
    "• rich enough to express the knowledge needed to solve the task\n",
    "• as close to a natural specification of the task as possible\n",
    "• amenable to efficient computation\n",
    "• able to be acquired from people, data, and past experiences.\n",
    "Being as close to a natural specification of the task as possible means should\n",
    "be compact, natural, and maintainable. It should be easy to see the relationship\n",
    "between the representation and the domain being represented, so that it is easy\n",
    "to determine whether the knowledge represented is correct; a small change\n",
    "in the task should result in a small change in the representation of the task.\n",
    "There is an active debate about how much of the internal structure of reasoning\n",
    "should be explainable; the field of explainable AI is about how to make more\n",
    "aspects of the decision making amenable to being explained to a person.\n",
    "Efficient computation enables the agent to act quickly enough to be effective. A tractable algorithm is one with reasonable asymptotic complexity, often meaning the computation time is polynomial in the input size (page 95),\n",
    "however often linear complexity is too slow. To ensure this, representations\n",
    "exploit features of the task for computational gain and trade off accuracy and\n",
    "computation time.\n",
    "Many different representation languages have been designed. Many of\n",
    "these start with some of these objectives and are then expanded to include\n",
    "the other objectives. For example, some are designed for learning, perhaps\n",
    "inspired by neurons, and then expanded to allow richer task-solving and inference abilities. Some representation languages are designed with expressiveness in mind, and then inference and learning are added on. Some language\n",
    "designers focus on tractability and enhance richness, naturalness, and the ability to be acquired.\n",
    "1.6.3 Defining a Solution\n",
    "Given an informal description of a task, before even considering a computer,\n",
    "an agent designer should determine what would constitute a solution. This\n",
    "question arises not only in AI but in any software design. Much of software\n",
    "engineering involves refining the specification of the task.\n",
    "Tasks are typically not well specified. Not only is there usually much left\n",
    "unspecified, but also the unspecified parts cannot be filled in arbitrarily. For\n",
    "example, if a user asks a trading agent to find out all the information about\n",
    "resorts that may have unsanitary food practices, they do not want the agent to\n",
    "return all the information about all resorts, even though all of the information\n",
    "1.6. Designing Agents 41\n",
    "requested is in the result. However, if the trading agent does not have complete\n",
    "knowledge about the resorts, returning all of the information may be the only\n",
    "way for it to guarantee that all of the requested information is there. Similarly,\n",
    "one does not want a delivery robot, when asked to take all of the trash to the\n",
    "garbage can, to take everything to the garbage can, even though this may be\n",
    "the only way to guarantee that all of the trash has been taken. Much work in\n",
    "AI is motivated by commonsense reasoning; the computer should be able to\n",
    "reach commonsense conclusions about the unstated assumptions.\n",
    "Given a well-defined task, the next issue is whether it matters if the answer\n",
    "returned is incorrect or incomplete. For example, if the specification asks for\n",
    "all instances, does it matter if some are missing? Does it matter if there are\n",
    "some extra instances? Often a person does not want just any solution but the\n",
    "best solution according to some criteria. There are four common classes of\n",
    "solutions:\n",
    "Optimal solution An optimal solution to a task is one that is the best solution\n",
    "according to some measure of solution quality. This measure is typically\n",
    "specified as an ordinal, where only the order matters. In some situations\n",
    "a cardinal measure, where the relative magnitudes also matter, is used.\n",
    "For example, a robot may need to take out as much trash as possible;\n",
    "the more trash it can take out, the better. In a more complex example,\n",
    "you may want the delivery robot to take as much of the trash as possible to the garbage can, minimizing the distance traveled, and explicitly\n",
    "specify a trade-off between the effort required and the proportion of the\n",
    "trash taken out. There are also costs associated with making mistakes\n",
    "and throwing out items that are not trash. It may be better to miss some\n",
    "trash than to waste too much time. One general measure of desirability\n",
    "that interacts with probability is utility (page 518).\n",
    "Satisficing solution Often an agent does not need the best solution to a task\n",
    "but just needs some solution. A satisficing solution is one that is good\n",
    "enough according to some description of which solutions are adequate.\n",
    "For example, a person may tell a robot that it must take all of the trash\n",
    "out, or tell it to take out three items of trash.\n",
    "Approximately optimal solution One of the advantages of a cardinal measure of success is that it allows for approximations. An approximately\n",
    "optimal solution is one whose measure of quality is close to the best that\n",
    "could theoretically be obtained. Typically, agents do not need optimal\n",
    "solutions to tasks; they only need to get close enough. For example, the\n",
    "robot may not need to travel the optimal distance to take out the trash\n",
    "but may only need to be within, say, 10% of the optimal distance. Some\n",
    "approximation algorithms guarantee that a solution is within some range\n",
    "of optimal, but for some algorithms no guarantees are available.\n",
    "For some tasks, it is much easier computationally to get an approximately optimal solution than to get an optimal solution. However, for\n",
    "other tasks, it is just as difficult to find an approximately optimal solution\n",
    "42 1. Artificial Intelligence and Agents\n",
    "that is guaranteed to be within some bounds of optimal as it is to find an\n",
    "optimal solution.\n",
    "Probable solution A probable solution is one that, even though it may not\n",
    "actually be a solution to the task, is likely to be a solution. This is one way\n",
    "to approximate, in a precise manner, a satisficing solution. For example,\n",
    "in the case where the delivery robot could drop the trash or fail to pick\n",
    "it up when it attempts to, you may need the robot to be 80% sure that\n",
    "it has picked up three items of trash. Often you want to distinguish a\n",
    "false-positive error (positive answers that are not correct) from a falsenegative error (negative answers that are correct). Some applications are\n",
    "much more tolerant of one of these types of errors than the other.\n",
    "These categories are not exclusive. A form of learning known as probably\n",
    "approximately correct (PAC) learning considers probably learning an approximately correct concept.\n",
    "1.6.4 Representations\n",
    "Once you have some requirements on the nature of a solution, you must represent the task so a computer can solve it.\n",
    "Computers and human minds are examples of physical symbol systems.\n",
    "A symbol is a meaningful pattern that can be manipulated. Examples of symbols are written words, sentences, gestures, marks on paper, or sequences of\n",
    "bits. A symbol system creates, copies, modifies, and destroys symbols. Essentially, a symbol is one of the patterns manipulated as a unit by a symbol system.\n",
    "The term “physical” is used, because symbols in a physical symbol system are\n",
    "physical objects that are part of the real world, even though they may be internal to computers and brains. They may also need to physically affect action or\n",
    "motor control.\n",
    "The physical symbol system hypothesis of Newell and Simon [1976] is\n",
    "that:\n",
    "A physical symbol system has the necessary and sufficient means\n",
    "for general intelligent action.\n",
    "This is a strong hypothesis. It means that any intelligent agent is necessarily a\n",
    "physical symbol system. It also means that a physical symbol system is all that\n",
    "is needed for intelligent action; there is no magic or as-yet-to-be-discovered\n",
    "quantum phenomenon required. It does not imply that a physical symbol system does not need a body to sense and act in the world.\n",
    "One aspect of this hypothesis is particularly controversial, namely whether\n",
    "symbols are needed at all levels. For example, consider recognizing a “cat” in\n",
    "a picture. At the top level is the symbol for a cat. At the bottom level are pixels\n",
    "from a camera. There are many intermediate levels that, for example, combine\n",
    "pixels to form lines and textures. These intermediate features are learned from\n",
    "1.6. Designing Agents 43\n",
    "data, and are not learned with the constraint that they are interpretable. Although some people have tried to interpret them, it is reasonable to say that\n",
    "these are not symbols. However, at a high level, they are either trained to be\n",
    "symbols (e.g., by learning a mapping between pixels and symbols, such as cat)\n",
    "or can be interpreted as symbols.\n",
    "An agent can use a physical symbol system to model the world. A model\n",
    "of a world is a representation of an agent’s beliefs about what is true in the\n",
    "world or how the world changes. The world does not have to be modeled at\n",
    "the most detailed level to be useful. All models are abstractions; they represent\n",
    "only part of the world and leave out many of the details. An agent can have\n",
    "a very simplistic model of the world, or it can have a very detailed model of\n",
    "the world. The level of abstraction provides a partial ordering of abstraction.\n",
    "A lower-level abstraction includes more details than a higher-level abstraction.\n",
    "An agent can have multiple, even contradictory, models of the world. Models\n",
    "are judged not by whether they are correct, but by whether they are useful.\n",
    "Example 1.34 A delivery robot can model the environment at a high level of\n",
    "abstraction in terms of rooms, corridors, doors, and obstacles, ignoring distances, its size, the steering angles needed, the slippage of the wheels, the\n",
    "weight of parcels, the details of obstacles, the political situation in Canada, and\n",
    "virtually everything else. The robot could model the environment at lower levels of abstraction by taking some of these details into account. Some of these\n",
    "details may be irrelevant for the successful implementation of the robot, but\n",
    "some may be crucial for the robot to succeed. For example, in some situations\n",
    "the size of the robot and the steering angles may be crucial for not getting stuck\n",
    "around a particular corner. In other situations, if the robot stays close to the\n",
    "center of the corridor, it may not need to model its width or the steering angles.\n",
    "Choosing an appropriate level of abstraction is difficult for the following\n",
    "reasons:\n",
    "• A high-level description is easier for a human to specify and understand.\n",
    "• A low-level description can be more accurate and more predictive. Often,\n",
    "high-level descriptions abstract away details that may be important for\n",
    "actually solving the task.\n",
    "• The lower the level, the more difficult it is to reason with. This is because\n",
    "a solution at a lower level of detail involves more steps and many more\n",
    "possible courses of action exist from which to choose.\n",
    "• An agent may not know the information needed for a low-level description. For example, the delivery robot may not know what obstacles it will\n",
    "encounter or how slippery the floor will be at the time that it must decide\n",
    "what to do.\n",
    "It is often a good idea to model an environment at multiple levels of abstraction. This issue is discussed further in Section 2.2 (page 58).\n",
    "Biological systems, and computers, can be described at multiple levels of\n",
    "abstraction. At successively lower levels of animals are the neuronal level,\n",
    "44 1. Artificial Intelligence and Agents\n",
    "the biochemical level (what chemicals and what electrical potentials are being transmitted), the chemical level (what chemical reactions are being carried\n",
    "out), and the level of physics (in terms of forces on atoms and quantum phenomena). What levels above the neuronal level are needed to account for intelligence is still an open question. These levels of description are echoed in the\n",
    "hierarchical structure of science itself, where scientists are divided into physicists, chemists, biologists, psychologists, anthropologists, and so on. Although\n",
    "no level of description is more important than any other, it is plausible that you\n",
    "do not have to emulate every level of a human to build an AI agent but rather\n",
    "you can emulate the higher levels and build them on the foundation of modern\n",
    "computers. This conjecture is part of what AI studies.\n",
    "The following are two levels that seem to be common to both biological and\n",
    "computational entities:\n",
    "• The knowledge level is the level of abstraction that considers what an\n",
    "agent knows and believes and what its goals are. The knowledge level\n",
    "considers what an agent knows, but not how it reasons. For example, the\n",
    "delivery agent’s behavior can be described in terms of whether it knows\n",
    "that a parcel has arrived or not and whether it knows where a particular\n",
    "person is or not. Both human and robotic agents are describable at the\n",
    "knowledge level. At this level, you do not specify how the solution will\n",
    "be computed or even which of the many possible strategies available to\n",
    "the agent will be used.\n",
    "• The symbol level is a level of description of an agent in terms of the reasoning it does. To implement the knowledge level, an agent manipulates\n",
    "symbols to produce answers. Many cognitive science experiments are\n",
    "designed to determine what symbol manipulation occurs during reasoning. Whereas the knowledge level is about what the agent believes about\n",
    "the external world and what its goals are in terms of the outside world,\n",
    "the symbol level is about what goes on inside an agent to reason about\n",
    "the external world.\n",
    "1.7 Social Impact\n",
    "AI systems are now widely deployed in society. Individuals, corporations,\n",
    "governments, and other organizations are using AI for applications as varied as voice dictation, text synthesis, text-to-video generation, movie recommendations, personal finance, chatbots, credit scoring, screening employment\n",
    "applications, social media propagation and monitoring, face recognition, semiautonomous cars, and warehouse automation. Many of these systems can be\n",
    "broadly beneficial. However, there are often adverse impacts on people in\n",
    "racialized populations and underserved communities, and on election results\n",
    "and vaccination campaigns.\n",
    "There are significant ethical and social impacts of AI systems, leading to\n",
    "demands for human-centered AI that is explainable, transparent, and trust-\n",
    "1.8. Overview of the Book 45\n",
    "worthy. The inputs to an AI agent include the goals and preferences of the\n",
    "agent, but it is not clear whose preferences they are or should be.\n",
    "Each chapter concludes with a social impact section discussing issues directly relevant to that chapter’s topics. The social impact sections are of two\n",
    "types, sometimes containing both:\n",
    "• broader impacts of AI, which includes intended or unintended downstream consequences of upstream decisions on the design of the AI system or on the choice of data\n",
    "• use cases about user-facing applications of AI that have had an impact on\n",
    "society or science, either positive or negative.\n",
    "Chapter 18 on the social impact of AI considers the effects of AI on the\n",
    "digital economy, work and automation, transportation and sustainability. It\n",
    "highlights the roles of human-centered AI, values, bias, ethics, certification,\n",
    "and regulation.\n",
    "1.8 Overview of the Book\n",
    "The rest of the book explores the design space defined by the dimensions of\n",
    "complexity. It considers each dimension separately, where this can be done\n",
    "sensibly.\n",
    "Part I considers the big view of agents as a coherent vision of AI.\n",
    "Chapter 2 analyzes what is inside the black box of Figure 1.4 (page 15) and\n",
    "discusses the modular and hierarchical decomposition of intelligent agents.\n",
    "Part II considers the case of no uncertainty, which is a useful abstraction of\n",
    "many domains.\n",
    "Chapter 3 considers the simplest case of determining what to do in the case\n",
    "of a single agent that reasons with explicit states, no uncertainty, and has goals\n",
    "to be achieved, but with an indefinite horizon. In this case, the task of solving\n",
    "the goal can be abstracted to searching for a path in a graph. It is shown how\n",
    "extra knowledge of the domain can help the search.\n",
    "Chapters 4 and 5 show how to exploit features. In particular, Chapter 4\n",
    "considers how to find possible states given constraints on the assignments of\n",
    "values to features represented as variables. Chapter 5 presents reasoning with\n",
    "propositions in various forms.\n",
    "Chapter 6 considers the task of planning, in particular determining sequences of actions to solve a goal in deterministic domains.\n",
    "Part III considers learning and reasoning with uncertainty. In particular, it\n",
    "considers sensing uncertainty and effect uncertainty.\n",
    "Chapter 7 shows how an agent can learn from past experiences and data.\n",
    "It covers the most common case of learning, namely supervised learning with\n",
    "features, where a function from input features into target features is learned\n",
    "46 1. Artificial Intelligence and Agents\n",
    "from observational data. Chapter 8 studies neural networks and deep learning\n",
    "and how features themselves can be learned from sensory observation.\n",
    "Chapter 9 shows how to reason with uncertainty, in particular with probability and graphical models of independence. Chapter 10 introduces learning\n",
    "with uncertainty. Chapter 11 shows how to model causality and learn the effects of interventions (which cannot be learned from observation alone).\n",
    "Part IV considers planning and acting with uncertainty.\n",
    "Chapter 12 considers the task of planning with uncertainty. Chapter 13\n",
    "deals with reinforcement learning, where agents learn what to do. Chapter 14\n",
    "expands planning to deal with issues arising from multiple agents.\n",
    "Part V extends the state and feature-based representations to deal with relational representations, in terms of relations and individuals.\n",
    "Chapter 15 shows how to reason in terms of individuals and relations.\n",
    "Chapter 16 discusses how to enable semantic interoperability using knowledge\n",
    "graphs and ontologies. Chapter 17 shows how reasoning about individuals\n",
    "and relations can be combined with learning and probabilistic reasoning.\n",
    "Part VI steps back from the details and gives the big picture.\n",
    "In Chapter 18 on the social impact of AI, further ethical and social concerns\n",
    "are addressed, by considering various questions, such as: What are the effects,\n",
    "benefits, costs, and risks of deployed AI systems for society? What are the\n",
    "ethical, equity, and regulatory considerations involved in building intelligent\n",
    "agents? How can you ensure that AI systems are fair, transparent, explainable,\n",
    "and trustworthy? How can AI systems be human-centered? What is the impact\n",
    "on sustainability?\n",
    "Chapter 19 reviews the design space of AI and shows how the material presented can fit into that design space. It also considers some likely and possible\n",
    "future scenarios for the development of AI science and technology.\n",
    "1.9 Review\n",
    "The following are the main points you should have learned from this chapter:\n",
    "• Artificial intelligence is the study of computational agents that act intelligently.\n",
    "• An agent acts in an environment and only has access to its abilities, its\n",
    "prior knowledge, its history of stimuli, and its goals and preferences.\n",
    "• A physical symbol system manipulates symbols to determine what to do.\n",
    "• A designer of an intelligent agent should be concerned about modularity,\n",
    "how to describe the world, how far ahead to plan, uncertainty in both\n",
    "perception and the effects of actions, the structure of goals or preferences,\n",
    "other agents, how to learn from experience, how the agent can reason\n",
    "while interacting with the environment, and the fact that all real agents\n",
    "have limited computational resources.\n",
    "• To solve a task by computer, the computer must have an effective representation with which to reason.\n",
    "1.10. References and Further Reading 47\n",
    "• To know when it has solved a task, an agent must have a definition of\n",
    "what constitutes an adequate solution, such as whether it has to be optimal, approximately optimal, or almost always optimal, or whether a\n",
    "satisficing solution is adequate.\n",
    "• In choosing a representation, an agent designer should find a representation that is as close as possible to the task, so that it is easy to determine\n",
    "what is represented and so it can be checked for correctness and be able\n",
    "to be maintained. Often, users want an explanation of why they should\n",
    "believe the answer.\n",
    "• The social impacts, both beneficial and harmful, of pervasive AI applications are significant, leading to calls for ethical and human-centered AI,\n",
    "certification and regulation.\n",
    "1.10 References and Further Reading\n",
    "The ideas in this chapter have been derived from many sources. Here, we try to\n",
    "acknowledge those that are explicitly attributable to particular authors. Most\n",
    "of the other ideas are part of AI folklore; trying to attribute them to anyone\n",
    "would be impossible.\n",
    "Levesque [2012] provides an accessible account of how thinking can be\n",
    "seen in terms of computation. Haugeland [1997] contains a good collection\n",
    "of articles on the philosophy behind artificial intelligence, including that classic paper of Turing [1950] that proposes the Turing test. Grosz [2012] and Cohen [2005] discuss the Turing test from a more recent perspective. Winograd\n",
    "schemas are described by Levesque [2014]. Srivastava et al. [2022] provide a\n",
    "Beyond the Imitation Game benchmark (BIG-bench) consisting of 204 tasks designed to challenge modern learning systems. Grosz [2018] discusses research\n",
    "on what it takes to implement dialog, not just answering one-off questions.\n",
    "Zador et al. [2023] discuss an embodied Turing test, and the role of neuroscience in AI.\n",
    "Nilsson [2010] and Buchanan [2005] provide accessible histories of AI. Chrisley and Begeer [2000] present many classic papers on AI. Jordan [2019] and the\n",
    "associated commentaries discuss intelligence augmentation.\n",
    "For discussions on the foundations of AI and the breadth of research in\n",
    "AI, see Kirsh [1991a], Bobrow [1993], and the papers in the corresponding volumes, as well as Schank [1990] and Simon [1995]. The importance of knowledge in AI is discussed in Lenat and Feigenbaum [1991], Sowa [2000], Darwiche [2018], and Brachman and Levesque [2022b].\n",
    "The physical symbol system hypothesis was posited by Newell and Simon\n",
    "[1976]. Simon [1996] discusses the role of symbol systems in a multidisciplinary\n",
    "context. The distinctions between real, synthetic, and artificial intelligence are\n",
    "discussed by Haugeland [1985], who also provides useful introductory material on interpreted, automatic formal symbol systems and the Church–Turing\n",
    "thesis. Brooks [1990] and Winograd [1990] critique the symbol system hy-\n",
    "48 1. Artificial Intelligence and Agents\n",
    "pothesis. Nilsson [2007] evaluates the hypothesis in terms of such criticisms.\n",
    "Shoham [2016] and Marcus and Davis [2019] argue for the importance of symbolic knowledge representation in modern applications.\n",
    "The use of anytime algorithms is due to Horvitz [1989] and Boddy and\n",
    "Dean [1994]. See Dean and Wellman [1991], Zilberstein [1996], and Russell\n",
    "[1997] for introductions to bounded rationality.\n",
    "For overviews of cognitive science and the role that AI and other disciplines\n",
    "play in that field, see Gardner [1985], Posner [1989], and Stillings et al. [1987].\n",
    "Conati et al. [2002] describe a tutoring agent for elementary physics. du\n",
    "Boulay et al. [2023] overview modern tutoring agents. Wellman [2011] overviews\n",
    "research in trading agents. Sandholm [2007] describes how AI can be used for\n",
    "procurement of multiple goods with complex preferences.\n",
    "A number of AI texts are valuable as reference books complementary to this\n",
    "book, providing a different perspective on AI. In particular, Russell and Norvig\n",
    "[2020] give a more encyclopedic overview of AI. They provide an excellent\n",
    "complementary source for many of the topics covered in this book and also an\n",
    "outstanding review of the scientific literature, which we do not try to duplicate.\n",
    "The Association for the Advancement of Artificial Intelligence (AAAI) provides introductory material and news at their AI Topics website (https://aitopics.\n",
    "org/). AI Magazine, published by AAAI, often has excellent overview articles\n",
    "and descriptions of particular applications. IEEE Intelligent Systems also provides accessible articles on AI research.\n",
    "There are many journals that provide in-depth research contributions and\n",
    "conferences where the most up-to-date research is found. These include the\n",
    "journals Artificial Intelligence, the Journal of Artificial Intelligence Research, IEEE\n",
    "Transactions on Pattern Analysis and Machine Intelligence, and Computational Intelligence, as well as more specialized journals. Much of the cutting-edge research is published first in conferences. Those of most interest to a general\n",
    "audience are the International Joint Conference on Artificial Intelligence (IJCAI), the AAAI Annual Conference, the European Conference on AI (ECAI),\n",
    "the Pacific Rim International Conference on AI (PRICAI), various national conferences, and many specialized conferences, which are referred to in the relevant chapters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dac14ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullArgSpec(args=['self', 'separator', 'is_separator_regex'], varargs=None, varkw='kwargs', defaults=('\\n\\n', False), kwonlyargs=[], kwonlydefaults=None, annotations={'return': 'None', 'separator': 'str', 'is_separator_regex': 'bool', 'kwargs': 'Any'})\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getfullargspec(CharacterTextSplitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b055343",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator= '\\n',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "split_text = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2c19a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence and Agents\\nThe history of AI is a history of fantasies, possibilities, demonstrations,\\nand promise. Ever since Homer wrote of mechanical “tripods” waiting on\\nthe gods at dinner, imagined mechanical assistants have been a part of our\\nculture. However, only in the last half century have we, the AI community, been able to build experimental machines that test hypotheses about\\nthe mechanisms of thought and intelligent behavior and thereby demonstrate mechanisms that formerly existed only as theoretical possibilities.\\n– Bruce Buchanan [2005]\\nThis book is about artificial intelligence (AI), a field built on centuries of thought,\\nwhich has been a recognized discipline for over 60 years. As well as solving\\npractical tasks, AI provides tools to test hypotheses about the nature of thought\\nitself. Deep scientific and engineering problems have already been solved and\\nmany more are waiting to be solved. Many practical applications are currently',\n",
       " 'many more are waiting to be solved. Many practical applications are currently\\ndeployed and the potential exists for an almost unlimited number of future\\napplications. This book presents the principles that underlie intelligent computational agents.\\n1.1 What is Artificial Intelligence?\\nArtificial intelligence, or AI, is the field that studies the synthesis and analysis of\\ncomputational agents that act intelligently. Consider each part of this definition.\\nAn agent is something that acts in an environment; it does something.\\nAgents include worms, dogs, thermostats, airplanes, robots, humans, companies, and countries.\\nAn agent is judged solely by how it acts. Agents that have the same effect\\nin the world are equally good.\\n3\\n4 1. Artificial Intelligence and Agents\\nIntelligence is a matter of degree. The aspects that go into an agent acting\\nintelligently include\\n• what it does is appropriate for its circumstances, its goals, and its perceptual and computational limitations',\n",
       " '• it takes into account the short-term and long-term consequences of its\\nactions, including the effects on society and the environment\\n• it learns from experience\\n• it is flexible to changing environments and changing goals.\\nA computational agent is an agent whose decisions about its actions can be\\nexplained in terms of computation. That is, the decision can be broken down\\ninto primitive operations that can be implemented in a physical device. This\\ncomputation can take many forms. In humans, this computation is carried out\\nin “wetware”; in computers it is carried out in “hardware.” Although there are\\nsome agents that are arguably not computational, such as the wind and rain\\neroding a landscape, it is an open question whether all intelligent agents are\\ncomputational.\\nAll agents are limited. No agent is omniscient (all knowing) or omnipotent\\n(can do anything). Agents can only observe everything in very specialized and',\n",
       " '(can do anything). Agents can only observe everything in very specialized and\\nconstrained domains. Agents have finite memory. Agents in the real world do\\nnot have unlimited time to act.\\nThe central scientific goal of AI is to understand the principles that make\\nintelligent behavior possible in natural or artificial systems. This is done by\\n• the analysis of natural and artificial agents\\n• formulating and testing hypotheses about what it takes to construct intelligent agents\\n• designing, building, and experimenting with computational systems that\\nperform tasks commonly viewed as requiring intelligence.\\nAs part of science, researchers build empirical systems to test hypotheses or to\\nexplore the space of possible designs. These are distinct from applications that\\nare built to be useful for an application domain.\\nThe definition is not for intelligent thought. The role of thought is to affect\\naction and lead to more intelligent behavior.',\n",
       " 'action and lead to more intelligent behavior.\\nThe central engineering goal of AI is the design and synthesis of agents\\nthat act intelligently, which leads to useful artifacts.\\nBuilding general intelligence isn’t the only goal of AI researchers. The aim\\nof intelligence augmentation is to augment human intelligence and creativity.\\nA diagnostic agent helps medical practitioners make better decisions, a search\\nengine augments human memory, and natural language translation systems\\nhelp people communicate. AI systems are often in human-in-the-loop mode,\\nwhere humans and agents work together to solve problems. Sometimes the\\nactions of artificial agents are to give advice to a human. Sometimes humans\\ngive advice or feedback to artificial agents, particularly for cases where decisions are made quickly or repeatedly.\\n1.1. What is Artificial Intelligence? 5\\n1.1.1 Artificial and Natural Intelligence',\n",
       " '1.1. What is Artificial Intelligence? 5\\n1.1.1 Artificial and Natural Intelligence\\nArtificial intelligence (AI) is the established name for the field, but the term “artificial intelligence” is a source of much confusion because artificial intelligence\\nmay be interpreted as the opposite of real intelligence.\\nFor any phenomenon, you can distinguish real versus fake, where the fake\\nis non-real. You can also distinguish natural versus artificial. Natural means\\noccurring in nature and artificial means made by people.\\nExample 1.1 A tsunami is a large wave in an ocean. Natural tsunamis occur from time to time and are caused by earthquakes or landslides. You could\\nimagine an artificial tsunami that was made by people, for example, by exploding a bomb in the ocean, yet which is still a real tsunami. One could also\\nimagine fake tsunamis: either artificial, using computer graphics, or natural,\\nsuch as a mirage that looks like a tsunami but is not one.',\n",
       " 'such as a mirage that looks like a tsunami but is not one.\\nIt is arguable that intelligence is different: you cannot have fake intelligence.\\nIf an agent behaves intelligently, it is intelligent. It is only the external behavior\\nthat defines intelligence; acting intelligently is being intelligent. Thus, artificial\\nintelligence, if and when it is achieved, will be real intelligence created artificially.\\nThis idea of intelligence being defined by external behavior was the motivation for a test for intelligence designed by Turing [1950], which has become\\nknown as the Turing test. The Turing test consists of an imitation game where\\nan interrogator can ask a witness, via a text interface, any question. If the interrogator cannot distinguish the witness from a human, the witness must be\\nintelligent. Figure 1.1 shows a possible dialog that Turing suggested. An agent\\nthat is not really intelligent could not fake intelligence for arbitrary topics.',\n",
       " 'that is not really intelligent could not fake intelligence for arbitrary topics.\\nInterrogator: In the first line of your sonnet which reads “Shall I compare thee\\nto a summer’s day,” would not ”a spring day” do as well or better?\\nWitness: It wouldn’t scan.\\nInterrogator: How about “a winter’s day,” That would scan all right.\\nWitness: Yes, but nobody wants to be compared to a winter’s day.\\nInterrogator: Would you say Mr. Pickwick reminded you of Christmas?\\nWitness: In a way.\\nInterrogator: Yet Christmas is a winter’s day, and I do not think Mr. Pickwick\\nwould mind the comparison.\\nWitness: I don’t think you’re serious. By a winter’s day one means a typical\\nwinter’s day, rather than a special one like Christmas.\\nFigure 1.1: Part of Turing’s possible dialog for the Turing test\\n6 1. Artificial Intelligence and Agents\\nThere has been much debate about the usefulness of the Turing test. Unfortunately, although it may provide a test for how to recognize intelligence, it',\n",
       " 'does not provide a way to realize intelligence.\\nLevesque [2014] suggested a new form of question, a Winograd schema\\nafter the following example of Winograd [1972]:\\n• The city councilmen refused the demonstrators a permit because they\\nfeared violence. Who feared violence?\\n• The city councilmen refused the demonstrators a permit because they\\nadvocated violence. Who advocated violence?\\nThese two sentences only differ in one word – feared/advocated – but have the\\nopposite answer.\\nWinograd schemas have the property that (a) humans can easily disambiguate them and (b) there is no simple grammatical or statistical test that could\\ndisambiguate them. For example, the sentences above would not qualify if the\\nphrase “demonstrators feared violence” was much less or more likely than the\\nphrase “councilmen feared violence” independently of the context, and similarly with advocating.\\nExample 1.2 The following examples are due to Davis [2015]:',\n",
       " 'Example 1.2 The following examples are due to Davis [2015]:\\n• Steve follows Fred’s example in everything. He [admires/influences] him\\nhugely. Who [admires/influences] whom?\\n• The table won’t fit through the doorway because it is too [wide/narrow].\\nWhat is too [wide/narrow]?\\n• Grace was happy to trade me her sweater for my jacket. She thinks it looks\\n[great/dowdy] on her. What looks [great/dowdy] on Grace?\\n• Bill thinks that calling attention to himself was rude [to/of] Bert. Who\\ncalled attention to himself?\\nEach of these have their own reasons why one answer is preferred to the other.\\nA computer that can reliably answer these questions needs to know about all of\\nthese reasons, and arguably requires the ability to do commonsense reasoning.\\nCommon sense should also allow it to reject sentences such as “The doorway\\nwon’t fit through the chair because it is too narrow. What is too narrow?”.\\nFigure 1.2 (page 7) shows some answers provided by ChatGPT [OpenAI,',\n",
       " 'Figure 1.2 (page 7) shows some answers provided by ChatGPT [OpenAI,\\n2022], based on GPT-3 [Brown et al., 2020], one of the most capable large language models in 2022. ChatGPT gives a different answer each time it is called.\\nYou can decide whether it solves this Winograd schema. The technology behind GPT-3 and related models is described in Section 8.5 (page 350).\\nGrosz [2012], arguing that language is inherently social and connected to\\nhuman cooperation, suggested that a more appropriate test should involve\\npurposeful natural language, not language just for the purpose of tricking a\\nhuman. She suggested the question:\\n1.1. What is Artificial Intelligence? 7\\nHuman: The table won’t fit through the doorway because it is too narrow.\\nWhat is too narrow?\\nChatGPT: If a table won’t fit through a doorway because it is too narrow,\\nit means that the width of the table is greater than the width of the doorway.\\nIn other words, the doorway is not wide enough to allow the table to pass',\n",
       " 'In other words, the doorway is not wide enough to allow the table to pass\\nthrough it. This can be a problem if you are trying to move the table from one\\nroom to another and the only way to do so is through the doorway. In order\\nto solve this problem, you may need to remove the door or trim the table so\\nthat it fits through the doorway. Alternatively, you may need to find a different\\nroute to move the table, such as through a window or by taking it apart and\\nreassembling it in the other room.\\nHuman: The table won’t fit through the doorway because it is too wide.\\nWhat is too wide?\\nChatGPT: If a table won’t fit through a doorway because it is too wide, it\\nmeans that the width of the doorway is smaller than the width of the table.\\nIn other words, the table is too big to fit through the doorway. This can be\\na problem if you are trying to move the table from one room to another and\\nthe only way to do so is through the doorway. In order to solve this problem,',\n",
       " 'the only way to do so is through the doorway. In order to solve this problem,\\nyou may need to remove the door or trim the table so that it fits through the\\ndoorway. Alternatively, you may need to find a different route to move the\\ntable, such as through a window or by taking it apart and reassembling it in\\nthe other room.\\nHuman: The doorway won’t fit through the chair because it is too narrow.\\nWhat is too narrow?\\nChatGPT: ”Too narrow” means that the doorway is not wide enough to\\nfit through the chair. In this context, the doorway is the object that is being\\ncompared to the chair, and it is not wide enough to pass through the chair\\nwithout encountering some difficulty or obstruction. This could be because\\nthe doorway is physically smaller in width than the chair, or because there is\\nsome other physical feature of the chair that is preventing the doorway from',\n",
       " 'some other physical feature of the chair that is preventing the doorway from\\nfitting through it. For example, the chair might have armrests or other protuberances that make it difficult for the doorway to pass through. In general, the\\nphrase ”too narrow” is used to describe something that is not wide enough to\\nfit through or pass a particular opening or gap.\\nHuman: The doorway won’t fit through the chair because it is too narrow.\\nWhat is too narrow?\\nChatGPT: If a doorway is too narrow to fit through a chair, it means that the\\nwidth of the doorway is smaller than the width of the chair. This can happen if\\nthe chair is too wide to fit through the doorway, or if the doorway is not wide\\nenough to accommodate the chair. In either case, the chair will not be able to\\npass through the doorway because there is not enough space for it to fit.\\nFigure 1.2: ChatGPT answers to some Winograd schemas\\n8 1. Artificial Intelligence and Agents',\n",
       " 'Figure 1.2: ChatGPT answers to some Winograd schemas\\n8 1. Artificial Intelligence and Agents\\nIs it imaginable that a computer (agent) team member could behave, over\\nthe long term and in uncertain, dynamic environments, in such a way that\\npeople on the team will not notice it is not human?\\n– Barbara Grosz [2012]\\nAn equal member of the team needs to be trusted enough to act in the world\\nappropriately, know when to ask questions, and when to not act. This challenge also allows for incremental improvement; starting with simple group\\ninteractions before moving to complex ones.\\nInteracting in natural language is not the only aspect of intelligence. An\\nagent acting in an environment needs common sense, “the ability to make\\neffective use of ordinary, everyday, experiential knowledge in achieving ordinary, practical goals” [Brachman and Levesque, 2022b]. Here, knowledge\\nis used in a general way to mean any non-transient information in an agent.',\n",
       " 'is used in a general way to mean any non-transient information in an agent.\\nSuch knowledge is typically not stated in natural language; people do not state\\nwhat everyone knows. Some knowledge, such as how to ride a bike or recognize a face, cannot be effectively conveyed by natural language. Formalizing\\ncommon sense has a long history [McCarthy, 1958; Davis, 1990], including the\\ndevelopment of representations and actual commonsense knowledge.\\n1.1.2 Natural Intelligence\\nThe obvious naturally intelligent agent is the human being. Some people might\\nsay that worms, insects, or bacteria are intelligent, but more people would say\\nthat dogs, whales, or monkeys are intelligent (see Exercise 1.1 (page 48)). One\\nclass of intelligent agents that may be more intelligent than humans is the class\\nof organizations. Ant colonies are a prototypical example of organizations.\\nEach individual ant may not be very intelligent, but an ant colony can act more',\n",
       " 'Each individual ant may not be very intelligent, but an ant colony can act more\\nintelligently than any individual ant. The colony can discover food and exploit it very effectively, as well as adapt to changing circumstances. Corporations can be more intelligent than individual people. Companies develop,\\nmanufacture, and distribute products where the sum of the skills required is\\nmuch more than any individual could master. Modern computers, from lowlevel hardware to high-level software, are more complicated than any single\\nhuman can understand, yet they are manufactured daily by organizations of\\nhumans. Human society viewed as an agent is arguably the most intelligent\\nagent known.\\nIt is instructive to consider where human intelligence comes from. There\\nare three main sources:\\nBiology Humans have evolved into adaptable animals that can survive in various habitats.',\n",
       " 'Biology Humans have evolved into adaptable animals that can survive in various habitats.\\nCulture Culture provides not only language, but also useful tools, useful concepts, and the wisdom that is passed from parents and teachers to children.\\n1.2. A Brief History of Artificial Intelligence 9\\nLifelong learning Humans learn throughout their life and accumulate knowledge and skills.\\nThese sources interact in complex ways. Biological evolution has provided\\nstages of growth that allow for different learning at different stages of life.\\nBiology and culture have evolved together; humans can be helpless at birth,\\npresumably because of our culture of looking after infants. Culture interacts\\nstrongly with learning. A major part of lifelong learning is what people are\\ntaught by parents and teachers. Language, which is part of culture, provides\\ndistinctions in the world that are useful for learning.\\nWhen building an intelligent system, the designers have to decide which',\n",
       " 'When building an intelligent system, the designers have to decide which\\nof these sources of intelligence need to be programmed in, and which can be\\nlearned. It is very unlikely that anyone will be able to build an agent that starts\\nwith a clean slate and learns everything, particularly for non-repetitive tasks.\\nSimilarly, most interesting and useful intelligent agents learn to improve their\\nbehavior.\\n1.2 A Brief History of Artificial Intelligence\\nThroughout human history, people have used technology to model themselves.\\nThere is evidence of this from ancient China, Egypt, and Greece, bearing witness to the universality of this activity. Each new technology has, in its turn,\\nbeen exploited to build intelligent agents or models of mind. Clockwork, hydraulics, telephone switching systems, holograms, analog computers, and digital computers have all been proposed both as technological metaphors for intelligence and as mechanisms for modeling mind.',\n",
       " 'Hobbes (1588–1679), who has been described by Haugeland [1985, p. 85]\\nas the “Grandfather of AI,” espoused the position that thinking was symbolic\\nreasoning, like talking out loud or working out an answer with pen and paper. The idea of symbolic reasoning was further developed by Descartes (1596–\\n1650), Pascal (1623–1662), Spinoza (1632–1677), Leibniz (1646–1716), and others\\nwho were pioneers in the European philosophy of mind.\\nThe idea of symbolic operations became more concrete with the development of computers. Babbage (1792–1871) designed the first general-purpose\\ncomputer, the Analytical Engine. Leonardo Torres y Quevedo build a chess\\nplaying machine based on similar ideas in 1911 [Randell, 1982]. In the early\\npart of the twentieth century, there was much work done on understanding\\ncomputation. Several models of computation were proposed, including the\\nTuring machine by Alan Turing (1912–1954), a theoretical machine that writes',\n",
       " 'Turing machine by Alan Turing (1912–1954), a theoretical machine that writes\\nsymbols on an infinitely long tape, and the lambda calculus of Church (1903–\\n1995), which is a mathematical formalism for rewriting formulas. It can be\\nshown that these very different formalisms are equivalent in that any function\\ncomputable by one is computable by the others. This leads to the Church–\\nTuring thesis:\\n10 1. Artificial Intelligence and Agents\\nAny effectively computable function can be carried out on a Turing\\nmachine (and so also in the lambda calculus or any of the other\\nequivalent formalisms).\\nEffectively computable means following well-defined operations. In Turing’s\\nday, “computers” were people who followed well-defined steps; computers\\nas known today did not exist. This thesis says that all computation can be\\ncarried out on a Turing machine or one of the other equivalent computational\\nmachines. The Church–Turing thesis cannot be proved but it is a hypothesis',\n",
       " 'machines. The Church–Turing thesis cannot be proved but it is a hypothesis\\nthat has stood the test of time. No one has built a machine that has carried\\nout computation that cannot be computed by a Turing machine. There is no\\nevidence that people can compute functions that are not Turing computable.\\nThis provides an argument that computation is more than just a metaphor for\\nintelligence; reasoning is computation and computation can be carried out by\\na computer.\\nSome of the first applications of computers were AI programs. Samuel\\n[1959] built a checkers program in 1952 and implemented a program that learns\\nto play checkers in the late 1950s. His program beat the Connecticut state\\ncheckers champion in 1961. Wang [1960] implemented a program that proved\\nevery logic theorem (nearly 400) in Principia Mathematica [Whitehead and Russell, 1925, 1927]. Newell and Simon [1956] built a program, Logic Theorist,\\nthat discovers proofs in propositional logic.',\n",
       " 'that discovers proofs in propositional logic.\\nIn parallel, there was also much work on neural networks learning inspired\\nby how neurons work. McCulloch and Pitts [1943] showed how a simple\\nthresholding “formal neuron” could be the basis for a Turing-complete machine. Learning for artificial neural networks was first described by Minsky\\n[1952]. One of the early significant works was the perceptron of Rosenblatt\\n[1958]. The work on neural networks became less prominent for a number of\\nyears after the 1968 book by Minsky and Papert [1988], which argued that the\\nrepresentations learned were inadequate for intelligent action. Many technical\\nfoundations for neural networks were laid in the 1980s and 1990s [Rumelhart\\net al., 1986; Hochreiter and Schmidhuber, 1997; LeCun et al., 1998a]. Widespread\\nadoption followed the success by Krizhevsky et al. [2012] for ImageNet [Deng',\n",
       " 'adoption followed the success by Krizhevsky et al. [2012] for ImageNet [Deng\\net al., 2009], a dataset of over 3 million images labelled with over 5000 categories. Subsequent major advances include the introduction of generative adversarial networks (GANs) [Goodfellow et al., 2014] and transformers [Vaswani\\net al., 2017]. Neural networks in various forms are now the state of the art for\\npredictive models for large perceptual datasets, including images, video, and\\nspeech, as well as some tasks for text. They are also used for generative AI,\\nto generate images, text, code, molecules, and other structured output. See\\nChapter 8.\\nNeural networks are one of many machine learning tools used for making\\npredictions from data in modern applications. Other methods have been developed though the years, including decision trees [Breiman et al., 1984; Quinlan,\\n1993] and logistic regression, introduced by Verhulst in 1832 [Cramer, 2002].\\n1.2. A Brief History of Artificial Intelligence 11',\n",
       " '1.2. A Brief History of Artificial Intelligence 11\\nThese have diverse applications in many areas of science. Combining these algorithms leads to the state-of-the-art gradient-boosted trees [Friedman, 2001;\\nChen and Guestrin, 2016], which demonstrates the close interconnections between statistics and machine learning.\\nWhile useful, making predictions is not sufficient to determine what an\\nagent should do; an agent also needs to plan. Planning in AI was initially\\nbased on deterministic actions. Fikes and Nilsson [1971] used deterministic\\nactions to control a mobile robot. Planning under uncertainty has a long history. Markov decision processes (MDPs), the foundation for much of planning\\nunder uncertainty, and dynamic programming, a general way to solve them,\\nwere invented by Bellman [1957]. These were extended into decision-theoretic\\nplanning in the 1990’s [Boutilier et al., 1999]. Decision-theoretic planning with',\n",
       " 'planning in the 1990’s [Boutilier et al., 1999]. Decision-theoretic planning with\\nlearning is called reinforcement learning. The first reinforcement learning\\nprograms were due to Andreae [1963] and Michie [1963]. Major advances\\ncame with the inventions of temporal-difference learning [Sutton, 1988] and\\nQ-learning [Watkins and Dayan, 1992]. Work in reinforcement learning has\\nexploded, including superhuman performance in chess, Go and other games\\n[Silver et al., 2017].\\nPlanning requires representations. The need for representations was recognized early.\\nA computer program capable of acting intelligently in the world must have\\na general representation of the world in terms of which its inputs are interpreted. Designing such a program requires commitments about what\\nknowledge is and how it is obtained. . . .More specifically, we want a computer program that decides what to do by inferring in a formal language',\n",
       " 'that a certain strategy will achieve its assigned goal. This requires formalizing concepts of causality, ability, and knowledge.\\n– McCarthy and Hayes [1969]\\nMany of the early representations were ad hoc, such as frames [Minsky,\\n1975], like the schemas of Kant [1787], Bartlett [1932], and Piaget [1953]. Later\\nrepresentations were based on logic [Kowalski, 1979], with knowledge being\\ndefined in logic and efficient inference. This resulted in languages such as Prolog [Kowalski, 1988; Colmerauer and Roussel, 1996].\\nProbabilities were eschewed in AI, because of the number of parameters\\nrequired, until the breakthrough of Bayesian networks (belief networks) and\\ngraphical models [Pearl, 1988], which exploit conditional independence, and\\nform a basis for modeling causality. Combining first-order logic and probability is the topic of statistical relational AI [De Raedt et al., 2016].\\nThere has been a continual tension between how much knowledge is learned',\n",
       " 'There has been a continual tension between how much knowledge is learned\\nand how much is provided by human experts or is innate to an agent. It has\\nlong been recognized that learning is needed, and it is known that learning\\ncannot be achieved with data alone (page 315). During the 1970s and 1980s,\\n12 1. Artificial Intelligence and Agents\\nexpert systems came to prominence, where the aim was to capture the knowledge of an expert in some domain so that a computer could carry out expert\\ntasks. DENDRAL [Buchanan and Feigenbaum, 1978], developed from 1965 to\\n1983 in the field of organic chemistry, proposed plausible structures for new\\norganic compounds. MYCIN [Buchanan and Shortliffe, 1984], developed from\\n1972 to 1980, diagnosed infectious diseases of the blood, prescribed antimicrobial therapy, and explained its reasoning.\\nAn alternative approach, de-emphasizing explicit knowledge representations, emphasized situated embodied agents [Brooks, 1990; Mackworth, 2009].',\n",
       " 'The hypothesis is that intelligence emerges, in evolution and individual development, through ongoing interaction and coupling with a real environment.\\nDuring the 1960s and 1970s, natural language understanding systems were\\ndeveloped for limited domains. For example, the STUDENT program of Bobrow [1967] could solve high-school algebra tasks expressed in natural language. Winograd’s [1972] SHRDLU system could, using restricted natural\\nlanguage, discuss and carry out tasks in a simulated blocks world. CHAT80 [Warren and Pereira, 1982] could answer geographical questions placed to it\\nin natural language. Figure 1.3 (page 13) shows some questions that CHAT-80\\nanswered based on a database of facts about countries, rivers, and so on. These\\nsystems could only reason in very limited domains using restricted vocabulary and sentence structure. Interestingly, IBM’s Watson, which beat the world\\nchampion in the TV game show Jeopardy! in 2011, used a technique similar',\n",
       " 'champion in the TV game show Jeopardy! in 2011, used a technique similar\\nto CHAT-80 [Lally et al., 2012] for understanding questions; see Section 15.7\\n(page 674).\\nIn applications using language in the wild, such as speech recognition and\\ntranslation in phones, many technologies are combined, including neural networks; see Chapter 8. Large language models (page 364), trained on huge\\ndatasets, can be used to predict the next word in a text, enabling predictive\\nspelling and the creation of new text.\\n1.2.1 Relationship to Other Disciplines\\nAI is a very young discipline. Other disciplines as diverse as philosophy, neurobiology, evolutionary biology, psychology, economics, political science, sociology, anthropology, control engineering, statistics, and many more have been\\nstudying aspects of intelligence much longer.\\nThe science of AI could be described as “synthetic psychology,” “experimental philosophy,” or “computational epistemology” – epistemology is the',\n",
       " 'study of knowledge. AI can be seen as a way to study the nature of knowledge\\nand intelligence, but with more powerful experimental tools than were previously available. Instead of being able to observe only the external behavior of\\nintelligent systems, as philosophy, psychology, economics, and sociology have\\ntraditionally been able to do, AI researchers experiment with executable models of intelligent behavior. Most important, such models are open to inspection,\\nredesign, and experimentation in a complete and rigorous way. Modern com-\\n1.2. A Brief History of Artificial Intelligence 13\\nputers provide a way to construct the models about which philosophers have\\nonly been able to theorize. AI researchers can experiment with these models as opposed to just discussing their abstract properties. AI theories can be\\nempirically grounded in implementations. Sometimes simple agents exhibit',\n",
       " 'empirically grounded in implementations. Sometimes simple agents exhibit\\ncomplex behavior, and sometimes sophisticated, theoretically motivated algorithms don’t work in real-world domains, which would not be known without\\nimplementing the agents.\\nIt is instructive to consider an analogy between the development of flying machines over the past few centuries and the development of thinking\\nmachines over the past few decades. There are several ways to understand\\nflying. One is to dissect known flying animals and hypothesize their common structural features as necessary fundamental characteristics of any flying\\nagent. With this method, an examination of birds, bats, and insects would suggest that flying involves the flapping of wings made of some structure covered\\nwith feathers or a membrane. Furthermore, the hypothesis could be tested by\\nstrapping feathers to one’s arms, flapping, and jumping into the air, as Icarus',\n",
       " 'strapping feathers to one’s arms, flapping, and jumping into the air, as Icarus\\ndid. An alternative methodology is to try to understand the principles of flying\\nwithout restricting oneself to the natural occurrences of flying. This typically\\ninvolves the construction of artifacts that embody the hypothesized principles,\\nDoes Afghanistan border China?\\nWhat is the capital of Upper Volta?\\nWhich country’s capital is London?\\nWhich is the largest African country?\\nHow large is the smallest American country?\\nWhat is the ocean that borders African countries and that borders\\nAsian countries?\\nWhat are the capitals of the countries bordering the Baltic?\\nHow many countries does the Danube flow through?\\nWhat is the total area of countries south of the Equator and not in\\nAustralasia?\\nWhat is the average area of the countries in each continent?\\nIs there more than one country in each continent?\\nWhat are the countries from which a river flows into the Black Sea?',\n",
       " 'What are the countries from which a river flows into the Black Sea?\\nWhat are the continents no country in which contains more than two\\ncities whose population exceeds 1 million?\\nWhich country bordering the Mediterranean borders a country that\\nis bordered by a country whose population exceeds the population\\nof India?\\nWhich countries with a population exceeding 10 million border the\\nAtlantic?\\nFigure 1.3: Some questions CHAT-80 could answer\\n14 1. Artificial Intelligence and Agents\\neven if they do not behave like flying animals in any way except flying. This\\nsecond method has provided both useful tools – airplanes – and a better understanding of the principles underlying flying, namely aerodynamics. Birds\\nare still much better at flying though forests.\\nAI takes an approach analogous to that of aerodynamics. AI researchers\\nare interested in testing general hypotheses about the nature of intelligence\\nby building machines that are intelligent and that do not necessarily mimic',\n",
       " 'by building machines that are intelligent and that do not necessarily mimic\\nhumans or organizations. This also offers an approach to the question, “Can\\ncomputers really think?” by considering the analogous question, “Can airplanes really fly?”\\nAI is intimately linked with the discipline of computer science because the\\nstudy of computation is central to AI. It is essential to understand algorithms,\\ndata structures, and combinatorial complexity to build intelligent machines. It\\nis also surprising how much of computer science started as a spinoff from AI,\\nfrom timesharing to computer algebra systems.\\nFinally, AI can be seen as coming under the umbrella of cognitive science.\\nCognitive science links various disciplines that study cognition and reasoning, from psychology to linguistics to anthropology to neuroscience. AI distinguishes itself within cognitive science by providing tools to build intelligence',\n",
       " 'rather than just studying the external behavior of intelligent agents or dissecting the inner workings of intelligent systems.\\n1.3 Agents Situated in Environments\\nAI is about practical reasoning: reasoning in order to do something. A coupling\\nof perception, reasoning, and acting comprises an agent. An agent acts in an\\nenvironment. An agent’s environment often includes other agents. An agent\\ntogether with its environment is called a world.\\nAn agent could be, for example, a coupling of a computational engine with\\nphysical sensors and actuators, called a robot, where the environment is a\\nphysical setting. An autonomous agent is one that acts in the world without human intervention. A semi-autonomous agent acts with a human-inthe-loop who may provide perceptual information and carry out the task. An\\nagent could be a program that acts in a purely computational environment, a\\nsoftware agent, often called a bot.\\nFigure 1.4 (page 15) shows a black-box view of an agent in terms of its',\n",
       " 'Figure 1.4 (page 15) shows a black-box view of an agent in terms of its\\ninputs and outputs. At any time, what an agent does depends on:\\n• prior knowledge about the agent and the environment\\n• stimuli received from the environment, which can include observations\\nabout the environment (e.g., light, sound, keyboard commands, web requests) as well as actions that the environment imposes on the agent (e.g.,\\nbumping the agent)\\n• past experiences, including history of interaction with the environment\\n(its previous actions and stimuli) and other data, from which it can learn\\n1.3. Agents Situated in Environments 15\\n• goals that it must try to achieve or preferences over states of the world\\n• abilities, the primitive actions the agent is capable of carrying out.\\nInside the black box, an agent has a belief state that can encode beliefs\\nabout its environment, what it has learned, what it is trying to do, and what\\nit intends to do. An agent updates this internal state based on stimuli. It uses',\n",
       " 'it intends to do. An agent updates this internal state based on stimuli. It uses\\nthe belief state and stimuli to decide on its actions. Much of this book is about\\nwhat is inside this black box.\\nPurposive agents have preferences or goals. They prefer some states of the\\nworld to other states, and they act to try to achieve the states they prefer most.\\nThe non-purposive agents are grouped together and called nature. Whether\\nor not an agent is purposive is a modeling assumption that may, or may not,\\nbe appropriate. For example, for some applications it may be appropriate to\\nmodel a dog as purposive, such as drug-sniffing dogs, and for others it may\\nsuffice to model a dog as non-purposive, such as when they are just part of the\\nenvironment.\\nIf an agent does not have preferences, by definition it does not care what\\nworld state it ends up in, and so it does not matter to it what it does. The reason\\nto design an agent is to instill preferences in it – to make it prefer some world',\n",
       " 'to design an agent is to instill preferences in it – to make it prefer some world\\nstates and try to achieve them. An agent does not have to know its preferences\\nexplicitly. For example, a thermostat for a heater is an agent that senses the\\nworld and turns the heater either on or off. There are preferences embedded\\nin the thermostat, such as to keep the room at a pleasant temperature, even\\nthough the thermostat arguably does not know these are its preferences. The\\npreferences of an agent are often the preferences of the designer of the agent,\\nbut sometimes an agent can acquire goals and preferences at run time.\\nPrior Knowledge\\nEnvironment\\nStimuli\\nActions\\nPast Experiences\\nGoals/Preferences\\nAgent\\nAbilities\\nFigure 1.4: An agent interacting with an environment\\n16 1. Artificial Intelligence and Agents',\n",
       " 'Figure 1.4: An agent interacting with an environment\\n16 1. Artificial Intelligence and Agents\\nThis is an all-encompassing view of intelligent agents varying in complexity from a simple thermostat, to a diagnostic advising system whose perceptions and actions are mediated by human beings, to a team of mobile robots,\\nto society itself. An agent does not have access to anything else; anything that\\ndoes not affect one of these inputs cannot affect the agent’s action.\\n1.4 Prototypical Applications\\nAI applications are widespread and diverse and include medical diagnosis,\\nscheduling factory processes, robots for hazardous environments, game playing, autonomous cars, natural language translation systems, choosing advertisements, personal assistants, and tutoring agents. Rather than treating each\\napplication separately, we abstract the essential features of such applications to\\nbetter understand the principles behind intelligent reasoning and action.',\n",
       " 'better understand the principles behind intelligent reasoning and action.\\nFive main application domains are developed in examples throughout the\\nbook. Although the particular examples presented are simple – otherwise they\\nwould not fit into the book – the application domains are representative of the\\nrange of domains in which AI techniques can be, and are being, used.\\n1.4.1 An Autonomous Delivery and Helping Robot\\nImagine a robot with wheels and the ability to pick up, put down and manipulate objects. It has sensing capabilities allowing it to recognize objects and\\nto avoid obstacles. It can be given orders in natural language and obey them,\\nmaking reasonable choices about what to do when its goals conflict. Such a\\nrobot could deliver packages or coffee in an office environment, clean a home\\nand put things in their appropriate place, or help caregivers in a hospital. Embedded in a wheelchair, it could help disabled people. It should be useful as\\nwell as safe.',\n",
       " 'well as safe.\\nIn terms of the black-box characterization of an agent in Figure 1.4 (page 15),\\nthe autonomous delivery robot has as inputs:\\n• prior knowledge, provided by the agent designer, about the agent’s capabilities, what objects it may encounter and have to differentiate, what\\nrequests mean, and perhaps about its environment, such as a map\\n• past experience obtained while acting, for instance, about the effects of its\\nactions (and – hopefully limited – experiences of breaking objects), what\\nobjects are common in the world, and what requests to expect at different\\ntimes of the day\\n• goals in terms of what it should deliver and when, as well as preferences\\nspecifying trade-offs, such as when it must forgo one goal to pursue another, or the trade-off between acting quickly and acting safely\\n• stimuli about its environment from observations from input devices such\\nas cameras, sonar, touch, sound, laser range finders, or keyboards as well',\n",
       " 'as cameras, sonar, touch, sound, laser range finders, or keyboards as well\\nas stimuli such as the agent being forcibly moved or crashing.\\n1.4. Prototypical Applications 17\\nThe robot’s outputs are motor controls specifying how its wheels should turn,\\nwhere its limbs should move, and what it should do with its grippers. Other\\noutputs may include speech and a video display.\\nExample 1.3 Figure 1.5 depicts a typical laboratory environment for a delivery\\nrobot. This environment consists of four laboratories and many offices. In our\\nexamples, the robot can only push doors, and the directions of the doors in\\nthe diagram reflect the directions in which the robot can travel. Rooms require\\nkeys and those keys can be obtained from various sources. The robot must\\ndeliver parcels, beverages, and dishes from room to room. The environment\\nalso contains a stairway that is potentially hazardous to the robot.\\n1.4.2 A Diagnostic Assistant',\n",
       " 'also contains a stairway that is potentially hazardous to the robot.\\n1.4.2 A Diagnostic Assistant\\nA diagnostic assistant is intended to advise a human about some particular\\nsystem such as a medical patient, the electrical system in a home, or an automobile. The diagnostic assistant should advise about potential underlying\\nfaults or diseases, what tests to carry out, and what treatment to prescribe. To\\ngive such advice, the assistant requires a model of the system, including knowledge of potential causes, available tests, available treatments, and observations\\nof the system (which are often called symptoms).\\nstairs\\nlab A lab B\\nlab D lab C\\nr101 r103 r105 r107 r109 r111\\nr113\\nr115\\nr117\\nr131 r129 r127 r125 r123 r121 r119\\nmain\\noffice\\nFigure 1.5: A typical laboratory environment for the delivery robot. This shows\\nthe locations of the doors and which way they open.\\n18 1. Artificial Intelligence and Agents\\nTo be useful, the diagnostic assistant must provide added value, be easy for',\n",
       " 'To be useful, the diagnostic assistant must provide added value, be easy for\\na human to use, and not be more trouble than it is worth. A diagnostic assistant\\nconnected to the Internet can draw on expertise from throughout the world,\\nand its actions can be based on the most up-to-date research. However, it must\\nbe able to justify why the suggested diagnoses or actions are appropriate. Humans are, and should be, suspicious of computer systems that are opaque and\\nimpenetrable. When humans are responsible for what they do, even if their\\nactions are based on a computer system’s advice, the system needs to convince\\nthe human that the suggested actions are defensible.\\nExample 1.4 Figure 1.6 shows an electrical distribution system in a home.\\nIn this home, power comes into the home through circuit breakers and then it\\ngoes to power outlets or to lights through light switches. For example, light l1\\nis on if there is power coming into the home, if circuit breaker cb1 is on, and if',\n",
       " 'is on if there is power coming into the home, if circuit breaker cb1 is on, and if\\nswitches s1 and s2 are either both up or both down. This is the sort of model that\\nsomeone may have of the electrical power in the home, which they could use\\nto determine what is wrong given evidence about the position of the switches\\nand which lights are on and which are off. The diagnostic assistant is there to\\nhelp a resident or an electrician troubleshoot electrical problems.\\nIn terms of the black-box definition of an agent in Figure 1.4 (page 15), the\\ndiagnostic assistant has as inputs:\\n• prior knowledge, such as how switches and lights normally work, how\\ndiseases or malfunctions manifest themselves, what information tests provide, the effects of repairs or treatments, and how to find out information\\nw0\\ns2\\nw4\\ns1\\ns3\\nw1\\nw2\\nl\\n1\\nl\\n2\\ncb1\\nw3\\np1\\ncb2\\nw6\\np2\\noutside_power\\nw5\\nswitch\\ncircuit\\nbreaker\\nlight\\npower\\noutlet\\nFigure 1.6: An electrical environment for the diagnostic assistant',\n",
       " 'breaker\\nlight\\npower\\noutlet\\nFigure 1.6: An electrical environment for the diagnostic assistant\\n1.4. Prototypical Applications 19\\n• past experience, in terms of data of previous cases that include the effects\\nof repairs or treatments, the prevalence of faults or diseases, the prevalence of symptoms for these faults or diseases, and the accuracy of tests\\n• goals of fixing the device or preferences between repairing or replacing\\ncomponents, or a patient’s preferences between living longer or reducing\\npain\\n• stimuli that are observations of symptoms of a device or patient.\\nThe output of the diagnostic assistant is in terms of recommendations of treatments and tests, along with a rationale for its recommendations.\\n1.4.3 A Tutoring Agent\\nA tutoring agent tutors students in some domain of study. The environment\\nof the agent includes students who interact through a computer or tablet interface, and perhaps the students’ parents and teachers.',\n",
       " 'Example 1.5 Consider a tutoring agent to teach elementary physics, such as\\nmechanics, that interacts with a student. In order to successfully tutor a student, the agent needs to be able to solve problems in the physics domain, determine the student’s knowledge and misunderstanding based on interacting\\nwith them, and converse using natural language, mathematics, and diagrams.\\nIn terms of the black-box definition of an agent in Figure 1.4 (page 15), a\\ntutoring agent has the following as inputs:\\n• prior knowledge, provided by the agent designer, about the subject matter\\nbeing taught, teaching strategies, possible student errors and misconceptions.\\n• past experience, which the tutoring agent has acquired by interacting with\\nstudents, such as, what errors students make, how many examples and\\nproblems it takes various students to learn various topics, and what students forget; this can be information about students in general as well as\\nabout a particular student.',\n",
       " 'about a particular student.\\n• preferences about the importance of each topic, the level of achievement\\nof the student that is desired, and the importance given to student motivation and engagement; there are often complex trade-offs among these.\\n• stimuli include observations of a student’s test results and observations of\\nthe student’s interaction (or non-interaction) with the agent; students can\\nalso ask questions or request help on new examples and problems.\\nThe actions of the tutoring agent include presenting the theory and workedout examples, proposing suitable problems, providing help and feedback on\\na student’s solution, asking the student questions, answering their questions,\\nand producing reports for parents and teachers.\\n20 1. Artificial Intelligence and Agents\\n1.4.4 A Trading Agent',\n",
       " '20 1. Artificial Intelligence and Agents\\n1.4.4 A Trading Agent\\nA trading agent is like a robot, but instead of interacting with a physical environment, it interacts with an information environment. Its task is to procure goods and services for a user. It must be able to be told the needs of a\\nuser, and it must interact with sellers (e.g., on the Web). The simplest trading\\nagent involves proxy bidding for a user on an auction site, where the system\\nwill keep bidding until the user’s price limit is reached. A more complicated\\ntrading agent will buy multiple complementary items, like booking a flight, a\\nhotel, and a rental car that fit together, in addition to trading off competing\\npreferences of the user. Web services provide tools on the Web designed to\\nbe combined by trading agents. Another example of a trading agent is one\\nthat monitors how much food and groceries are in a household, monitors the\\nprices, and orders goods before they are needed, while trying to keep costs to\\na minimum.',\n",
       " 'prices, and orders goods before they are needed, while trying to keep costs to\\na minimum.\\nIn terms of the black-box definition of an agent in Figure 1.4 (page 15), the\\ntrading agent has as inputs:\\n• prior knowledge about types of goods and services, selling practices, and\\nhow auctions work\\n• past experience about where is the best place to look for specials, how\\nprices vary with time in an auction, and when specials tend to turn up\\n• preferences in terms of what the user wants and how to trade off competing goals\\n• stimuli including observations about what items are available, their price,\\nand, perhaps, how long they are available.\\nThe output of the trading agent is either a recommendation the user can accept\\nor reject, or an actual purchase.\\nBecause of the personalized nature of the trading agent, it should be able\\nto do better than a generic purchaser that, for example, only offers packaged\\ntours.\\n1.4.5 Smart Home',\n",
       " 'tours.\\n1.4.5 Smart Home\\nA smart home is a home that looks after itself and its inhabitants. It can be seen\\nas a mix of the other applications.\\nA smart home is an inside-out robot. It has physical sensors and actuators.\\nIt should be able to sense where people, pets, and objects are. It should be able\\nto adjust lighting, sound, heat, etc., to suit the needs of its occupants, while\\nreducing costs and minimizing environmental impacts. A smart home will\\nnot only have fixed sensors and actuators, but will be combined with mobile\\nrobots, and other actuators, such as arms on the kitchen walls to help with\\ncooking, cleaning, and finding ingredients.\\nA purchaser of a smart home may expect it to be able to clean floors, dishes,\\nand clothes and to put things where they are kept. It is easy to clean a floor with\\n1.5. Agent Design Space 21\\nthe assumption that everything small on the floor is garbage. It is much more\\ndifficult to know which of the small items are precious toys and which are',\n",
       " 'difficult to know which of the small items are precious toys and which are\\njunk that should be discarded, and this depends on the individual inhabitants\\nand their age. Each person may have their own categorization of objects and\\nwhere they are expected to be kept, which forces a smart home to adapt to the\\ninhabitants.\\nA smart home also must act as a diagnostician. When something goes\\nwrong, it should be able to determine what is the problem and fix it. It should\\nalso be able to observe the inhabitants and determine if there is something\\nwrong, such as someone has been injured or there is a burglary.\\nSometimes a smart home needs to act as a tutoring agent. It may have\\nto teach the occupants how the appliances work, and how to interact with the\\nhome (e.g., what should an person expect to happen when they put their coffee\\ncup on the vacuum cleaner). In order to do this, it has to take into account the\\nknowledge and level of understanding of the person.',\n",
       " 'knowledge and level of understanding of the person.\\nA smart home may also need to act as a purchasing agent. The home should\\nnotice when items, such as toilet paper, soap, or essential foodstuffs, are running low and order more of them. Given a decision about what food each\\ninhabitant wants, it should make sure the ingredients are in stock. It might\\neven need to decide when inessential items, such as junk food, should be kept\\nin stock. It also might need to decide when to discard perishable items, without\\ncreating too much waste or putting people’s health at risk.\\nA smart home would include energy management. For example, with solar\\nenergy providing power during daylight hours, it could determine whether to\\nstore the energy locally or buy and sell energy on the smart grid. It could\\nmanage appliances to minimize the cost of energy, such as washing clothes\\nwhen water and electricity are cheaper.\\n1.5 Agent Design Space',\n",
       " 'when water and electricity are cheaper.\\n1.5 Agent Design Space\\nAgents acting in environments range in complexity from thermostats to companies with multiple goals acting in competitive environments. The ten dimensions of complexity in the design of intelligent agents below are designed\\nto help us understand work that has been done, as well as the potential and\\nlimits of AI. These dimensions may be considered separately but must be combined to build an intelligent agent. These dimensions define a design space\\nfor AI; different points in this space are obtained by varying the values on each\\ndimension.\\nThese dimensions give a coarse division of the design space for intelligent\\nagents. There are many other design choices that must also be made to build\\nan intelligent agent.\\n22 1. Artificial Intelligence and Agents\\n1.5.1 Modularity\\nThe first dimension is the level of modularity.',\n",
       " '1.5.1 Modularity\\nThe first dimension is the level of modularity.\\nModularity is the extent to which a system can be decomposed into interacting modules that can be understood separately.\\nModularity is important for reducing complexity. It is apparent in the structure of the brain, serves as a foundation of computer science, and is an important aspect of any large organization.\\nModularity is typically expressed in terms of a hierarchical decomposition.\\nIn the modularity dimension, an agent’s structure is one of the following:\\n• flat – there is no organizational structure\\n• modular – the system is decomposed into interacting modules that can\\nbe understood on their own\\n• hierarchical – the system is modular, and the modules themselves are decomposed into simpler modules, each of which are hierarchical systems\\nor simple components.\\nIn a flat or modular structure the agent typically reasons at a single level of\\nabstraction. In a hierarchical structure the agent reasons at multiple levels of',\n",
       " 'abstraction. In a hierarchical structure the agent reasons at multiple levels of\\nabstraction. The lower levels of the hierarchy involve reasoning at a lower level\\nof abstraction.\\nExample 1.6 The delivery robot at the highest level has to plan its day, making\\nsure it can deliver coffee on time, but still has time for longer trips and cleaning\\na room. At the lowest level, it needs to choose what motor controls to send to its\\nwheels, and what movement its gripper should do. Even a task like picking up\\na glass involves many precise movements that need to be coordinated. Picking\\nup a glass may be just one part of the larger task of cleaning part of a room.\\nCleaning the room might be one task that has to be scheduled into the robot’s\\nday.\\nIn a flat representation, the agent chooses one level of abstraction and reasons at that level. A modular representation would divide the task into a number of subtasks that can be solved separately (e.g., pick up coffee, move from',\n",
       " 'the corridor to lab B, put down coffee). In a hierarchical representation, the\\nagent will solve these subtasks in a hierarchical way, until the task is reduced\\nto simple tasks such a sending an http request or making a particular motor\\ncontrol.\\nExample 1.7 A tutoring agent may have high-level teaching strategies, where\\nit needs to decide which topics are taught and in what order. At a much lower\\nlevel, it must design the details of concrete examples and specific questions for\\na test. At the lowest level it needs to combine words and lines in diagrams to\\nexpress the examples and questions. Students can also be treated as learning in\\na hierarchical way, with detailed examples as well as higher-level concepts.\\n1.5. Agent Design Space 23\\nExample 1.8 For the trading agent, consider the task of making all of the arrangements and purchases for a custom holiday for a traveler. The agent should\\nbe able to make bookings for flights that fit together. Only when it knows where',\n",
       " 'be able to make bookings for flights that fit together. Only when it knows where\\nthe traveller is staying and when, can it make more detailed arrangements such\\nas dinner and event reservations.\\nA hierarchical decomposition is important for reducing the complexity of\\nbuilding an intelligent agent that acts in a complex environment. Large organizations have a hierarchical organization so that the top-level decision makers\\nare not overwhelmed by details and do not have to micromanage all activities\\nof the organization. Procedural abstraction and object-oriented programming\\nin computer science are designed to enable simplification of a system by exploiting modularity and abstraction. There is much evidence that biological\\nsystems are also hierarchical.\\nTo explore the other dimensions, initially ignore the hierarchical structure\\nand assume a flat representation. Ignoring hierarchical decomposition is often',\n",
       " 'and assume a flat representation. Ignoring hierarchical decomposition is often\\nfine for small or moderately sized tasks, as it is for simple animals, small organizations, or small to moderately sized computer programs. When tasks or\\nsystems become complex, some hierarchical organization is required.\\nHow to build hierarchically organized agents is discussed in Section 2.2\\n(page 58).\\n1.5.2 Planning Horizon\\nThe planning horizon dimension is how far ahead in time the agent plans. For\\nexample, consider a dog as an agent. When a dog is called to come, it should\\nturn around to start running in order to get a reward in the future. It does not\\nact only to get an immediate reward. Plausibly, a dog does not act for goals\\narbitrarily far in the future (e.g., in a few months), whereas people do (e.g.,\\nworking hard now to get a holiday next year).\\nHow far the agent “looks into the future” when deciding what to do is\\ncalled the planning horizon. For completeness, let’s include the non-planning',\n",
       " 'called the planning horizon. For completeness, let’s include the non-planning\\ncase where the agent is not reasoning in time. The time points considered by\\nan agent when planning are called stages.\\nIn the planning horizon dimension, an agent is one of the following:\\n• A non-planning agent is an agent that does not consider the future when\\nit decides what to do or when time is not involved.\\n• A finite horizon planner is an agent that looks for a fixed finite number\\nof stages. For example, a doctor may have to treat a patient but may have\\ntime for a test and so there may be two stages to plan for: a testing stage\\nand a treatment stage. In the simplest case, a greedy or myopic agent\\nonly looks one time step ahead.\\n• An indefinite horizon planner is an agent that looks ahead some finite,\\nbut not predetermined, number of stages. For example, an agent that\\nmust get to some location may not know a priori how many steps it will\\n24 1. Artificial Intelligence and Agents',\n",
       " '24 1. Artificial Intelligence and Agents\\ntake to get there, but, when planning, it does not consider what it will do\\nafter it gets to the location.\\n• An infinite horizon planner is an agent that plans on going on forever.\\nThis is often called a process. For example, the stabilization module of\\na legged robot should go on forever; it cannot stop when it has achieved\\nstability, because the robot has to keep from falling over.\\nThe modules in a hierarchical decomposition may have different horizons,\\nas in the following example.\\nExample 1.9 For the delivery and helping agent, at the lowest level the module that keeps the robot stable, safe, and attentive to requests may be on an\\ninfinite horizon, assuming it is running forever. The task of delivering coffee to\\na particular person may be an indefinite horizon problem. Planning for a fixed\\nnumber of hours may be a finite horizon problem.\\nExample 1.10 In a tutoring agent, for some subtasks, a finite horizon may be',\n",
       " 'Example 1.10 In a tutoring agent, for some subtasks, a finite horizon may be\\nappropriate, such as in a fixed teach, test, re-teach sequence. For other cases,\\nthere may be an indefinite horizon where the system may not know at design\\ntime how many steps it will take until the student has mastered some concept.\\nIt may also be possible to model teaching as an ongoing process of learning and\\ntesting with appropriate breaks, with no expectation of the system finishing.\\n1.5.3 Representation\\nThe representation dimension concerns how the world is described.\\nThe different ways the world could be are called states. A state of the world\\nspecifies the agent’s internal state (its belief state) and the environment state.\\nAt the simplest level, an agent can reason explicitly in terms of individually\\nidentified states.\\nExample 1.11 A thermostat for a heater may have two belief states: off and\\nheating. The environment may have three states: cold, comfortable, and hot. There',\n",
       " 'heating. The environment may have three states: cold, comfortable, and hot. There\\nare thus six states corresponding to the different combinations of belief and\\nenvironment states. These states may not fully describe the world, but they\\nare adequate to describe what a thermostat should do. The thermostat should\\nmove to, or stay in, heating if the environment is cold and move to, or stay in,\\noff if the environment is hot. If the environment is comfortable, the thermostat\\nshould stay in its current state. The thermostat agent turns or keeps the heater\\non in the heating state and turns or keeps the heater off in the off state.\\nInstead of enumerating states, it is often easier to reason in terms of features\\nof the state or propositions that are true or false of the state. A state may be\\ndescribed in terms of features, where a feature has a value in each state (see\\nSection 4.1, page 127).\\n1.5. Agent Design Space 25\\nExample 1.12 Consider designing an agent to diagnose electrical problems',\n",
       " '1.5. Agent Design Space 25\\nExample 1.12 Consider designing an agent to diagnose electrical problems\\nin the home of Figure 1.6 (page 18). It may have features for the position of\\neach switch, the status of each switch (whether it is working okay, whether it\\nis shorted, or whether it is broken), and whether each light works. The feature\\nposition s2 may be a feature that has value up when switch s2 is up and has\\nvalue down when the switch is down. The state of the home’s lighting may be\\ndescribed in terms of values for each of these features. These features depend\\non each other, but not in arbitrarily complex ways; for example, whether a light\\nis on may just depend on whether it is okay, whether the switch is turned on,\\nand whether there is electricity.\\nA proposition is a Boolean feature, which means that its value is either true\\nor false. Thirty propositions can encode 230 = 1, 073, 741, 824 states. It may be\\neasier to specify and reason with the thirty propositions than with more than a',\n",
       " 'easier to specify and reason with the thirty propositions than with more than a\\nbillion states. Moreover, having a compact representation of the states indicates\\nunderstanding, because it means that an agent has captured some regularities\\nin the domain.\\nExample 1.13 Consider an agent that has to recognize digits. Suppose the\\nagent observes a binary image, a 28 × 28 grid of pixels, where each of the 282 =\\n784 grid points is either black or white. The action is to determine which of the\\ndigits {0, . . . , 9} is shown in the image. There are 2784 different possible states\\nof the image, and so 102784 different functions from the image state into the\\ncharacters {a,..., z}. You cannot represent such functions in terms of the state\\nspace. Instead, handwriting recognition systems define features of the image,\\nsuch as line segments, and define the function from images to characters in\\nterms of these features. Modern implementations learn the features that are\\nuseful; see Example 8.3 (page 336).',\n",
       " 'useful; see Example 8.3 (page 336).\\nWhen describing a complex world, the features can depend on relations\\nand individuals. An individual is also called a thing, an object, or an entity. A\\nrelation on a single individual is a property. There is a feature for each possible\\nrelationship among the individuals.\\nExample 1.14 The agent that looks after a home in Example 1.12 could have\\nthe lights and switches as individuals, and relations position and connected to.\\nInstead of the feature position s2 = up, it could use the relation position(s2, up).\\nThis relation enables the agent to reason about all switches or for an agent to\\nhave general knowledge about switches that can be used when the agent encounters a switch.\\nExample 1.15 If an agent is enrolling students in courses, there could be a\\nfeature that gives the grade of a student in a course, for every student–course\\npair where the student took the course. There would be a passed feature for',\n",
       " 'pair where the student took the course. There would be a passed feature for\\nevery student–course pair, which depends on the grade feature for that pair. It\\nmay be easier to reason in terms of individual students, courses, and grades,\\nand the relations grade and passed. By defining how passed depends on grade\\n26 1. Artificial Intelligence and Agents\\nonce, the agent can apply the definition for each student and course. Moreover,\\nthis can be done before the agent knows which individuals exist, and so before\\nit knows any of the features.\\nThe two-argument relation passed, with 1000 students and 100 courses, can\\nrepresent 1000 ∗ 100 = 100, 000 propositions and so 2100,000 states.\\nBy reasoning in terms of relations and individuals, an agent can reason\\nabout whole classes of individuals without ever enumerating the features or\\npropositions, let alone the states. An agent may have to reason about infinite\\nsets of individuals, such as the set of all numbers or the set of all sentences. To',\n",
       " 'sets of individuals, such as the set of all numbers or the set of all sentences. To\\nreason about an unbounded or infinite number of individuals, an agent cannot\\nreason in terms of states or features; it must reason at the relational level.\\nIn the representation dimension, the agent reasons in terms of\\n• states\\n• features, or\\n• individuals and relations (often called relational representations).\\nSome of the frameworks will be developed in terms of states, some in terms of\\nfeatures, and some in terms of individuals and relations.\\nReasoning in terms of states is introduced in Chapter 3. Reasoning in terms\\nof features is introduced in Chapter 4. Relational reasoning is considered starting from Chapter 15.\\n1.5.4 Computational Limits\\nSometimes an agent can decide on its best action quickly enough for it to act.\\nOften there are computational resource limits that prevent an agent from carrying out the best action. That is, the agent may not be able to find the best',\n",
       " 'action quickly enough within its memory limitations to act while that action is\\nstill the best thing to do. For example, it may not be much use to take 10 minutes to derive what was the best thing to do 10 minutes ago, when the agent\\nhas to act now. Often, instead, an agent must trade off how long it takes to get\\na solution with how good the solution is; it may be better to find a reasonable\\nsolution quickly than to find a better solution later because the world will have\\nchanged during the computation.\\nThe computational limits dimension determines whether an agent has\\n• perfect rationality, where an agent reasons about the best action without\\ntaking into account its limited computational resources, or\\n• bounded rationality, where an agent decides on the best action that it\\ncan find given its computational limitations.\\nComputational resource limits include computation time, memory, and numerical accuracy caused by computers not representing real numbers exactly.',\n",
       " 'An anytime algorithm is an algorithm where the solution quality improves\\nwith time. In particular, it is one that can produce its current best solution at\\n1.5. Agent Design Space 27\\nany time, but given more time it could produce even better solutions. To ensure\\nthat the quality does not decrease, the agent can store the best solution found\\nso far, and return that when asked for a solution. Although the solution quality\\nmay increase with time, waiting to act has a cost; it may be better for an agent\\nto act before it has found what would be the best solution.\\nExample 1.16 The delivery robot cannot think for a long time about how to\\navoid a person. There might be a best way to avoid the person and to achieve\\nits other goals, however it might take time to determine that optimal path, and\\nit might be better to act quickly and then recover from a non-optimal action. In\\nthe simplest case, a robot could just stop if it encounters a person, but even that',\n",
       " 'the simplest case, a robot could just stop if it encounters a person, but even that\\nis error prone as robots have momentum, so it cannot stop immediately and\\npeople behind may run into it if it stops suddenly.\\nExample 1.17 Even a tutoring agent that can act at longer scales than a robot\\nsometimes has to act quickly. When a student has completed a task and wants\\na new task, the agent needs to decide whether it should assign the student\\nthe best task it has found so far, or compute for longer, trying to find an even\\nbetter task. As the student waits, they might become distracted, which might\\nbe worse than giving them a non-optimal task. The computer can be planning\\nthe next task when the student is working. Modern computers, as fast as they\\nmay be, cannot find optimal solutions to difficult problems quickly.\\nExample 1.18 Figure 1.7 shows how the computation time of an anytime algorithm can affect the solution quality. The agent has to carry out an action but',\n",
       " 'can do some computation to decide what to do. The absolute solution quality,\\n0 1 2 3 4 5\\nValue of Action\\nTime of Action\\nFigure 1.7: Solution quality as a function of time for an anytime algorithm. The\\nmeaning is described in Example 1.18\\n28 1. Artificial Intelligence and Agents\\nhad the action been carried out at time zero, shown as the dashed line at the\\ntop, is improving as the agent takes time to reason. However, there is a penalty\\nassociated with taking time to act. In this figure, the penalty, shown as the dotted line at the bottom, is negative and proportional to the time taken before\\nthe agent acts. These two values can be added to get the discounted quality,\\nthe time-dependent value of computation; this is the solid line in the middle of\\nthe graph. For the example of Figure 1.7 (page 27), an agent should compute\\nfor about 2.5 time units, and then act, at which point the discounted quality\\nachieves its maximum value. If the computation lasts for longer than 4.3 time',\n",
       " 'achieves its maximum value. If the computation lasts for longer than 4.3 time\\nunits, the resulting discounted solution quality is worse than if the algorithm\\noutputs the initial guess it can produce with virtually no computation. It is typical that the solution quality improves in jumps; when the current best solution\\nchanges, there is a jump in the quality. The penalty associated with waiting is\\nrarely a straight line; it is typically a function of deadlines, which may not be\\nknown by the agent.\\nTo take into account bounded rationality, an agent must decide whether it\\nshould act or reason for longer. This is challenging because an agent typically\\ndoes not know how much better off it would be if it only spent a little bit more\\ntime reasoning. Moreover, the time spent thinking about whether it should\\nreason may detract from actually reasoning about the domain.\\n1.5.5 Learning\\nIn some cases, a designer of an agent may have a good model of the agent and',\n",
       " '1.5.5 Learning\\nIn some cases, a designer of an agent may have a good model of the agent and\\nits environment. But often a designer does not have a good model, and so an\\nagent should use data from its past experiences and other sources to help it\\ndecide what to do.\\nThe learning dimension determines whether\\n• knowledge is given, or\\n• knowledge is learned (from prior knowledge and data or past experience).\\nLearning typically means finding the best model that fits the data. Sometimes this is as simple as tuning a fixed set of parameters, but it can also mean\\nchoosing the best representation out of a class of representations. Learning is\\na huge field in itself but does not stand in isolation from the rest of AI. There\\nare many issues beyond fitting data, including how to incorporate background\\nknowledge, what data to collect, how to represent the data and the resulting\\nrepresentations, what learning biases are appropriate, and how the learned',\n",
       " 'representations, what learning biases are appropriate, and how the learned\\nknowledge can be used to affect how the agent acts.\\nLearning is considered in Chapters 7, 8, 10, 13, and 17.\\nExample 1.19 A robot has a great deal to learn, such as how slippery floors\\nare as a function of their shininess, where each person hangs out at different\\n1.5. Agent Design Space 29\\nparts of the day, when they will ask for coffee, and which actions result in the\\nhighest rewards.\\nModern vision systems are trained to learn good features (such as lines and\\ntextures) on millions if not billions of images and videos. These features can\\nbe used to recognize objects and for other tasks, even if there have been few\\nexamples of the higher-level concepts. A robot might not have seen a baby\\ncrawling on a highway, or a particular mug, but should be able to deal with\\nsuch situations.\\nExample 1.20 Learning is fundamental to diagnosis. It is through learning',\n",
       " 'such situations.\\nExample 1.20 Learning is fundamental to diagnosis. It is through learning\\nand science that medical professionals understand the progression of diseases\\nand how well treatments work or do not work. Diagnosis is a challenging domain for learning, because all patients are different, and each individual doctor’s experience is only with a few patients with any particular set of symptoms. Doctors also see a biased sample of the population; those who come to\\nsee them usually have unusual or painful symptoms. Drugs are not given to\\npeople randomly. You cannot learn the effect of treatment by observation alone,\\nbut need a causal model of the causes and effects; see Chapter 11 for details\\non building causal models. To overcome the limitations of learning from observations alone, drug companies spend billions of dollars doing randomized\\ncontrolled trials in order to learn the efficacy of drugs.\\n1.5.6 Uncertainty',\n",
       " 'controlled trials in order to learn the efficacy of drugs.\\n1.5.6 Uncertainty\\nAn agent could assume there is no uncertainty, or it could take uncertainty in\\nthe domain into consideration. Uncertainty is divided into two dimensions:\\none for uncertainty from sensing and one for uncertainty about the effects of\\nactions.\\nSensing Uncertainty\\nIn some cases, an agent can observe the state of the world directly. For example,\\nin some board games or on a factory floor, an agent may know exactly the state\\nof the world. In many other cases, it may have some noisy perception of the\\nstate and the best it can do is to have a probability distribution over the set\\nof possible states based on what it perceives. For example, given a patient’s\\nsymptoms, a medical doctor may not actually know which disease a patient\\nhas and may have only a probability distribution over the diseases the patient\\nmay have.\\nThe sensing uncertainty dimension concerns whether the agent can determine the state from the stimuli:',\n",
       " '• Fully observable means the agent knows the state of the world from the\\nstimuli.\\n30 1. Artificial Intelligence and Agents\\n• Partially observable means the agent does not directly observe the state\\nof the world. This occurs when many possible states can result in the\\nsame stimuli or when stimuli are misleading.\\nAssuming the world is fully observable is a common simplifying assumption\\nto keep reasoning tractable.\\nExample 1.21 The delivery robot does not know exactly where it is, or what\\nelse there is, based on its limited sensors. Looking down a corridor does not\\nprovide enough information to know where it is or who is behind the doors.\\nKnowing where it was a second ago will help determine where it is now, but\\neven robots can get lost. It may not know where the person who requested\\ncoffee is. When it is introduced into a new environment, it may have much\\nmore uncertainty.\\nExample 1.22 The tutoring agent cannot directly observe the knowledge of',\n",
       " 'more uncertainty.\\nExample 1.22 The tutoring agent cannot directly observe the knowledge of\\nthe student. All it has is some sensing input, based on questions the student\\nasks or does not ask, facial expressions, distractedness, and test results. Even\\ntest results are very noisy, as a mistake may be due to distraction or test anxiety\\ninstead of lack of knowledge, and a correct answer might be due to a lucky\\nguess instead of real understanding. Sometimes students make mistakes in\\ntesting situations they wouldn’t make at other times.\\nExample 1.23 A trading agent does not know all available options and their\\navailability, but must find out information that can become outdated quickly\\n(e.g., if a hotel becomes booked up). A travel agent does not know whether a\\nflight will be canceled or delayed, or whether the passenger’s luggage will be\\nlost. This uncertainty means that the agent must plan for the unanticipated.\\nEffect Uncertainty',\n",
       " 'lost. This uncertainty means that the agent must plan for the unanticipated.\\nEffect Uncertainty\\nA model of the dynamics of the world is a model of how the world changes as\\na result of actions, including the case of how it changes if the action were to do\\nnothing. In some cases an agent knows the effects of its action. That is, given\\na state and an action, the agent can accurately predict the state resulting from\\ncarrying out that action in that state. For example, a software agent interacting\\nwith the file system of a computer may be able to predict the effects of deleting\\na file given the state of the file system. However, in many cases, it is difficult\\nto predict the effects of an action, and the best an agent can do is to have a\\nprobability distribution over the effects. For example, a teacher may not know\\nthe effects explaining a concept, even if the state of the students is known. At\\nthe other extreme, if the teacher has no inkling of the effect of its actions, there',\n",
       " 'the other extreme, if the teacher has no inkling of the effect of its actions, there\\nwould be no reason to choose one action over another.\\nThe dynamics in the effect uncertainty dimension can be\\n• deterministic when the state resulting from an action is determined by\\nan action and the prior state, or\\n1.5. Agent Design Space 31\\n• stochastic when there is a probability distribution over the resulting states.\\nExample 1.24 For the delivery robot, there can be uncertainty about the effects\\nof an action, both at the low level, say due to slippage of the wheels, or at the\\nhigh level because the agent might not know whether putting the coffee on a\\nperson’s desk succeeded in delivering coffee to the person. This may depend\\non the individual preferences of users.\\nExample 1.25 Even a trading agent does not know the effect of putting in\\na trade order, such as booking a flight or a hotel room. These can become unavailable at very short notice (consider two trading agents trying to book the',\n",
       " 'same room at the same time), or the price can vary.\\nThe effect dimension only makes sense when the world is fully observable.\\nIf the world is partially observable, a stochastic system can be modeled as a\\ndeterministic system where the effect of an action depends on unobserved features. It is a separate dimension because many of the frameworks developed\\nare for the fully observable, stochastic action case.\\nPlanning with deterministic actions is considered in Chapter 6. Planning\\nwith stochastic actions is considered in Chapter 12.\\n1.5.7 Preference\\nAgents normally act to have better outcomes. The only reason to choose one\\naction over another is because the preferred action leads to more desirable outcomes.\\nAn agent may have a simple goal, which is a proposition the agent wants\\nto be true in a final state. For example, the goal of getting Sam coffee means\\nthe agent wants to reach a state where Sam has coffee. Other agents may have',\n",
       " 'the agent wants to reach a state where Sam has coffee. Other agents may have\\nmore complex preferences. For example, a medical doctor may be expected to\\ntake into account suffering, life expectancy, quality of life, monetary costs (for\\nthe patient, the doctor, and society), and the ability to justify decisions in case\\nof a lawsuit. The doctor must trade these considerations off when they conflict,\\nas they invariably do.\\nThe preference dimension considers whether the agent has goals or richer\\npreferences:\\n• A goal is either an achievement goal, which is a proposition to be true in\\nsome final state, or a maintenance goal, a proposition that must be true\\nin all visited states. For example, the goals for a robot may be to deliver\\na cup of coffee and a banana to Sam, and not to make a mess or hurt\\nanyone.\\n• Complex preferences involve trade-offs among the desirability of various outcomes, perhaps at different times. An ordinal preference is where',\n",
       " 'only the ordering of the preferences is important. A cardinal preference\\nis where the magnitude of the values matters. For example, an ordinal\\n32 1. Artificial Intelligence and Agents\\npreference may be that Sam prefers cappuccino over black coffee and\\nprefers black coffee over tea. A cardinal preference may give a tradeoff between the wait time and the type of beverage, and a mess versus\\ntaste trade-off, where Sam is prepared to put up with more mess in the\\npreparation of the coffee if the taste of the coffee is exceptionally good.\\nExample 1.26 The delivery robot could be given goals, such as “deliver coffee\\nto Chris and make sure you always have power.” A more complex goal may be\\nto “clean up the lab, and put everything where it belongs”, which can only be\\nachieved to some degree. There can be complex preferences, such as “deliver\\nmail when it arrives and service coffee requests as soon as possible, but it is\\nmore important to deliver messages marked as urgent, and Chris needs her',\n",
       " 'more important to deliver messages marked as urgent, and Chris needs her\\ncoffee quickly when she asks for it.”\\nExample 1.27 For the diagnostic assistant, the goal may be as simple as “fix\\nwhat is wrong,” but often there are complex trade-offs involving costs, pain,\\nlife expectancy, and preferences related to the uncertainty that the diagnosis is\\ncorrect and uncertainty as to efficacy and side-effects of the treatment. There is\\nalso a problem of whose preferences are to be taken into account; the patient,\\nthe doctor, the payer, and society may all have different preferences that must\\nbe reconciled.\\nExample 1.28 Although it may be possible for the tutoring agent to have\\na simple goal such, as to teach some particular concept, it is more likely that\\ncomplex preferences must be taken into account. One reason is that, with uncertainty, there may be no way to guarantee that the student knows the concept',\n",
       " 'being taught; any method that tries to maximize the probability that the student knows a concept will be very annoying, because it will repeatedly teach\\nand test if there is a slight chance that the student’s errors are due to misunderstanding as opposed to fatigue or boredom. More complex preferences would\\nenable a trade-off among fully teaching a concept, boring the student, the time\\ntaken, and the amount of retesting. The student may also have a preference\\nfor a teaching style that could be taken into account. The student, the teacher,\\nthe parents, and future employers may have different preferences. The student\\nmay have incompatible preferences, for example, to not work hard and to get\\na good mark. If the teacher is optimizing student evaluations, it might both\\nallow the student to not work hard, and also give good marks. But that might\\nundermine the goal of the student actually learning something.\\nExample 1.29 For a trading agent, preferences of users are typically in terms',\n",
       " 'Example 1.29 For a trading agent, preferences of users are typically in terms\\nof functionality, not components. For example, typical computer buyers have\\nno idea of what hardware to buy, but they know what functionality they want\\nand they also want the flexibility to be able to use new software features that\\nmight not even exist yet. Similarly, in a travel domain, what activities a user\\nwants may depend on the location. Users also may want the ability to participate in a local custom at their destination, even though they may not know\\nwhat those customs are. Even a simple path-finding algorithm, such as Google\\n1.5. Agent Design Space 33\\nMaps, which, at the time of writing, assumes all users’ preferences are to minimize travel time, could take into account each individual user’s preferences\\nfor diverse views or avoiding going too close to where some particular relative\\nlives.',\n",
       " 'for diverse views or avoiding going too close to where some particular relative\\nlives.\\nGoals are considered in Chapters 3 and 6. Complex preferences are considered in Chapter 12, and the following chapters.\\n1.5.8 Number of Agents\\nAn agent reasoning about what it should do in an environment where it is the\\nonly agent is difficult enough. However, reasoning about what to do when\\nthere are other agents who are also reasoning is much more difficult. An agent\\nin a multiagent setting may need to reason strategically about other agents;\\nthe other agents may act to trick or manipulate the agent or may be available\\nto cooperate with the agent. With multiple agents, it is often optimal to act\\nrandomly because other agents can exploit deterministic strategies. Even when\\nthe agents are cooperating and have a common goal, the task of coordination\\nand communication makes multiagent reasoning more challenging. However,\\nmany domains contain multiple agents and ignoring other agents’ strategic',\n",
       " 'many domains contain multiple agents and ignoring other agents’ strategic\\nreasoning is not always the best way for an agent to reason.\\nTaking the point of view of a single agent, the number of agents dimension\\nconsiders whether the agent explicitly considers other agents:\\n• Single agent reasoning means the agent assumes that there are no other\\nagents in the environment or that all other agents are part of nature, and\\nso are non-purposive. This is a reasonable assumption if there are no\\nother agents or if the other agents are not going to change what they do\\nbased on the agent’s action.\\n• Adversarial reasoning considers another agent, where when one agent\\nwins, the other loses. This is sometimes called a two-player zero-sum\\ngame, as the payoffs for the agents (e.g., +1 for a win and −1 for a loss)\\nsum to zero. This is a simpler case than allowing for arbitrary agents as\\nthere is no need to cooperate or otherwise coordinate.',\n",
       " 'there is no need to cooperate or otherwise coordinate.\\n• Multiple agent reasoning (or multiagent reasoning) means the agent\\ntakes the reasoning of other agents into account. This occurs when there\\nare other intelligent agents whose goals or preferences depend, in part,\\non what the agent does or if the agent must communicate with other\\nagents. Agents may need to cooperate because coordinated actions can\\nresult in outcomes that are better for all agents than each agent considering the other agents as part of nature.\\nReasoning in the presence of other agents is much more difficult if the\\nagents can act simultaneously or if the environment is only partially observable. Multiagent systems are considered in Chapter 14. Note that the adversarial case is separate as there are some methods that only work for that case.\\n34 1. Artificial Intelligence and Agents\\nExample 1.30 There can be multiple delivery robots, which can coordinate to',\n",
       " 'Example 1.30 There can be multiple delivery robots, which can coordinate to\\ndeliver coffee and parcels more efficiently. They can compete for power outlets\\nor for space to move. Only one might be able to go closest to the wall when\\nturning a corner. There may also be children out to trick the robot, or pets that\\nget in the way.\\nWhen automated vehicles have to go on a highway, it may be much more\\nefficient and safer for them to travel in a coordinated manner, say one centimeter apart in a convoy, than to travel three vehicle lengths apart. It is more\\nefficient because they can reduce wind drag, and many more vehicles can fit on\\na highway. It is safer because the difference in speeds is small; if one vehicle\\nslams on its brakes or has engine problems, the car that might crash into the\\nback is going approximately the same speed.\\nExample 1.31 A trading agent has to reason about other agents. In commerce,\\nprices are governed by supply and demand; this means that it is important to',\n",
       " 'prices are governed by supply and demand; this means that it is important to\\nreason about the other competing agents. This happens particularly in a world\\nwhere many items are sold by auction. Such reasoning becomes particularly\\ndifficult when there are items that must complement each other, such as flights\\nand hotel bookings, and items that can substitute for each other, such as bus\\ntransport or taxis. You don’t want to book the flights if there is no accommodation, or book accommodation if there are no flights.\\n1.5.9 Interactivity\\nIn deciding what an agent will do, there are three aspects of computation that\\nmust be distinguished: (1) the design-time computation that goes into the design of the agent, carried out by the designer of the agent, not the agent itself;\\n(2) the computation that the agent can do before it observes the world and\\nneeds to act; and (3) the computation that is done by the agent as it is acting.\\nThe interactivity dimension considers whether the agent does',\n",
       " 'The interactivity dimension considers whether the agent does\\n• only offline reasoning, where offline reasoning is the computation done\\nby the agent before it has to act, and can include compilation, learning\\nor finding solutions from every state the agent could find itself in; under\\nthis assumption, the agent can carry out simple fixed-cost computation\\nwhile acting, sometimes even just looking up the action in a table\\n• significant online reasoning, where online computation is the computation done by the agent between observing the environment and acting.\\nAn agent acting in the world usually does not have the luxury of having\\nthe world wait for it to consider the best option. However, offline reasoning,\\nwhere the agent can reason about the best thing to do before having to act,\\nis often a simplifying assumption. Online reasoning can include long-range\\nstrategic reasoning as well as determining how to react in a timely manner to\\nthe environment; see Chapter 2.\\n1.5. Agent Design Space 35',\n",
       " 'the environment; see Chapter 2.\\n1.5. Agent Design Space 35\\nExample 1.32 A delivery robot may be able to compute a plan for its day\\noffline, but then it needs to be able to adapt to changes, for example, when\\nsomeone wants coffee early or something urgent needs to be delivered. It cannot plan for who it will meet and need to avoid in the corridors. It either needs\\nto be able to anticipate and plan for all possible eventualities, or it needs to\\nreason online when it finds something unexpected.\\nExample 1.33 A tutoring agent can determine the general outline of what\\nshould be taught offline. But then it needs to be able to react to unexpected\\nbehavior online when it occurs. It is difficult to be able to anticipate all eventualities, and might be easier to deal with them online when it encounters them.\\n1.5.10 Interaction of the Dimensions\\nFigure 1.8 summarizes the dimensions of complexity.\\nIn terms of the dimensions of complexity, the simplest case for the robot is',\n",
       " 'In terms of the dimensions of complexity, the simplest case for the robot is\\na flat system, represented in terms of states, with no uncertainty, with achievement goals, with no other agents, with given knowledge, and with perfect rationality. In this case, with an indefinite stage planning horizon, the problem\\nof deciding what to do is reduced to the problem of finding a path in a graph\\nof states. This is explored in Chapter 3.\\nIn going beyond the simplest cases, these dimensions cannot be considered\\nindependently because they interact in complex ways. Consider the following\\nexamples of the interactions.\\nThe representation dimension interacts with the modularity dimension in\\nthat some modules in a hierarchy may be simple enough to reason in terms of\\na finite set of states, whereas other levels of abstraction may require reasoning\\nabout individuals and relations. For example, in a delivery robot, a module\\nDimension Values\\nModularity flat, modular, hierarchical',\n",
       " 'Dimension Values\\nModularity flat, modular, hierarchical\\nPlanning horizon non-planning, finite stage,\\nindefinite stage, infinite stage\\nRepresentation states, features, relations\\nComputational limits perfect rationality, bounded rationality\\nLearning knowledge is given, knowledge is learned\\nSensing uncertainty fully observable, partially observable\\nEffect uncertainty deterministic, stochastic\\nPreference goals, complex preferences\\nNumber of agents single agent, adversaries, multiple agents\\nInteractivity offline, online\\nFigure 1.8: Dimensions of complexity\\n36 1. Artificial Intelligence and Agents\\nthat maintains balance may only have a few states. A module that must prioritize the delivery of multiple parcels to multiple people may have to reason\\nabout multiple individuals (e.g., people, packages, and rooms) and the relations between them. At a higher level, a module that reasons about the activity\\nover the day may only require a few states to cover the different phases of the',\n",
       " 'over the day may only require a few states to cover the different phases of the\\nday (e.g., there might be three states of the robot: busy, available for requests,\\nand recharging).\\nThe planning horizon interacts with the modularity dimension. For example, at a high level, a dog may be getting an immediate reward when it comes\\nand gets a treat. At the level of deciding where to place its paws, there may be\\na long time until it gets the reward, and so at this level it may have to plan for\\nan indefinite stage.\\nSensing uncertainty probably has the greatest impact on the complexity of\\nreasoning. It is much easier for an agent to reason when it knows the state of\\nthe world than when it does not.\\nThe uncertainty dimensions interact with the modularity dimension: at one\\nlevel in a hierarchy, an action may be deterministic, whereas at another level, it\\nmay be stochastic. As an example, consider the result of flying to a particular',\n",
       " 'may be stochastic. As an example, consider the result of flying to a particular\\noverseas destination with a companion you are trying to impress. At one level\\nyou may know which country you are in. At a lower level, you may be quite\\nlost and not know where you are on a map of the airport. At an even lower\\nlevel responsible for maintaining balance, you may know where you are: you\\nare standing on the ground. At the highest level, you may be very unsure\\nwhether you have impressed your companion.\\nPreference models interact with uncertainty because an agent needs to trade\\noff between satisfying a very desirable goal with low probability or a less desirable goal with a higher probability. This issue is explored in Section 12.1\\n(page 518).\\nMultiple agents can also be used for modularity; one way to design a single agent is to build multiple interacting agents that share a common goal of',\n",
       " 'making the higher-level agent act intelligently. Some researchers, such as Minsky [1986], argue that intelligence is an emergent feature from a “society” of\\nunintelligent agents.\\nLearning is often cast in terms of learning with features – determining which\\nfeature values best predict the value of another feature. However, learning can\\nalso be carried out with individuals and relations. Learning with hierarchies,\\nsometimes called deep learning, has enabled the learning of more complex\\nconcepts. Much work has been done on learning in partially observable domains, and learning with multiple agents. Each of these is challenging in its\\nown right without considering interactions with multiple dimensions.\\nThe interactivity dimension interacts with the planning horizon dimension\\nin that when the agent is reasoning and acting online, it also needs to reason\\nabout the long-term horizon. The interactivity dimension also interacts with',\n",
       " 'about the long-term horizon. The interactivity dimension also interacts with\\nthe computational limits; even if an agent is reasoning offline, it cannot take\\nhundreds of years to compute an answer. However, when it has to reason\\n1.6. Designing Agents 37\\nabout what to do in, say, 1/10 of a second, it needs to be concerned about the\\ntime taken to reason, and the trade-off between thinking and acting.\\nTwo of these dimensions, modularity and bounded rationality, promise to\\nmake reasoning more efficient. Although they make the formalism more complicated, breaking the system into smaller components, and making the approximations needed to act in a timely fashion and within memory limitations,\\nshould help build more complex systems.\\n1.6 Designing Agents\\nArtificial agents are designed for particular tasks. Researchers have not yet\\ngot to the stage of designing an intelligent agent for the task of surviving and\\nreproducing in a complex natural environment.',\n",
       " 'reproducing in a complex natural environment.\\n1.6.1 Simplifying Environments and Simplifying Agents\\nIt is important to distinguish between the knowledge in the mind of an agent\\nand the knowledge in the mind of the designer of the agent. Consider the\\nextreme cases:\\n• At one extreme is a highly specialized agent that works well in the environment for which it was designed, but is helpless outside of this niche.\\nThe designer may have done considerable work in building the agent,\\nbut the agent can be extremely specialized to operate well. An example\\nis a traditional thermostat. It may be difficult to design a thermostat so\\nthat it turns on and off at exactly the right temperatures, but the thermostat itself does not have to do much computation. Another example\\nis a car-painting robot that always paints the same parts in an automobile factory. There may be much design time or offline computation to\\nget it to work perfectly, but the painting robot can paint parts with little',\n",
       " 'get it to work perfectly, but the painting robot can paint parts with little\\nonline computation; it senses that there is a part in position, but then it\\ncarries out its predefined actions. These very specialized agents do not\\nadapt well to different environments or to changing goals. The painting\\nrobot would not notice if a different sort of part were present and, even\\nif it did, it would not know what to do with it. It would have to be redesigned or reprogrammed to paint different parts or to change into a\\nsanding machine or a dog-washing machine.\\n• At the other extreme is a very flexible agent that can survive in arbitrary\\nenvironments and accept new tasks at run time. Simple biological agents\\nsuch as insects can adapt to complex changing environments, but they\\ncannot carry out arbitrary tasks. Designing an agent that can adapt to\\ncomplex environments and changing goals is a major challenge. The\\nagent will know much more about the particulars of a situation than the',\n",
       " 'agent will know much more about the particulars of a situation than the\\ndesigner. Even biology has not produced many such agents. Humans\\n38 1. Artificial Intelligence and Agents\\nmay be the only extant example, but even humans need time to adapt to\\nnew environments.\\nEven if the flexible agent is our ultimate dream, researchers have to reach this\\ngoal via more mundane goals. Rather than building a universal agent, which\\ncan adapt to any environment and solve any task, researchers have been restricted to particular agents for particular environmental niches. The designer\\ncan exploit the structure of the particular niche and the agent does not have to\\nreason about other possibilities.\\nTwo broad strategies have been pursued in building agents:\\n• The first is to simplify environments and build complex reasoning systems for these simple environments. For example, factory robots can do\\nsophisticated tasks in the engineered environment of a factory, but they',\n",
       " 'sophisticated tasks in the engineered environment of a factory, but they\\nmay be hopeless in a natural environment. Much of the complexity of\\nthe task can be reduced by simplifying the environment. This is also important for building practical systems because many environments can\\nbe engineered to make them simpler for agents.\\n• The second strategy is to build simple agents in natural environments.\\nThis is inspired by seeing how insects can survive in complex environments even though they have very limited reasoning abilities. Modern\\nlanguage systems can predict the probability of the next word in an arbitrary text, but this does not mean they can be used for decision making.\\nResearchers then make the agents have more reasoning abilities as their\\ntasks become more complicated.\\nOne of the advantages of simplifying environments is that it may enable us to',\n",
       " 'One of the advantages of simplifying environments is that it may enable us to\\nprove properties of agents or to optimize agents for particular situations. Proving properties or optimization typically requires a model of the agent and its\\nenvironment. The agent may do a little or a lot of reasoning, but an observer or\\ndesigner of the agent may be able to reason about the agent and the environment. For example, the designer may be able to prove whether the agent can\\nachieve a goal, whether it can avoid getting into situations that may be bad for\\nthe agent (safety), whether it can avoid getting stuck somewhere (liveness),\\nor whether it will eventually get around to each of the things it should do\\n(fairness). Of course, the proof is only as good as the model.\\nThe advantage of building agents for complex environments is that these\\nare the types of environments in which humans live and where agents could\\nbe useful.\\nEven natural environments can be abstracted into simpler environments.',\n",
       " 'be useful.\\nEven natural environments can be abstracted into simpler environments.\\nFor example, for an autonomous car driving on public roads the environment\\ncan be conceptually simplified so that everything is either a road, another car,\\nor something to be avoided. Although autonomous cars have sophisticated\\nsensors, they only have limited actions available, namely steering, accelerating,\\nand braking.\\nFortunately, research along both lines, and between these extremes, is being\\ncarried out. In the first case, researchers start with simple environments and\\n1.6. Designing Agents 39\\nmake the environments more complex. In the second case, researchers increase\\nthe complexity of the behaviors that the agents can carry out.\\n1.6.2 Tasks\\nOne way that AI representations differ from computer programs in traditional\\nlanguages is that an AI representation typically specifies what needs to be computed, not how it is to be computed. You might specify that the agent should',\n",
       " 'find the most likely disease a patient has, or specify that a robot should get\\ncoffee, but not give detailed instructions on how to do these things. Much AI\\nreasoning involves searching through the space of possibilities to determine\\nhow to complete a task.\\nTypically, a task is only given informally, such as “deliver parcels promptly\\nwhen they arrive” or “fix whatever is wrong with the electrical system of the\\nhome.”\\nThe general framework for solving tasks by computer is given in Figure 1.9.\\nTo solve a task, the designer of a system must:\\n• determine what constitutes a solution\\n• represent the task in a way a computer can reason about\\n• use the computer to compute an output; either answers presented to a\\nuser or actions to be carried out in the environment\\n• interpret the output as a solution to the task.\\nIn AI, knowledge is long-term representation of a domain whereas belief is\\nabout the immediate environment, for example where the agent is and where',\n",
       " 'about the immediate environment, for example where the agent is and where\\nother object are. In philosophy, knowledge is usually defined as justified true\\nbelief, but in AI the term is used more generally to be any relatively stable information, as opposed to belief, which is more transitory information. The reason\\nfor this terminology is that it is difficult for an agent to determine truth, and\\n“justified” is subjective. Knowledge in AI can be represented in terms of logic,\\ntask solution\\nrepresentation output\\nsolve\\ncompute\\nrepresent interpret informal\\nformal\\nFigure 1.9: The role of representations in solving tasks\\n40 1. Artificial Intelligence and Agents\\nneural networks, or probabilistic models, but belief is typically represented as\\na distribution over the states.\\nA representation of some piece of knowledge is the particular data structures used to encode the knowledge so it can be reasoned with.\\nThe form of representation – what is represented – is a compromise among',\n",
       " 'The form of representation – what is represented – is a compromise among\\nmany competing objectives. A representation should be:\\n• rich enough to express the knowledge needed to solve the task\\n• as close to a natural specification of the task as possible\\n• amenable to efficient computation\\n• able to be acquired from people, data, and past experiences.\\nBeing as close to a natural specification of the task as possible means should\\nbe compact, natural, and maintainable. It should be easy to see the relationship\\nbetween the representation and the domain being represented, so that it is easy\\nto determine whether the knowledge represented is correct; a small change\\nin the task should result in a small change in the representation of the task.\\nThere is an active debate about how much of the internal structure of reasoning\\nshould be explainable; the field of explainable AI is about how to make more\\naspects of the decision making amenable to being explained to a person.',\n",
       " 'aspects of the decision making amenable to being explained to a person.\\nEfficient computation enables the agent to act quickly enough to be effective. A tractable algorithm is one with reasonable asymptotic complexity, often meaning the computation time is polynomial in the input size (page 95),\\nhowever often linear complexity is too slow. To ensure this, representations\\nexploit features of the task for computational gain and trade off accuracy and\\ncomputation time.\\nMany different representation languages have been designed. Many of\\nthese start with some of these objectives and are then expanded to include\\nthe other objectives. For example, some are designed for learning, perhaps\\ninspired by neurons, and then expanded to allow richer task-solving and inference abilities. Some representation languages are designed with expressiveness in mind, and then inference and learning are added on. Some language',\n",
       " 'designers focus on tractability and enhance richness, naturalness, and the ability to be acquired.\\n1.6.3 Defining a Solution\\nGiven an informal description of a task, before even considering a computer,\\nan agent designer should determine what would constitute a solution. This\\nquestion arises not only in AI but in any software design. Much of software\\nengineering involves refining the specification of the task.\\nTasks are typically not well specified. Not only is there usually much left\\nunspecified, but also the unspecified parts cannot be filled in arbitrarily. For\\nexample, if a user asks a trading agent to find out all the information about\\nresorts that may have unsanitary food practices, they do not want the agent to\\nreturn all the information about all resorts, even though all of the information\\n1.6. Designing Agents 41\\nrequested is in the result. However, if the trading agent does not have complete\\nknowledge about the resorts, returning all of the information may be the only',\n",
       " 'knowledge about the resorts, returning all of the information may be the only\\nway for it to guarantee that all of the requested information is there. Similarly,\\none does not want a delivery robot, when asked to take all of the trash to the\\ngarbage can, to take everything to the garbage can, even though this may be\\nthe only way to guarantee that all of the trash has been taken. Much work in\\nAI is motivated by commonsense reasoning; the computer should be able to\\nreach commonsense conclusions about the unstated assumptions.\\nGiven a well-defined task, the next issue is whether it matters if the answer\\nreturned is incorrect or incomplete. For example, if the specification asks for\\nall instances, does it matter if some are missing? Does it matter if there are\\nsome extra instances? Often a person does not want just any solution but the\\nbest solution according to some criteria. There are four common classes of\\nsolutions:',\n",
       " 'best solution according to some criteria. There are four common classes of\\nsolutions:\\nOptimal solution An optimal solution to a task is one that is the best solution\\naccording to some measure of solution quality. This measure is typically\\nspecified as an ordinal, where only the order matters. In some situations\\na cardinal measure, where the relative magnitudes also matter, is used.\\nFor example, a robot may need to take out as much trash as possible;\\nthe more trash it can take out, the better. In a more complex example,\\nyou may want the delivery robot to take as much of the trash as possible to the garbage can, minimizing the distance traveled, and explicitly\\nspecify a trade-off between the effort required and the proportion of the\\ntrash taken out. There are also costs associated with making mistakes\\nand throwing out items that are not trash. It may be better to miss some\\ntrash than to waste too much time. One general measure of desirability',\n",
       " 'trash than to waste too much time. One general measure of desirability\\nthat interacts with probability is utility (page 518).\\nSatisficing solution Often an agent does not need the best solution to a task\\nbut just needs some solution. A satisficing solution is one that is good\\nenough according to some description of which solutions are adequate.\\nFor example, a person may tell a robot that it must take all of the trash\\nout, or tell it to take out three items of trash.\\nApproximately optimal solution One of the advantages of a cardinal measure of success is that it allows for approximations. An approximately\\noptimal solution is one whose measure of quality is close to the best that\\ncould theoretically be obtained. Typically, agents do not need optimal\\nsolutions to tasks; they only need to get close enough. For example, the\\nrobot may not need to travel the optimal distance to take out the trash\\nbut may only need to be within, say, 10% of the optimal distance. Some',\n",
       " 'but may only need to be within, say, 10% of the optimal distance. Some\\napproximation algorithms guarantee that a solution is within some range\\nof optimal, but for some algorithms no guarantees are available.\\nFor some tasks, it is much easier computationally to get an approximately optimal solution than to get an optimal solution. However, for\\nother tasks, it is just as difficult to find an approximately optimal solution\\n42 1. Artificial Intelligence and Agents\\nthat is guaranteed to be within some bounds of optimal as it is to find an\\noptimal solution.\\nProbable solution A probable solution is one that, even though it may not\\nactually be a solution to the task, is likely to be a solution. This is one way\\nto approximate, in a precise manner, a satisficing solution. For example,\\nin the case where the delivery robot could drop the trash or fail to pick\\nit up when it attempts to, you may need the robot to be 80% sure that\\nit has picked up three items of trash. Often you want to distinguish a',\n",
       " 'it has picked up three items of trash. Often you want to distinguish a\\nfalse-positive error (positive answers that are not correct) from a falsenegative error (negative answers that are correct). Some applications are\\nmuch more tolerant of one of these types of errors than the other.\\nThese categories are not exclusive. A form of learning known as probably\\napproximately correct (PAC) learning considers probably learning an approximately correct concept.\\n1.6.4 Representations\\nOnce you have some requirements on the nature of a solution, you must represent the task so a computer can solve it.\\nComputers and human minds are examples of physical symbol systems.\\nA symbol is a meaningful pattern that can be manipulated. Examples of symbols are written words, sentences, gestures, marks on paper, or sequences of\\nbits. A symbol system creates, copies, modifies, and destroys symbols. Essentially, a symbol is one of the patterns manipulated as a unit by a symbol system.',\n",
       " 'The term “physical” is used, because symbols in a physical symbol system are\\nphysical objects that are part of the real world, even though they may be internal to computers and brains. They may also need to physically affect action or\\nmotor control.\\nThe physical symbol system hypothesis of Newell and Simon [1976] is\\nthat:\\nA physical symbol system has the necessary and sufficient means\\nfor general intelligent action.\\nThis is a strong hypothesis. It means that any intelligent agent is necessarily a\\nphysical symbol system. It also means that a physical symbol system is all that\\nis needed for intelligent action; there is no magic or as-yet-to-be-discovered\\nquantum phenomenon required. It does not imply that a physical symbol system does not need a body to sense and act in the world.\\nOne aspect of this hypothesis is particularly controversial, namely whether\\nsymbols are needed at all levels. For example, consider recognizing a “cat” in',\n",
       " 'symbols are needed at all levels. For example, consider recognizing a “cat” in\\na picture. At the top level is the symbol for a cat. At the bottom level are pixels\\nfrom a camera. There are many intermediate levels that, for example, combine\\npixels to form lines and textures. These intermediate features are learned from\\n1.6. Designing Agents 43\\ndata, and are not learned with the constraint that they are interpretable. Although some people have tried to interpret them, it is reasonable to say that\\nthese are not symbols. However, at a high level, they are either trained to be\\nsymbols (e.g., by learning a mapping between pixels and symbols, such as cat)\\nor can be interpreted as symbols.\\nAn agent can use a physical symbol system to model the world. A model\\nof a world is a representation of an agent’s beliefs about what is true in the\\nworld or how the world changes. The world does not have to be modeled at\\nthe most detailed level to be useful. All models are abstractions; they represent',\n",
       " 'the most detailed level to be useful. All models are abstractions; they represent\\nonly part of the world and leave out many of the details. An agent can have\\na very simplistic model of the world, or it can have a very detailed model of\\nthe world. The level of abstraction provides a partial ordering of abstraction.\\nA lower-level abstraction includes more details than a higher-level abstraction.\\nAn agent can have multiple, even contradictory, models of the world. Models\\nare judged not by whether they are correct, but by whether they are useful.\\nExample 1.34 A delivery robot can model the environment at a high level of\\nabstraction in terms of rooms, corridors, doors, and obstacles, ignoring distances, its size, the steering angles needed, the slippage of the wheels, the\\nweight of parcels, the details of obstacles, the political situation in Canada, and',\n",
       " 'weight of parcels, the details of obstacles, the political situation in Canada, and\\nvirtually everything else. The robot could model the environment at lower levels of abstraction by taking some of these details into account. Some of these\\ndetails may be irrelevant for the successful implementation of the robot, but\\nsome may be crucial for the robot to succeed. For example, in some situations\\nthe size of the robot and the steering angles may be crucial for not getting stuck\\naround a particular corner. In other situations, if the robot stays close to the\\ncenter of the corridor, it may not need to model its width or the steering angles.\\nChoosing an appropriate level of abstraction is difficult for the following\\nreasons:\\n• A high-level description is easier for a human to specify and understand.\\n• A low-level description can be more accurate and more predictive. Often,\\nhigh-level descriptions abstract away details that may be important for\\nactually solving the task.',\n",
       " 'high-level descriptions abstract away details that may be important for\\nactually solving the task.\\n• The lower the level, the more difficult it is to reason with. This is because\\na solution at a lower level of detail involves more steps and many more\\npossible courses of action exist from which to choose.\\n• An agent may not know the information needed for a low-level description. For example, the delivery robot may not know what obstacles it will\\nencounter or how slippery the floor will be at the time that it must decide\\nwhat to do.\\nIt is often a good idea to model an environment at multiple levels of abstraction. This issue is discussed further in Section 2.2 (page 58).\\nBiological systems, and computers, can be described at multiple levels of\\nabstraction. At successively lower levels of animals are the neuronal level,\\n44 1. Artificial Intelligence and Agents',\n",
       " '44 1. Artificial Intelligence and Agents\\nthe biochemical level (what chemicals and what electrical potentials are being transmitted), the chemical level (what chemical reactions are being carried\\nout), and the level of physics (in terms of forces on atoms and quantum phenomena). What levels above the neuronal level are needed to account for intelligence is still an open question. These levels of description are echoed in the\\nhierarchical structure of science itself, where scientists are divided into physicists, chemists, biologists, psychologists, anthropologists, and so on. Although\\nno level of description is more important than any other, it is plausible that you\\ndo not have to emulate every level of a human to build an AI agent but rather\\nyou can emulate the higher levels and build them on the foundation of modern\\ncomputers. This conjecture is part of what AI studies.\\nThe following are two levels that seem to be common to both biological and\\ncomputational entities:',\n",
       " 'The following are two levels that seem to be common to both biological and\\ncomputational entities:\\n• The knowledge level is the level of abstraction that considers what an\\nagent knows and believes and what its goals are. The knowledge level\\nconsiders what an agent knows, but not how it reasons. For example, the\\ndelivery agent’s behavior can be described in terms of whether it knows\\nthat a parcel has arrived or not and whether it knows where a particular\\nperson is or not. Both human and robotic agents are describable at the\\nknowledge level. At this level, you do not specify how the solution will\\nbe computed or even which of the many possible strategies available to\\nthe agent will be used.\\n• The symbol level is a level of description of an agent in terms of the reasoning it does. To implement the knowledge level, an agent manipulates\\nsymbols to produce answers. Many cognitive science experiments are',\n",
       " 'symbols to produce answers. Many cognitive science experiments are\\ndesigned to determine what symbol manipulation occurs during reasoning. Whereas the knowledge level is about what the agent believes about\\nthe external world and what its goals are in terms of the outside world,\\nthe symbol level is about what goes on inside an agent to reason about\\nthe external world.\\n1.7 Social Impact\\nAI systems are now widely deployed in society. Individuals, corporations,\\ngovernments, and other organizations are using AI for applications as varied as voice dictation, text synthesis, text-to-video generation, movie recommendations, personal finance, chatbots, credit scoring, screening employment\\napplications, social media propagation and monitoring, face recognition, semiautonomous cars, and warehouse automation. Many of these systems can be\\nbroadly beneficial. However, there are often adverse impacts on people in\\nracialized populations and underserved communities, and on election results',\n",
       " 'racialized populations and underserved communities, and on election results\\nand vaccination campaigns.\\nThere are significant ethical and social impacts of AI systems, leading to\\ndemands for human-centered AI that is explainable, transparent, and trust-\\n1.8. Overview of the Book 45\\nworthy. The inputs to an AI agent include the goals and preferences of the\\nagent, but it is not clear whose preferences they are or should be.\\nEach chapter concludes with a social impact section discussing issues directly relevant to that chapter’s topics. The social impact sections are of two\\ntypes, sometimes containing both:\\n• broader impacts of AI, which includes intended or unintended downstream consequences of upstream decisions on the design of the AI system or on the choice of data\\n• use cases about user-facing applications of AI that have had an impact on\\nsociety or science, either positive or negative.\\nChapter 18 on the social impact of AI considers the effects of AI on the',\n",
       " 'Chapter 18 on the social impact of AI considers the effects of AI on the\\ndigital economy, work and automation, transportation and sustainability. It\\nhighlights the roles of human-centered AI, values, bias, ethics, certification,\\nand regulation.\\n1.8 Overview of the Book\\nThe rest of the book explores the design space defined by the dimensions of\\ncomplexity. It considers each dimension separately, where this can be done\\nsensibly.\\nPart I considers the big view of agents as a coherent vision of AI.\\nChapter 2 analyzes what is inside the black box of Figure 1.4 (page 15) and\\ndiscusses the modular and hierarchical decomposition of intelligent agents.\\nPart II considers the case of no uncertainty, which is a useful abstraction of\\nmany domains.\\nChapter 3 considers the simplest case of determining what to do in the case\\nof a single agent that reasons with explicit states, no uncertainty, and has goals\\nto be achieved, but with an indefinite horizon. In this case, the task of solving',\n",
       " 'to be achieved, but with an indefinite horizon. In this case, the task of solving\\nthe goal can be abstracted to searching for a path in a graph. It is shown how\\nextra knowledge of the domain can help the search.\\nChapters 4 and 5 show how to exploit features. In particular, Chapter 4\\nconsiders how to find possible states given constraints on the assignments of\\nvalues to features represented as variables. Chapter 5 presents reasoning with\\npropositions in various forms.\\nChapter 6 considers the task of planning, in particular determining sequences of actions to solve a goal in deterministic domains.\\nPart III considers learning and reasoning with uncertainty. In particular, it\\nconsiders sensing uncertainty and effect uncertainty.\\nChapter 7 shows how an agent can learn from past experiences and data.\\nIt covers the most common case of learning, namely supervised learning with\\nfeatures, where a function from input features into target features is learned',\n",
       " 'features, where a function from input features into target features is learned\\n46 1. Artificial Intelligence and Agents\\nfrom observational data. Chapter 8 studies neural networks and deep learning\\nand how features themselves can be learned from sensory observation.\\nChapter 9 shows how to reason with uncertainty, in particular with probability and graphical models of independence. Chapter 10 introduces learning\\nwith uncertainty. Chapter 11 shows how to model causality and learn the effects of interventions (which cannot be learned from observation alone).\\nPart IV considers planning and acting with uncertainty.\\nChapter 12 considers the task of planning with uncertainty. Chapter 13\\ndeals with reinforcement learning, where agents learn what to do. Chapter 14\\nexpands planning to deal with issues arising from multiple agents.\\nPart V extends the state and feature-based representations to deal with relational representations, in terms of relations and individuals.',\n",
       " 'Chapter 15 shows how to reason in terms of individuals and relations.\\nChapter 16 discusses how to enable semantic interoperability using knowledge\\ngraphs and ontologies. Chapter 17 shows how reasoning about individuals\\nand relations can be combined with learning and probabilistic reasoning.\\nPart VI steps back from the details and gives the big picture.\\nIn Chapter 18 on the social impact of AI, further ethical and social concerns\\nare addressed, by considering various questions, such as: What are the effects,\\nbenefits, costs, and risks of deployed AI systems for society? What are the\\nethical, equity, and regulatory considerations involved in building intelligent\\nagents? How can you ensure that AI systems are fair, transparent, explainable,\\nand trustworthy? How can AI systems be human-centered? What is the impact\\non sustainability?\\nChapter 19 reviews the design space of AI and shows how the material presented can fit into that design space. It also considers some likely and possible',\n",
       " 'future scenarios for the development of AI science and technology.\\n1.9 Review\\nThe following are the main points you should have learned from this chapter:\\n• Artificial intelligence is the study of computational agents that act intelligently.\\n• An agent acts in an environment and only has access to its abilities, its\\nprior knowledge, its history of stimuli, and its goals and preferences.\\n• A physical symbol system manipulates symbols to determine what to do.\\n• A designer of an intelligent agent should be concerned about modularity,\\nhow to describe the world, how far ahead to plan, uncertainty in both\\nperception and the effects of actions, the structure of goals or preferences,\\nother agents, how to learn from experience, how the agent can reason\\nwhile interacting with the environment, and the fact that all real agents\\nhave limited computational resources.\\n• To solve a task by computer, the computer must have an effective representation with which to reason.',\n",
       " '1.10. References and Further Reading 47\\n• To know when it has solved a task, an agent must have a definition of\\nwhat constitutes an adequate solution, such as whether it has to be optimal, approximately optimal, or almost always optimal, or whether a\\nsatisficing solution is adequate.\\n• In choosing a representation, an agent designer should find a representation that is as close as possible to the task, so that it is easy to determine\\nwhat is represented and so it can be checked for correctness and be able\\nto be maintained. Often, users want an explanation of why they should\\nbelieve the answer.\\n• The social impacts, both beneficial and harmful, of pervasive AI applications are significant, leading to calls for ethical and human-centered AI,\\ncertification and regulation.\\n1.10 References and Further Reading\\nThe ideas in this chapter have been derived from many sources. Here, we try to\\nacknowledge those that are explicitly attributable to particular authors. Most',\n",
       " 'acknowledge those that are explicitly attributable to particular authors. Most\\nof the other ideas are part of AI folklore; trying to attribute them to anyone\\nwould be impossible.\\nLevesque [2012] provides an accessible account of how thinking can be\\nseen in terms of computation. Haugeland [1997] contains a good collection\\nof articles on the philosophy behind artificial intelligence, including that classic paper of Turing [1950] that proposes the Turing test. Grosz [2012] and Cohen [2005] discuss the Turing test from a more recent perspective. Winograd\\nschemas are described by Levesque [2014]. Srivastava et al. [2022] provide a\\nBeyond the Imitation Game benchmark (BIG-bench) consisting of 204 tasks designed to challenge modern learning systems. Grosz [2018] discusses research\\non what it takes to implement dialog, not just answering one-off questions.\\nZador et al. [2023] discuss an embodied Turing test, and the role of neuroscience in AI.',\n",
       " 'Zador et al. [2023] discuss an embodied Turing test, and the role of neuroscience in AI.\\nNilsson [2010] and Buchanan [2005] provide accessible histories of AI. Chrisley and Begeer [2000] present many classic papers on AI. Jordan [2019] and the\\nassociated commentaries discuss intelligence augmentation.\\nFor discussions on the foundations of AI and the breadth of research in\\nAI, see Kirsh [1991a], Bobrow [1993], and the papers in the corresponding volumes, as well as Schank [1990] and Simon [1995]. The importance of knowledge in AI is discussed in Lenat and Feigenbaum [1991], Sowa [2000], Darwiche [2018], and Brachman and Levesque [2022b].\\nThe physical symbol system hypothesis was posited by Newell and Simon\\n[1976]. Simon [1996] discusses the role of symbol systems in a multidisciplinary\\ncontext. The distinctions between real, synthetic, and artificial intelligence are',\n",
       " 'context. The distinctions between real, synthetic, and artificial intelligence are\\ndiscussed by Haugeland [1985], who also provides useful introductory material on interpreted, automatic formal symbol systems and the Church–Turing\\nthesis. Brooks [1990] and Winograd [1990] critique the symbol system hy-\\n48 1. Artificial Intelligence and Agents\\npothesis. Nilsson [2007] evaluates the hypothesis in terms of such criticisms.\\nShoham [2016] and Marcus and Davis [2019] argue for the importance of symbolic knowledge representation in modern applications.\\nThe use of anytime algorithms is due to Horvitz [1989] and Boddy and\\nDean [1994]. See Dean and Wellman [1991], Zilberstein [1996], and Russell\\n[1997] for introductions to bounded rationality.\\nFor overviews of cognitive science and the role that AI and other disciplines\\nplay in that field, see Gardner [1985], Posner [1989], and Stillings et al. [1987].\\nConati et al. [2002] describe a tutoring agent for elementary physics. du',\n",
       " 'Conati et al. [2002] describe a tutoring agent for elementary physics. du\\nBoulay et al. [2023] overview modern tutoring agents. Wellman [2011] overviews\\nresearch in trading agents. Sandholm [2007] describes how AI can be used for\\nprocurement of multiple goods with complex preferences.\\nA number of AI texts are valuable as reference books complementary to this\\nbook, providing a different perspective on AI. In particular, Russell and Norvig\\n[2020] give a more encyclopedic overview of AI. They provide an excellent\\ncomplementary source for many of the topics covered in this book and also an\\noutstanding review of the scientific literature, which we do not try to duplicate.\\nThe Association for the Advancement of Artificial Intelligence (AAAI) provides introductory material and news at their AI Topics website (https://aitopics.\\norg/). AI Magazine, published by AAAI, often has excellent overview articles',\n",
       " 'org/). AI Magazine, published by AAAI, often has excellent overview articles\\nand descriptions of particular applications. IEEE Intelligent Systems also provides accessible articles on AI research.\\nThere are many journals that provide in-depth research contributions and\\nconferences where the most up-to-date research is found. These include the\\njournals Artificial Intelligence, the Journal of Artificial Intelligence Research, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, and Computational Intelligence, as well as more specialized journals. Much of the cutting-edge research is published first in conferences. Those of most interest to a general\\naudience are the International Joint Conference on Artificial Intelligence (IJCAI), the AAAI Annual Conference, the European Conference on AI (ECAI),\\nthe Pacific Rim International Conference on AI (PRICAI), various national conferences, and many specialized conferences, which are referred to in the relevant chapters.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd07c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "emb_model_path = r\"C:\\Users\\aksha\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\c9745ed1d9f207416be6d2e6f8de32d1f16199bf\"\n",
    "emb_model = SentenceTransformer(emb_model_path)\n",
    "embeddings = emb_model.encode(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb56931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06425917, -0.01319843,  0.0173543 , ...,  0.05364403,\n",
       "        -0.02337917, -0.03919852],\n",
       "       [-0.01563522, -0.01199588,  0.00286812, ...,  0.11127949,\n",
       "         0.01429084, -0.06432381],\n",
       "       [ 0.06159442,  0.02611762,  0.01889746, ...,  0.01170786,\n",
       "         0.01591869, -0.07640506],\n",
       "       ...,\n",
       "       [-0.0330135 ,  0.00541595, -0.04700977, ...,  0.12952472,\n",
       "         0.05331912, -0.04209765],\n",
       "       [-0.09339085,  0.00612115, -0.02652815, ...,  0.00730608,\n",
       "        -0.05596258,  0.02916876],\n",
       "       [-0.06650215,  0.00929145, -0.04098547, ...,  0.06698386,\n",
       "        -0.01833435, -0.01020474]], shape=(137, 384), dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27124361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e67b29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91774336",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be1c8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, 'doc_index.faiss')\n",
    "\n",
    "with open('doc_split_text.pkl', 'wb') as f:\n",
    "    pickle.dump(split_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c8d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
